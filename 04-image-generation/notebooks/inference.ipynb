{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a9cfb23-42af-4fed-a375-54d310c8e285",
   "metadata": {},
   "source": [
    "## DreamBooth on Stable Diffusion 2.1\n",
    "\n",
    "DreamBooth is a powerful technique for personalizing latent diffusion models, such as Stable Diffusion, allowing the model to be fine-tuned to generate specific images based on a limited set of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab16e0-eec7-4cc8-aee9-305cc190e597",
   "metadata": {},
   "source": [
    "### Step 0: Environment configuration\n",
    "\n",
    "This command installs the \"diffusers\" library directly from the Hugging Face GitHub repository.The diffusers library is used to work with latent diffusion models, such as Stable Diffusion, and provides tools for image generation, fine-tuning, and other functionalities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64567c2e-085f-4db3-bd89-f7ab3090ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1914541-0648-454e-9ed5-0f51eee56427",
   "metadata": {},
   "source": [
    "### Configuration and Secrets Loading\n",
    "In this section, we load configuration parameters and API keys from separate YAML files. This separation helps maintain security by keeping sensitive information (API keys) separate from configuration settings.\n",
    "\n",
    "- **config.yaml**: Contains non-sensitive configuration parameters like model sources and URLs\n",
    "- **secrets.yaml**: Contains sensitive API keys for services like Galileo and HuggingFace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859c807-3852-4404-b624-aabf86afb262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the src directory to the path to import utils\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../..\")))\n",
    "from src.utils import load_config_and_secrets\n",
    "\n",
    "config_path = \"../../configs/config.yaml\"\n",
    "secrets_path = \"../../configs/secrets.yaml\"\n",
    "\n",
    "config, secrets = load_config_and_secrets(config_path, secrets_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab779c6-4f85-479b-a2b2-831a02a2428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.custom_metrics.image_metrics_scorers import entropy_scorer, complexity_scorer, set_custom_image_path\n",
    "from core.deploy.deploy_image_generation import deploy_model\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "import promptquality as pq\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea33851-2215-4fab-b02d-b78aab3f3c08",
   "metadata": {},
   "source": [
    "### Download model local\n",
    "\n",
    "This code imports the snapshot_download function from the huggingface_hub library to download the latest version of the \"stabilityai/stable-diffusion-2-1\" model. It sets a local directory for saving the model (local_model_path), and the download is configured to be resumable in case it is interrupted, with an etag timeout set to 60 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b1ae0b-fcb1-46f6-9275-30746011c7ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Download the snapshot directly to the local directory\n",
    "local_model_path = os.path.join(\"..\", \"..\", \"..\", \"local\", \"stable-diffusion-2-1\")\n",
    "\n",
    "# Downloading the latest revision of the \"stabilityai/stable-diffusion-2-1\" model\n",
    "snapshot_download(\n",
    "    repo_id=\"stabilityai/stable-diffusion-2-1\", \n",
    "    local_dir=local_model_path,\n",
    "    resume_download=True,\n",
    "    etag_timeout=60  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841fcc24-ed5d-46d3-b9ce-3f48451a5e01",
   "metadata": {},
   "source": [
    "### Step 1: Load the Model\n",
    "We load the pre-trained Stable Diffusion 2.1 model from Hugging Face and move it to the GPU for efficient execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c4d448-a095-4db9-84e8-207c00b52cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.local_inference.inference import StableDiffusionPipelineOutput, load_config, run_inference\n",
    "\n",
    "config = load_config()\n",
    "\n",
    "run_inference(\n",
    "    prompt=\"A beautiful landscape\",\n",
    "    height=768,\n",
    "    width=768,\n",
    "    num_images=1,\n",
    "    num_inference_steps=60\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321b07ea-7694-4c3d-8959-82533d6021ed",
   "metadata": {},
   "source": [
    "## Step 3: Training Dreambooth\n",
    "\n",
    "This Bash script checks the available GPUs using PyTorch, selects a multi-GPU or single-GPU configuration file accordingly, and then launches a training script (using accelerate) for Dreambooth on Stable Diffusion with specified parameters. It also records and calculates the training duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03076937-b726-419b-b59f-5180ccd85bec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "NUM_GPUS=$(python3 -c \"import torch; print(torch.cuda.device_count())\")\n",
    "\n",
    "if [ \"$NUM_GPUS\" -ge 2 ]; then\n",
    "  CONFIG_FILE=\"../data/config/default_config_multi-gpu.yaml\"\n",
    "  echo \"Detected $NUM_GPUS GPUs, using $CONFIG_FILE\"\n",
    "else\n",
    "  CONFIG_FILE=\"../data/config/default_config_one-gpu.yaml\"\n",
    "  echo \"Detected $NUM_GPUS GPU, using $CONFIG_FILE\"\n",
    "fi\n",
    "\n",
    "START=$(date +%s)\n",
    "\n",
    "accelerate launch --config_file $CONFIG_FILE core/train/train_dreambooth_aistudio.py \\\n",
    "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-2-1\"  \\\n",
    "  --instance_data_dir=\"../data/img\" \\\n",
    "  --output_dir=\"./dreambooth/\" \\\n",
    "  --instance_prompt=\"A modern laptop on a sandy beach with the ocean in the background, sunlight reflecting off the screen\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --learning_rate=5e-6 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=400 \\\n",
    "  --logging_dir=\"/phoenix/tensorboard/tensorlogs\" \\\n",
    "  --report_to=\"tensorboard\" \\\n",
    "  --validation_prompt=\"A photo of an HP laptop on the sand with a sunset over the ocean in the background.\" \\\n",
    "  --num_validation_images=1 \\\n",
    "  --validation_steps=100\n",
    "\n",
    "END=$(date +%s)\n",
    "DIFF=$(( $END - $START ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623c9bbd-dcf8-41f6-9815-4c7a29f9ce9b",
   "metadata": {},
   "source": [
    "## Inference Local Model\n",
    "\n",
    "This code imports functions from the inference_dreambooth module, loads a configuration, and then runs inference to generate images. It uses a prompt to create three images with a resolution of 768x768 pixels, executing 100 inference steps per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0ab869-830d-45a8-81f9-239856d1cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.dreambooth_inference.inference_dreambooth import StableDiffusionPipelineOutput, load_config, run_inference\n",
    "\n",
    "config = load_config()\n",
    "\n",
    "run_inference(\n",
    "    prompt=\"A high-quality photo of an HP laptop placed on the sand at the beach, with a sunset over the ocean in the background.\", \n",
    "    height=768, \n",
    "    width=768, \n",
    "    num_images=3, \n",
    "    num_inference_steps=100  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2051d-f562-4d8b-b5e3-cd292e39241d",
   "metadata": {},
   "source": [
    "## Galileo Evaluate Custom metrics\n",
    "Galileo GenAI Studio supports Custom Metrics (programmatic or GPT-based) for all your Evaluate and Observe projects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d340fae2-64e8-44ed-b491-36a744bc7f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################################\n",
    "# In order to connect to Galileo, create a secrets.yaml file in the same folder as this notebook\n",
    "# This file should be an entry called Galileo, with the your personal Galileo API Key\n",
    "# Galileo API keys can be created on https://console.hp.galileocloud.io/settings/api-keys\n",
    "#########################################\n",
    "\n",
    "with open('secrets.yaml') as file:\n",
    "    secrets = yaml.safe_load(file)\n",
    "    os.environ['GALILEO_API_KEY'] = secrets[\"Galileo\"]\n",
    "\n",
    "os.environ['GALILEO_CONSOLE_URL'] = \"https://console.hp.galileocloud.io/\" \n",
    "\n",
    "pq.login(os.environ['GALILEO_CONSOLE_URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6ed1a2-cd33-4b42-ba5d-795199180d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################################\n",
    "\n",
    "# Returns the path of the most recent image that matches the specified pattern.\n",
    "\n",
    "#########################################\n",
    "\n",
    "def get_latest_generated_image(directory: str = \"./\", prefix: str = \"local_model_result_\", ext: str = \".png\") -> str:\n",
    "    files = glob.glob(os.path.join(directory, f\"{prefix}*{ext}\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No generated images founda.\")\n",
    "    latest_file = max(files, key=os.path.getmtime)\n",
    "    return latest_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fb69f4-dd2f-4cb4-b25f-25429169c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = load_config()\n",
    "\n",
    "prompt_text = (\"A high-quality photo of an HP laptop placed on the sand at the beach, \"\n",
    "               \"with a sunset over the ocean in the background.\")\n",
    "\n",
    "run_inference(\n",
    "    prompt=prompt_text, \n",
    "    height=768, \n",
    "    width=768, \n",
    "    num_images=1, \n",
    "    num_inference_steps=100  \n",
    ")\n",
    "\n",
    "generated_image_path = get_latest_generated_image()\n",
    "\n",
    "set_custom_image_path(generated_image_path)\n",
    "\n",
    "template = prompt_text\n",
    "\n",
    "result_custom = pq.run(template=template, scorers=[entropy_scorer, complexity_scorer])\n",
    "print(\"Result:\", result_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f737c7-b9ff-45c1-a7c2-df5e2ab479a8",
   "metadata": {},
   "source": [
    "## Model Service\n",
    "\n",
    "Using MLflow, we will save and load the model in an integrated manner, enabling the traceability and reproducibility of experiments. MLflow will facilitate model versioning, monitoring, and deployment, ensuring a robust pipeline for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d920eec1-6c4a-4141-9956-a0d8f1c688d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deploy_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
