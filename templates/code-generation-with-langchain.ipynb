{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb696e77-2b43-4599-91ee-32e70fe81af6",
   "metadata": {},
   "source": [
    "## Step 0: Configuring the environment\n",
    "\n",
    "In this step, we are installing the libraries allowed for our project, which involve the use of LangChain, integration with Huggingface models, OpenAI, in addition to the storage of embeddings using ChromaDB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baacc9ef-f902-4ff5-9a29-19159516c9e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  GitPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221d332-9889-4d45-a9d3-fb9b66c7c1e4",
   "metadata": {},
   "source": [
    "## Step 1: Downloading Notebooks and Extracting Code from Jupyter Notebooks\r\n",
    "\r\n",
    "In this step, we download Jupyter Notebook files (*.ipynb*) from a GitHub repository directly using the GitHub API, and extract both code and context from these notebooks. The code performs the following operations:\r\n",
    "\r\n",
    "- **Downloading Notebooks from a GitHub Repository**:  \r\n",
    "  We begin by downloading only the Jupyter Notebooks from the desired GitHub repository using the function `download_and_extract_notebooks`.\r\n",
    "\r\n",
    "  - **Function** `download_and_extract_notebooks(repo_owner, repo_name, save_dir='./notebooks')`:  \r\n",
    "    - **Objective**: This function uses the GitHub API to download all `.ipynb` files from a given repository and saves them in a specified directory.\r\n",
    "    - **Process**:\r\n",
    "        - First, it checks if the target directory already exists. If it does, the directory is deleted to ensure a fresh copy of the notebooks.\r\n",
    "        - The function calls the GitHub API to list the contents of the repository and recursively navigates through subdirectories, downloading only the files that end with `.ipynb`.\r\n",
    "        - For each notebook downloaded, the function extracts the code and context (explained below).\r\n",
    "    - **Input**:\r\n",
    "        - **repo_owner**: The owner of the GitHub repository (e.g., `passarel`).\r\n",
    "        - **repo_name**: The name of the repository (e.g., `crawler_data_source`).\r\n",
    "        - **save_dir**: The directory where the notebooks will be saved (default is `./notebooks`).\r\n",
    "\r\n",
    "- **Extracting Code and Context From Notebooks**:  \r\n",
    "  After downloading the notebooks, the next step is to extract both the code and any markdown context from each notebook.\r\n",
    "\r\n",
    "  - **Function** `extract_code_and_context(notebook_path)`:  \r\n",
    "    - **Objective**: This function reads a notebook and extracts the code cells and any corresponding markdown context.\r\n",
    "\r\n",
    "- **Process**:\r\n",
    "  - The notebook is opened using the `nbformat.read` function.\r\n",
    "  - The function iterates through each cell of the notebook:\r\n",
    "    - If the cell is of type markdown, it extracts the content of the markdown cell as context.\r\n",
    "    - If the cell is of type code, it creates a dictionary with the following fields:\r\n",
    "      - **ID**: A unique identifier for the code snippet, generated using `uuid.uuid4()`.\r\n",
    "      - **Embedding**: Initially set to None (embeddings will be generated later).\r\n",
    "      - **Code**: The code content of the cell.\r\n",
    "      - **Filename**: The name of the notebook file.\r\n",
    "      - **Context**: The markdown context associated with the code (if any).\r\n",
    "  - The extracted code and context are appended to a list.\r\n",
    "\r\n",
    "### Key Changes from the Original Approach:\r\n",
    "1. **No Cloning of the Entire Repository**:  \r\n",
    "   Instead of cloning the entire repository, we now directly interact with the GitHub API to download only the relevant `.ipynb` files, saving time and space.\r\n",
    "2. **Recursion into Subdirectories**:  \r\n",
    "   The code automatically handles subdirectories within the repository, ensuring that all notebooks, regardless of their location, are processed.\r\n",
    "3. **Cleaner Data Handling**:  \r\n",
    "   Each notebook is processed immediately after being downloaded, simplifying the workflow and ensuring that extracted data is directly available for further use.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45a8637c-cc1b-41e0-87be-ee4f622ae737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import shutil\n",
    "import nbformat\n",
    "import uuid\n",
    "\n",
    "# Function to download a specific file from GitHub\n",
    "def download_file(file_url, save_path):\n",
    "    response = requests.get(file_url)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded: {save_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {file_url}, Status Code: {response.status_code}\")\n",
    "\n",
    "# Function to get all .ipynb files from a GitHub repo and extract data\n",
    "def download_and_extract_notebooks(repo_owner, repo_name, save_dir='./notebooks'):\n",
    "    # Check if the directory exists and remove it if it does\n",
    "    if os.path.exists(save_dir):\n",
    "        shutil.rmtree(save_dir)  # Removes the entire directory\n",
    "        print(f\"Existing directory {save_dir} removed.\")\n",
    "    \n",
    "    # Create the directory to save the notebooks\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "    # GitHub API URL to list repo contents\n",
    "    api_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents\"\n",
    "\n",
    "    response = requests.get(api_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching repository contents: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    # Parse the JSON response\n",
    "    repo_contents = response.json()\n",
    "\n",
    "    # Extracted data list\n",
    "    all_extracted_data = []\n",
    "\n",
    "    # Filter for .ipynb files and download them\n",
    "    for item in repo_contents:\n",
    "        if item['type'] == 'file' and item['name'].endswith('.ipynb'):\n",
    "            notebook_path = os.path.join(save_dir, item['name'])\n",
    "            download_file(item['download_url'], notebook_path)\n",
    "            # Extract code and context\n",
    "            extracted_data = extract_code_and_context(notebook_path)\n",
    "            all_extracted_data.extend(extracted_data)\n",
    "        elif item['type'] == 'dir':  # If it's a directory, fetch the contents of the directory\n",
    "            download_notebooks_from_repo_dir(repo_owner, repo_name, item['path'], save_dir, all_extracted_data)\n",
    "\n",
    "    return all_extracted_data\n",
    "\n",
    "# Recursive function to list contents from a specific directory in a GitHub repo and extract data\n",
    "def download_notebooks_from_repo_dir(repo_owner, repo_name, dir_path, save_dir, all_extracted_data):\n",
    "    api_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{dir_path}\"\n",
    "    response = requests.get(api_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching directory contents: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    repo_contents = response.json()\n",
    "\n",
    "    for item in repo_contents:\n",
    "        if item['type'] == 'file' and item['name'].endswith('.ipynb'):\n",
    "            notebook_path = os.path.join(save_dir, os.path.basename(item['path']))\n",
    "            download_file(item['download_url'], notebook_path)\n",
    "            # Extract code and context\n",
    "            extracted_data = extract_code_and_context(notebook_path)\n",
    "            all_extracted_data.extend(extracted_data)\n",
    "        elif item['type'] == 'dir':  # Recurse into subdirectories\n",
    "            download_notebooks_from_repo_dir(repo_owner, repo_name, item['path'], save_dir, all_extracted_data)\n",
    "\n",
    "# Function to extract code and context from notebooks\n",
    "def extract_code_and_context(notebook_path):\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "    extracted_data = []\n",
    "    for cell in notebook['cells']:\n",
    "        if cell['cell_type'] == 'markdown':\n",
    "            context = ''.join(cell['source'])\n",
    "        elif cell['cell_type'] == 'code':\n",
    "            cell_data = {\n",
    "                \"id\": str(uuid.uuid4()),  \n",
    "                \"embedding\": None,        \n",
    "                \"code\": ''.join(cell['source']),\n",
    "                \"filename\": os.path.basename(notebook_path),\n",
    "                \"context\": context if 'context' in locals() else ''\n",
    "            }\n",
    "            extracted_data.append(cell_data)\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "# Example usage for the repository:\n",
    "repo_owner = \"passarel\"\n",
    "repo_name = \"crawler_data_source\"\n",
    "\n",
    "# This will download all .ipynb files from the repo, extract the data and return it\n",
    "extracted_notebooks_data = download_and_extract_notebooks(repo_owner, repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fb460b-5391-41e6-8b17-d0d44d834bcb",
   "metadata": {},
   "source": [
    "### Alternative code to download the repository in case of a connection error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5856355-f7a0-4196-88be-42b7db1420ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing directory ./notebooks removed.\n",
      "Repository cloned in: ./notebooks\n",
      "Extracting data from: ./notebooks/LLM_experiments/GemmaSummarization/fine-tuning-4bits.ipynb\n",
      "Extracting data from: ./notebooks/LLM_experiments/GemmaSummarization/fine-tuning-fullprec.ipynb\n",
      "Extracting data from: ./notebooks/LLM_experiments/GemmaSummarization/fine-tuning-8bits.ipynb\n",
      "Extracting data from: ./notebooks/LLM_experiments/Galileo/summarization-with-langchain.ipynb\n",
      "Extracting data from: ./notebooks/LLM_experiments/Galileo/text-generation-with-langchain.ipynb\n",
      "Extracting data from: ./notebooks/LLM_experiments/Galileo/code-generation-with-langchain.ipynb\n",
      "Extracting data from: ./notebooks/LLM_experiments/Galileo/chatbot-with-langchain.ipynb\n",
      "Extracting data from: ./notebooks/Natural_Language/bert_qa/Training.ipynb\n",
      "Extracting data from: ./notebooks/Natural_Language/bert_qa/Deployment.ipynb\n",
      "Extracting data from: ./notebooks/Natural_Language/bert_qa/Testing Mlflow Server.ipynb\n",
      "Extracting data from: ./notebooks/Natural_Language/text_classification/Spam_Detection.ipynb\n",
      "Extraction completed. Total notebooks processed: 11\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "#import git\n",
    "#import nbformat\n",
    "#import uuid\n",
    "#import shutil\n",
    "\n",
    "# Function to clone GitHub repository with validation\n",
    "#def clone_repo(repo_url, clone_dir=\"./notebooks\"):\n",
    "    # Check if the directory exists\n",
    "#    if os.path.exists(clone_dir):\n",
    "        # Remove the existing directory\n",
    "#        shutil.rmtree(clone_dir)\n",
    "#        print(f\"Existing directory {clone_dir} removed.\")\n",
    "        \n",
    "    # Clone the repo into the specified directory\n",
    "#    git.Repo.clone_from(repo_url, clone_dir)\n",
    "#    print(f\"Repository cloned in: {clone_dir}\")\n",
    "\n",
    "# Function to find all .ipynb notebooks in a directory\n",
    "#def find_all_notebooks(directory):\n",
    "#    notebooks = []\n",
    "#    for root, dirs, files in os.walk(directory):\n",
    "#        for file in files:\n",
    "#            if file.endswith(\".ipynb\"):\n",
    "#                notebooks.append(os.path.join(root, file))\n",
    "#    return notebooks\n",
    "\n",
    "# Function to extract code and context from notebooks\n",
    "#def extract_code_and_context(notebook_path):\n",
    "#    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "#        notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "#    extracted_data = []\n",
    "#    for cell in notebook['cells']:\n",
    "#        if cell['cell_type'] == 'markdown':\n",
    "#            context = ''.join(cell['source'])\n",
    "#        elif cell['cell_type'] == 'code':\n",
    "#            cell_data = {\n",
    "#                \"id\": str(uuid.uuid4()),  \n",
    "#                \"embedding\": None,        \n",
    "#                \"code\": ''.join(cell['source']),\n",
    "#                \"filename\": os.path.basename(notebook_path),\n",
    "#                \"context\": context if 'context' in locals() else ''\n",
    "#            }\n",
    "#            extracted_data.append(cell_data)\n",
    "\n",
    "#    return extracted_data\n",
    "\n",
    "# Main function to clone and process notebooks\n",
    "#def process_repo(repo_url, clone_dir=\"./notebooks\"):\n",
    "    # Clone the repository (if exists, remove and clone again)\n",
    "#    clone_repo(repo_url, clone_dir)\n",
    "    \n",
    "    # Find all notebooks in the cloned repo\n",
    "#    notebooks = find_all_notebooks(clone_dir)\n",
    "    \n",
    "#    all_extracted_data = []\n",
    "    \n",
    "    # Process each notebook to extract code and context\n",
    "#    for notebook in notebooks:\n",
    "#        print(f\"Extracting data from: {notebook}\")\n",
    "#        extracted_data = extract_code_and_context(notebook)\n",
    "#        all_extracted_data.extend(extracted_data)\n",
    "    \n",
    "#    print(f\"Extraction completed. Total notebooks processed: {len(notebooks)}\")\n",
    "#    return all_extracted_data\n",
    "\n",
    "# Example usage:\n",
    "#repo_url = \"https://github.com/passarel/crawler_data_source\"\n",
    "#extracted_notebooks_data = process_repo(repo_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7714591-6f9f-448f-a759-9bbff0ddd1b7",
   "metadata": {},
   "source": [
    "## Step 2: Generate metadata with llm  ðŸ”¢\n",
    "\n",
    "In this step, we use a language model (LLM) to generate descriptions and explanatory metadata for each extracted code snippet. The code performs the following operations:\n",
    "\n",
    "-  We define a prompt template that contains placeholders for the code snippet, the file name, and an optional context. The goal is for the model to provide a clear and concise explanation of what the code does, based on these three pieces of information.\n",
    "\n",
    "-  A PromptTemplate object is created from this template, allowing it to be used in conjunction with the language model.\n",
    "\n",
    "-  We use the OpenAI LLM, authenticated with an API key, to process the information and generate responses.\n",
    "\n",
    "- The function update_context_with_llm iterates through the data structure containing the extracted code, runs the language model for each item, and replaces the original context field with the explanation generated by the AI.\n",
    "\n",
    "- Finally, the data structure is updated with the new explanations, which are stored in the context field.\n",
    "\n",
    "-  The ultimate goal is to enrich the original data structure by providing clear explanations for each code snippet, making it easier to understand and use the information later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b30b2a9-59cd-4e71-b21e-2e61826ac357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a313f79-571e-4fec-b38c-ec94bb46551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\" #your api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9b18b4d-ff7f-44e0-a975-2477d3cabe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You will receive three pieces of information: a code snippet, a file name, and an optional context. Based on this information, explain in a clear, summarized and concise way what the code snippet is doing.\n",
    "\n",
    "Code:\n",
    "{code}\n",
    "\n",
    "File name:\n",
    "{filename}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Describe what the code above does.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cad693e1-eb5b-482e-95a4-5d83a6f0aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "\n",
    "llm_chain = prompt | llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213104ca-f28c-4d8b-b0db-579a6d0fc68e",
   "metadata": {},
   "source": [
    "### Generate metadata with llm local\n",
    "\n",
    "If you happen to be using a local model with LlamaCPP to generate metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d28bb47-0beb-40e6-85ac-c7797ba49412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Alternate code to load local models. \n",
    "###This specific example requires the project to have an asset call Llama7b, associated with the cloud S3 URI s3://dsp-demo-bucket/LLMs (public bucket)\n",
    "\n",
    "# from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "# from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# llm_local = LlamaCpp(\n",
    "            # model_path=\"/home/jovyan/datafabric/Llama7b/ggml-model-f16-Q5_K_M.gguf\",\n",
    "            # n_gpu_layers=64,\n",
    "            # n_batch=512,\n",
    "            # n_ctx=4096,\n",
    "            # max_tokens=1024,\n",
    "            # f16_kv=True,  \n",
    "            # callback_manager=callback_manager,\n",
    "            # verbose=False,\n",
    "            # stop=[],\n",
    "            # streaming=False,\n",
    "            # temperature=0.4,\n",
    "        # )\n",
    "\n",
    "# llm_chain = prompt | llm_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85ba720e-cea4-4535-805a-b20e7f717bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpcore\n",
    "\n",
    "def update_context_with_llm(data_structure):\n",
    "    updated_structure = []\n",
    "    \n",
    "    for item in data_structure:\n",
    "        code = item['code']\n",
    "        filename = item['filename']\n",
    "        context = item['context']\n",
    "        \n",
    "        try:\n",
    "            # Try calling an LLM to generate code explanation\n",
    "            response = llm_chain.invoke({\n",
    "                \"code\": code, \n",
    "                \"filename\": filename, \n",
    "                \"context\": context\n",
    "            })\n",
    "            \n",
    "            # Update item with LLM response\n",
    "            item['context'] = response.strip()\n",
    "            \n",
    "            # Print message indicating context was updated\n",
    "            #print(f\"Context generated for file {filename}: {item['context']}\")\n",
    "\n",
    "        except httpcore.ConnectError as e:\n",
    "            # API or model connection specific error\n",
    "            print(f\"Connection error processing file {filename}:The connection to the API or model has been corrupted. Details: {str(e)}\")\n",
    "            # Keep the original context in case of error\n",
    "            item['context'] = context\n",
    "        \n",
    "        except httpcore.ProtocolError as e:\n",
    "            # Protocol error, similar to the original error mentioned\n",
    "            print(f\"Protocol error when processing the file {filename}: {str(e)}\")\n",
    "            # Keep the original context\n",
    "            item['context'] = context\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Other general errors\n",
    "            print(f\"Error processing the file {filename}: {str(e)}\")\n",
    "            # Keep the original context\n",
    "            item['context'] = context\n",
    "        \n",
    "        # Add the updated item (or not) to the structure\n",
    "        updated_structure.append(item)\n",
    "    \n",
    "    return updated_structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9e8379a-6dc8-4f99-9f42-648ec2cbd629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "updated_data = update_context_with_llm(extracted_notebooks_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "311822ae-8d0b-4077-87ab-d2fed0b2f4d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '97f7b0f9-7b8c-4fa5-8869-867a35c4534a',\n",
       "  'embedding': None,\n",
       "  'code': '!pip install datasets # This one is for downloading our samsum dataset direclty from Hugging Face\\n!pip install peft # Both peft and trl are the libs that help us \\n!pip install trl # to configure our training methods and params\\n!pip install bitsandbytes # This one will help us to quantize the model\\n!pip install mlflow==2.11.0',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet installs several libraries needed for fine-tuning a model. Specifically, it installs datasets, peft, trl, bitsandbytes, and mlflow version 2.11.0. These libraries will be used to configure the training methods and parameters, quantize the model, and download the samsum dataset directly from Hugging Face. The file name fine-tuning-4bits.ipynb suggests that this code is used for fine-tuning a model with 4-bit precision.'},\n",
       " {'id': '88ae9f75-54c4-48b2-ac96-cef2535bb452',\n",
       "  'embedding': None,\n",
       "  'code': 'from datasets import load_dataset, Dataset, DatasetDict\\nimport os\\nimport json\\nimport re\\nfrom pprint import pprint\\nimport pandas as pd\\nimport torch\\nfrom datasets import Dataset, load_dataset\\nfrom huggingface_hub import notebook_login\\nfrom peft import LoraConfig, PeftModel, AutoPeftModelForCausalLM\\nfrom transformers import (\\n    BitsAndBytesConfig,\\n    TrainingArguments,\\n)\\nfrom trl import SFTTrainer\\nimport time\\nimport mlflow',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet is importing various libraries and modules, including datasets, os, json, re, pprint, pandas, torch, huggingface_hub, peft, and transformers. These libraries will be used for data manipulation, model training, and evaluation. The file name is fine-tuning-4bits.ipynb, suggesting that the code is used for fine-tuning a model with 4-bit precision. The context mentions that the libraries are already installed in a Deep Learning workspace and that the transformers library was installed with additional libraries. This suggests that the code is being used in a specific environment for deep learning tasks.'},\n",
       " {'id': '20216c09-0ae8-4feb-9cd9-7423d440fc50',\n",
       "  'embedding': None,\n",
       "  'code': '# \\'cuda:0\\' means that we want to use our GPU, if available. If not, uses CPU.\\nDEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\\n# Model name defines that we are using llama 2 with 7B parameters\\n# MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet defines a device and a model for fine-tuning. The device is set to \"cuda:0\" if a GPU is available, otherwise it is set to \"cpu\". The model name is set to \"meta-llama/Llama-2-7b-hf\", indicating that the model being used is llama 2 with 7 billion parameters. This code is likely part of a larger project or notebook called \"fine-tuning-4bits.ipynb\".'},\n",
       " {'id': '5b448fbd-4fd0-4983-a961-98640d125a17',\n",
       "  'embedding': None,\n",
       "  'code': 'HF_TOKEN = \"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\"',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet creates a variable called HF_TOKEN and assigns it a Hugging Face token, which is a string of characters used for authentication. The file name suggests that this code is used for fine-tuning a machine learning model with 4-bit precision.'},\n",
       " {'id': 'c411f0d3-d011-43eb-800a-1313091e6762',\n",
       "  'embedding': None,\n",
       "  'code': 'dataset = load_dataset(\"samsum\")',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet loads a dataset called \"samsum\" using a function called \"load_dataset\". The file name is \"fine-tuning-4bits.ipynb\" and it is likely being used to fine-tune or adjust a machine learning model using the loaded dataset. The context indicates that the dataset is being described or discussed in more detail.'},\n",
       " {'id': 'c33a2330-696f-4f55-88c6-f8daa6517130',\n",
       "  'embedding': None,\n",
       "  'code': \"def format_dialogue(dialogue):\\n    # Replace the '\\\\r\\\\n' with '\\\\n' to match the desired output format.\\n    formatted_dialogue = re.sub(r'\\\\r\\\\n', '\\\\n', dialogue)\\n    return formatted_dialogue\",\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet is a function called \"format_dialogue\" that takes in a string parameter called \"dialogue\". It uses the regular expression library \"re\" to replace any instances of \"\\\\r\\\\n\" (carriage return and line feed characters) in the dialogue with just \"\\\\n\" (line feed character). This is done to match a desired output format. The function then returns the formatted dialogue. The file name is \"fine-tuning-4bits.ipynb\" and it is likely related to fine-tuning a dataset for a machine learning model.'},\n",
       " {'id': '355cdc1c-5202-4b02-aa2c-4ff1de485a47',\n",
       "  'embedding': None,\n",
       "  'code': 'dataset',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet simply refers to a dataset, which is a collection of data used for analysis or machine learning. The file name suggests that the dataset is being used for fine-tuning a model with 4-bit precision, and the context may provide additional information or instructions for using the dataset.'},\n",
       " {'id': 'cd507d76-4884-4d15-9801-910bf26223dd',\n",
       "  'embedding': None,\n",
       "  'code': \"dataset['train'][0]\",\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet is accessing the first element of the training data in the dataset. This is likely part of a larger code file called \"fine-tuning-4bits.ipynb\", which could be used for fine-tuning a machine learning model. The context suggests that the code is being used to get a glimpse of the training data.'},\n",
       " {'id': '49cc607b-4fdd-478e-8ba4-0f786981f0ec',\n",
       "  'embedding': None,\n",
       "  'code': \"dataset['train'][0]['dialogue']\",\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet is accessing the first dialogue from the training dataset, which is stored as a nested dictionary. The file name suggests that this code is part of a fine-tuning process, likely for a machine learning model. The optional context mentions a datapoint, indicating that this code is used to retrieve specific data from the dataset for further processing.'},\n",
       " {'id': 'b22e80a8-ebe2-4d86-8eae-af26d15daf95',\n",
       "  'embedding': None,\n",
       "  'code': \"def format_dialogue(dialogue):\\n    # Replace the '\\\\r\\\\n' with '\\\\n' to match the desired output format.\\n    formatted_dialogue = re.sub(r'\\\\r\\\\n', '\\\\n', dialogue)\\n    return formatted_dialogue\",\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet defines a function called \"format_dialogue\" that takes in a string value called \"dialogue\". The function uses the regular expression (re) library to replace any instances of \"\\\\r\\\\n\" (carriage return and new line characters) with just \"\\\\n\" (new line character) in the dialogue. This is done to match the desired output format. The function then returns the formatted dialogue. \\n\\nThe file name associated with this code is \"fine-tuning-4bits.ipynb\", which suggests that it is part of a notebook used for fine-tuning or tweaking a specific aspect of a program or model that deals with 4-bit information. \\n\\nThe context provided explains that the code is used to clean up the formatting of dialogues, specifically replacing \"\\\\r\" characters with \"\\\\n\" characters. This is likely done in order to make the dialogues more readable and organized. The code is intended to be used in a larger function that formats dialogues automatically.'},\n",
       " {'id': '3945f3e1-60a2-40dd-858a-b687290e7b26',\n",
       "  'embedding': None,\n",
       "  'code': \"print(format_dialogue(dataset['train'][0]['dialogue']))\",\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code above is using the function \"format_dialogue\" to print the dialogue from the first train dialogue in the file \"fine-tuning-4bits.ipynb\" in a specific format. This is being done as a part of testing the function.'},\n",
       " {'id': '3fc22f3a-af9d-4bc5-a22e-1c10b9433211',\n",
       "  'embedding': None,\n",
       "  'code': 'DEFAULT_SYSTEM_PROMPT = \"\"\"\\nBelow is a conversation between friends in a chat. Write a summary of their conversation.\\n\"\"\".strip()',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet defines a default system prompt for a chat conversation between friends. The file name suggests that the code is used for fine-tuning a 4-bit system, and the context indicates that it is defining an input prompt. In summary, the code is setting a default prompt for a chat conversation in a file used for fine-tuning a 4-bit system.'},\n",
       " {'id': 'bb3796b4-dc93-4aad-8758-4a1b13e6f168',\n",
       "  'embedding': None,\n",
       "  'code': 'def generate_training_prompt(\\n    conversation: str, summary: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT\\n) -> str:\\n    return f\"\"\"### Instruction: {system_prompt}\\n\\n### Input:\\n{conversation.strip()}\\n\\n### Response:\\n{summary}\\n\"\"\".strip()',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet defines a function called \"generate_training_prompt\" that takes in a conversation string, a summary string, and an optional system prompt string. It then formats and returns a string that includes the system prompt, the conversation, and the summary. The file name \"fine-tuning-4bits.ipynb\" suggests that this code may be related to fine-tuning a machine learning model using 4-bit data. The optional context provides additional information about the purpose of the code, specifically that it is defining an input prompt.'},\n",
       " {'id': '8601be10-fd02-4047-b17c-9886c1bd71db',\n",
       "  'embedding': None,\n",
       "  'code': 'def generate_text(data_point):\\n    summary = data_point[\\'summary\\']\\n    conversation_text = format_dialogue(data_point[\\'dialogue\\'])\\n    return {\\n        \"conversation\": conversation_text,\\n        \"summary\": summary,\\n        \"text\": generate_training_prompt(conversation_text, summary),\\n    }',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code above defines a function called \"generate_text\" that takes in a data point as input. It then extracts the summary and dialogue information from the data point and formats the dialogue. Finally, it returns a dictionary containing the conversation text, summary, and a training prompt generated from the conversation text and summary. The file name suggests that this code may be used for fine-tuning a model with 4-bit input data, and the optional context may provide additional information on how this code is being used in a larger project or task.'},\n",
       " {'id': '3caf6e06-01c2-4329-be84-21113dc9fcfa',\n",
       "  'embedding': None,\n",
       "  'code': 'def process_dataset(data: Dataset):\\n    return (\\n        data.shuffle(seed=42)\\n        .map(generate_text))',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet defines a function named \"process_dataset\" that takes in a dataset as input. This dataset is then shuffled with a seed value of 42 and then mapped to generate text. The file name \"fine-tuning-4bits.ipynb\" suggests that this code may be used for fine-tuning a model with 4-bit precision. The context suggests that this code may be used for defining an input prompt for the model.'},\n",
       " {'id': 'a9c49cf2-1437-4ba2-acdd-b356aff71c93',\n",
       "  'embedding': None,\n",
       "  'code': '# dataset[\"train\"] = process_dataset(dataset[\"train\"])\\ndataset[\"train\"] = process_dataset(dataset[\"train\"].select(range(1500)))\\ndataset[\"test\"] = process_dataset(dataset[\"test\"])',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet is processing a dataset by calling a function called \"process_dataset\" on the \"train\" and \"test\" subsets of the dataset. The function is being called with a parameter, which is the result of selecting a range of 1500 data points from the \"train\" subset. This code is likely part of a larger file called \"fine-tuning-4bits.ipynb\" which is performing some sort of fine-tuning on the dataset.'},\n",
       " {'id': '4c65ecf8-1879-4694-bd00-46169c59dbc6',\n",
       "  'embedding': None,\n",
       "  'code': 'notebook_login()',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet is calling a function called \"notebook_login\" which may be used for logging into a notebook. The file name is \"fine-tuning-4bits.ipynb\" which suggests that the code is related to fine-tuning a model or algorithm with 4-bit precision. The context of \"instantiating the model and its tokenizer\" indicates that the code is creating an instance of a model and its corresponding tokenizer, possibly for the purpose of fine-tuning.'},\n",
       " {'id': '656bfa10-71db-42f4-baf5-9501a043742d',\n",
       "  'embedding': None,\n",
       "  'code': 'from transformers import AutoTokenizer, AutoModelForCausalLM',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet imports the libraries \"transformers\" and \"AutoTokenizer, AutoModelForCausalLM\". It then uses these libraries to perform quantization, tokenization, and model training. The file name indicates that this code is used for fine-tuning a model with 4-bit precision. The optional context suggests that this code may be part of a larger project or notebook.'},\n",
       " {'id': '24719aa0-a35e-40eb-b2d7-05aedd4d7ad4',\n",
       "  'embedding': None,\n",
       "  'code': 'def create_model_and_tokenizer():\\n    bnb_config = BitsAndBytesConfig( #BitsAndBytes is the reponsable librarie that help us to configure the desired quantization.\\n        load_in_4bit=True, # In this case, we are loading the model in a 4bir precision instead of its orginal precision.\\n        bnb_4bit_quant_type=\"nf4\", # normalized float 4 bit data type\\n        bnb_4bit_compute_dtype=torch.float16, # This is to use half the memory and fit the model\\n    )\\n\\n    model = AutoModelForCausalLM.from_pretrained( \\n        \"google/gemma-2b\", # Here we are defining the llama 2 7B model\\n        use_safetensors=True, #  for storing and loading tensors\\n        quantization_config=bnb_config, # with the desired quantization \\n        trust_remote_code=True, \\n        device_map=\"auto\" # and put each layer of the model depending on the available resources\\n        \\n    )\\n\\n    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\") # Here we tell we want to use the same tokenizer that the model ises\\n    tokenizer.pad_token = tokenizer.eos_token # Telling that all the padding tokens should be the same as the \\'end of sentence\\'\\n    tokenizer.padding_side = \"right\" # and the side padding is the right side\\n\\n    return model, tokenizer',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': \"The code snippet is creating a model and tokenizer for fine-tuning a neural network using quantization. The BitsAndBytesConfig class from the BitsAndBytes library is used to configure the desired quantization, specifically loading the model in 4-bit precision. The model being used is the llama 2 7B model from Google's Gemma-2b library. The tokenizer is also being configured to use the same tokenizer as the model and to pad tokens on the right side using the end of sentence token. The code is returning the model and tokenizer for further use in fine-tuning. The file name suggests that this code is being used for fine-tuning a model using 4-bit precision.\"},\n",
       " {'id': '7d258f15-b376-4295-a3e2-85c05171188a',\n",
       "  'embedding': None,\n",
       "  'code': 'model, tokenizer = create_model_and_tokenizer()\\nmodel.config.use_cache = False # The cache is only used for generation, not for training',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet is creating a model and tokenizer object and setting the configuration of the model to not use cache for training, but only for generation. This code is likely used for fine-tuning a model for a specific task, and the file name suggests it is using 4-bit precision for this fine-tuning. The context mentions that the code may take a few minutes to run, indicating that it may be a resource-intensive process.'},\n",
       " {'id': 'ada40f1f-fe37-4c78-a0bd-eccaecdc9587',\n",
       "  'embedding': None,\n",
       "  'code': 'lora_r = 16 # Ranking of the matrix\\nlora_alpha = 64 # Scaling facotr\\nlora_dropout = 0.1\\nlora_target_modules = [ # Selecting what layers of the model we want to use\\n    \"q_proj\",\\n    \"up_proj\",\\n    \"o_proj\",\\n    \"k_proj\",\\n    \"down_proj\",\\n    \"gate_proj\",\\n    \"v_proj\",\\n]\\n\\n\\npeft_config = LoraConfig(\\n    r=lora_r,\\n    lora_alpha=lora_alpha,\\n    lora_dropout=lora_dropout,\\n    target_modules=lora_target_modules,\\n    bias=\"none\",\\n    task_type=\"CAUSAL_LM\",\\n)',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code defines variables for the ranking and scaling factor of a matrix, as well as the dropout rate and layers of a model to be used for fine-tuning. It then creates a LoRA configuration object with the specified parameters, including the type of task and bias to be used. This code is likely used in a Jupyter notebook file named \"fine-tuning-4bits.ipynb\" to implement the LoRA approach for efficient fine-tuning of LLMs.'},\n",
       " {'id': '322c2382-4877-4873-816b-1666724652dc',\n",
       "  'embedding': None,\n",
       "  'code': \"os.environ['MLFLOW_EXPERIMENT_NAME'] = 'gemma2b-summary_task-quant4bit-2-trial'\",\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': \"The code snippet sets the environment variable 'MLFLOW_EXPERIMENT_NAME' to 'gemma2b-summary_task-quant4bit-2-trial'. This allows for easy access to the experiment on the Monitor tab. The file name is 'fine-tuning-4bits.ipynb'. The context explains the purpose of defining a name for the experiment.\"},\n",
       " {'id': '141bd727-c3cb-4339-8efb-947a00ea40c0',\n",
       "  'embedding': None,\n",
       "  'code': 'training_arguments = TrainingArguments(\\n    per_device_train_batch_size=4, # The batch size per GPU/TPU core/CPU for training.\\n    gradient_accumulation_steps=4, # Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\\n    optim=\"paged_adamw_32bit\", #Adam Optimizer\\n    logging_steps=20, # Number of update steps between two logs if logging_strategy=\"steps\".\\n    save_steps = 20,\\n    learning_rate=1e-3, # The initial learning rate for AdamW optimizer\\n    fp16=True, # Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\\n    max_grad_norm=0.3, # Maximum gradient norm\\n    num_train_epochs=3, # Number of epochs\\n    evaluation_strategy=\"steps\", # Evaluation is done (and logged) every eval_steps\\n    eval_steps=0.2,\\n    warmup_ratio=0.05, # Ratio of total training steps used for a linear warmup from 0 to learning_rate\\n    save_strategy=\"steps\", # Save is done at the end of each steps\\n    group_by_length=True, # Whether or not to group together samples of roughly the same length in the training dataset \\n    output_dir=\\'./gemma/gemma-4bit-2\\', # Path to save model checkpoints and bins\\n    report_to=\"mlflow\", # Integration to report the results and logs to\\n    save_safetensors=True, # Use safetensors saving and loading for state dicts instead of default torch.load and torch.save\\n    lr_scheduler_type=\"cosine\", # The scheduler type to use.\\n    seed=42, # Random seed that will be set at the beginning of training. \\n) ',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet is creating an object called \"training_arguments\" which stores various arguments used for training a model. These include the batch size, number of gradient accumulation steps, optimizer type, logging and saving settings, learning rate, precision type, maximum gradient norm, number of training epochs, evaluation strategy, warmup ratio, save strategy, grouping of samples, output directory for saving checkpoints, integration for reporting results and logs, use of safe tensor saving and loading, type of learning rate scheduler, and a random seed. The file name indicates that this code may be used for fine-tuning a model using 4-bit precision.'},\n",
       " {'id': '0bedf01a-e8e1-41f1-bb38-7031daefc52b',\n",
       "  'embedding': None,\n",
       "  'code': 'trainer = SFTTrainer(\\n    model=model,\\n    train_dataset=dataset[\"train\"], # Placing our dataset on train\\n    eval_dataset=dataset[\"test\"], # and test part\\n    peft_config=peft_config, #LoRA\\n    dataset_text_field=\"text\",\\n    max_seq_length=4096, # Specifies the maximum number of tokens of the input\\n    tokenizer=tokenizer, # Model\\'s tokenizer we loaded before\\n    args=training_arguments,\\n)',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code is creating a trainer object for fine-tuning a model. It takes in the model, train and test datasets, configuration for the LoRA optimization algorithm, a text field from the dataset, and various training arguments. The file name suggests that the code is for fine-tuning a model with 4-bit parameters. The optional context suggests that there is more information available in the Trainer documentation.'},\n",
       " {'id': 'c1436701-29e6-47b8-be99-55d099285df5',\n",
       "  'embedding': None,\n",
       "  'code': 'start_time = time.time()\\ntrainer.train()\\nend_time = time.time()\\ntraining_duration = end_time - start_time\\nmlflow.log_metric(\"training_time\", training_duration)',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet measures the duration of the training process and logs it as a metric using MLflow. The file name suggests that this code is used for fine-tuning a model with 4-bit precision, and the context mentions it is the start of the training step.'},\n",
       " {'id': 'a5903401-1946-40e0-b368-c387551f3fd9',\n",
       "  'embedding': None,\n",
       "  'code': 'trainer.save_model()',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet is calling the function \"save_model()\" which saves the trained model. The file name is \"fine-tuning-4bits.ipynb\" which suggests that this code is part of a notebook used for fine-tuning a model with 4 bits. The optional context suggests that this code is used as the first step in the training process.'},\n",
       " {'id': 'c3592178-ddd2-4973-b2f4-c085e2c72fbd',\n",
       "  'embedding': None,\n",
       "  'code': 'from peft import PeftModel',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet loads a pre-trained model using the PeftModel class from the peft library. The file name is \"fine-tuning-4bits.ipynb\" and it is likely a Jupyter Notebook used for fine-tuning the model. The context suggests that the model is being loaded for further use.'},\n",
       " {'id': 'daa0c701-5f82-474e-9eb1-71d714a24d5e',\n",
       "  'embedding': None,\n",
       "  'code': \"model = PeftModel.from_pretrained(model, './gemma/gemma-4bit-2')\",\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet loads a trained model from the file named \"gemma-4bit-2\" using the PeftModel library and assigns it to a variable called \"model\". This model is then used for fine-tuning in the file named \"fine-tuning-4bits.ipynb\".'},\n",
       " {'id': 'b9487897-3423-4546-859a-840de0d33f5a',\n",
       "  'embedding': None,\n",
       "  'code': 'def generate_prompt(\\n    conversation: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT\\n) -> str:\\n    return f\"\"\"### Instruction: {system_prompt}\\n\\n### Input:\\n{conversation.strip()}\\n\\n### Response:\\n\"\"\".strip()',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code defines a function called \"generate_prompt\" which takes in two parameters, \"conversation\" and \"system_prompt\" with a default value of \"DEFAULT_SYSTEM_PROMPT\". The function then returns a formatted string that includes the system prompt, input (conversation), and a response. The code also includes a file name \"fine-tuning-4bits.ipynb\" and a context which is to create a test dataset for the model. Overall, the code snippet is likely used to generate prompts for a conversation-based model.'},\n",
       " {'id': '4d577fb0-9336-4a59-955e-e181405c1cf6',\n",
       "  'embedding': None,\n",
       "  'code': 'examples = []\\n\\nfor data_point in dataset[\"test\"].select(range(5)):\\n    summary = data_point[\\'summary\\']\\n    conversation = data_point[\\'dialogue\\']\\n    examples.append(\\n        {\\n            \"summary\": summary,\\n            \"conversation\": conversation,\\n            \"prompt\": generate_prompt(conversation),\\n        }\\n    )\\ntest_df = pd.DataFrame(examples)',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet is creating a test dataset by selecting the first 5 data points from a larger dataset. It then extracts the summary and conversation data from each data point and creates a new dictionary with these values, along with a generated prompt. This dictionary is appended to a list called \"examples\". Finally, the code creates a pandas DataFrame from the \"examples\" list, using the data to be used for fine-tuning a model. The file name indicates that this code is used for fine-tuning a model with 4-bit data. The context suggests that this code is used to create a small test dataset for easily testing the model.'},\n",
       " {'id': 'd2b06d11-2810-4f91-bdf3-c09b1b7ec082',\n",
       "  'embedding': None,\n",
       "  'code': 'test_df',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet is creating a variable named \"test_df\", which is most likely a data frame or table structure. The file name associated with this code is \"fine-tuning-4bits.ipynb\", suggesting that the code is related to fine-tuning a machine learning model with 4-bit precision. The context mentions a function that helps tokenize input data and use the model, indicating that the code may be part of a larger process for preparing and utilizing data in a machine learning context.'},\n",
       " {'id': '42e47e32-d945-424f-824a-cbfdbff0c456',\n",
       "  'embedding': None,\n",
       "  'code': 'def summarize(model, text: str):\\n    inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\\n    inputs_length = len(inputs[\"input_ids\"][0])\\n    with torch.inference_mode():\\n        # Adjust temperature to a more stable value\\n        outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.7)\\n    return tokenizer.decode(outputs[0][inputs_length:], skip_special_tokens=True)',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet is a function that takes in a model and a text string as input. It uses a tokenizer to convert the text into a format suitable for the model and then sets the input length. The model then uses this tokenized input to generate a new sequence of tokens, with a specified maximum length and temperature. The function then decodes the generated tokens and returns the resulting string. This code is likely used for fine-tuning a model for natural language processing tasks. The \"fine-tuning-4bits.ipynb\" file name suggests that this function may be part of a notebook used for fine-tuning a model with 4-bit precision. The context mentions a function for tokenizing inputs, which could be used in conjunction with this function to prepare input data for the model.'},\n",
       " {'id': '60b5607f-ee6c-4db5-a546-1a0533d35e9a',\n",
       "  'embedding': None,\n",
       "  'code': 'import numpy as np\\nnp.random.seed(28)',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet imports the numpy library and sets a random seed value to 28. The file name suggests that it is related to fine-tuning a machine learning model with 4-bit precision. The context may provide further details on how the numpy library and random seed value are used in the fine-tuning process.'},\n",
       " {'id': '50367dd7-12ff-43a1-851d-913d4d720ffb',\n",
       "  'embedding': None,\n",
       "  'code': 'example = test_df.iloc[0]\\nprint(example.conversation)',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': \"The code snippet is selecting the first row of data from a dataframe called test_df and assigning it to a variable called 'example'. It then prints the 'conversation' column from the selected row. The file name suggests that this code may be used for fine-tuning a machine learning model with 4-bit precision.\"},\n",
       " {'id': '49488df9-8c17-4727-9f09-41f42647a403',\n",
       "  'embedding': None,\n",
       "  'code': 'print(example.summary)',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet is using the \"print\" function to display the summary of an example. The file name is \"fine-tuning-4bits.ipynb\" and it is likely a notebook file used for fine-tuning a model. The optional context is discussing inferences, so the code may be displaying the summary of the inferences made.'},\n",
       " {'id': '91885cdc-76a0-45b7-b28c-ec89483bf137',\n",
       "  'embedding': None,\n",
       "  'code': 'print(example.prompt)',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet is using the print function to display the variable \"example.prompt\" in the output. The file name suggests that the code is related to fine-tuning a machine learning model with 4-bit precision, and the optional context suggests that the code may be used for making inferences based on the model. Overall, the code is likely displaying the prompt for a fine-tuning process related to a machine learning model with 4-bit precision.'},\n",
       " {'id': 'f9227d83-c302-4c0e-9054-1fd17843a80d',\n",
       "  'embedding': None,\n",
       "  'code': '%%time\\nsummary = summarize(model, example.prompt)',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet is using the \"summarize\" function from the model to generate a summary of the example prompt. It is also measuring the time it takes to run this operation using the \"%%time\" magic command. This code is likely part of a larger fine-tuning process, as indicated by the file name \"fine-tuning-4bits.ipynb\". The context suggests that this code is performing inferences, or making predictions, based on the model.'},\n",
       " {'id': '1e935e93-f119-4c2a-af33-dc0cd35b8c04',\n",
       "  'embedding': None,\n",
       "  'code': 'print(summary)',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet is using the \"print\" function to display a summary of information. The file name is called \"fine-tuning-4bits.ipynb\" and it is most likely a notebook file. The context is likely a section or heading within the notebook file, and it is discussing the process of making inferences. Overall, the code is displaying a summary of information related to fine-tuning and making inferences in a notebook file.'},\n",
       " {'id': '37cb2f77-e0e5-4112-9588-ee704f9adb89',\n",
       "  'embedding': None,\n",
       "  'code': 'from peft import PeftModel, PeftConfig\\nfrom transformers import AutoModelForCausalLM\\n\\nconfig = PeftConfig.from_pretrained(\"morgana-rodrigues/gemma-2b-4bit\")\\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\\nmodel = PeftModel.from_pretrained(model, \"morgana-rodrigues/gemma-2b-4bit\")',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet is importing the necessary modules and libraries for fine-tuning a pre-trained PeftModel and PeftConfig. It then specifies the configuration and model to be used for fine-tuning, both of which are from the \"morgana-rodrigues/gemma-2b-4bit\" repository. Finally, the code creates a PeftModel from the pre-trained model and saves it in the specified file \"fine-tuning-4bits.ipynb\". This process allows for the model to be fine-tuned for a specific task or dataset.'},\n",
       " {'id': '9c91f51c-bddf-4149-8ec8-09f23808491b',\n",
       "  'embedding': None,\n",
       "  'code': 'import os\\n\\ndef get_size(start_path):\\n    total_size = 0\\n    for dirpath, dirnames, filenames in os.walk(start_path):\\n        for f in filenames:\\n            fp = os.path.join(dirpath, f)\\n            # skip if it is symbolic link\\n            if not os.path.islink(fp):\\n                total_size += os.path.getsize(fp)\\n\\n    return total_size\\n\\n# Caminhos dos diretÃ³rios do modelo\\nmodel_4bit_dir = \\'/home/jovyan/.cache/huggingface/hub/models--morgana-rodrigues--gemma-2b-4bit\\'\\n\\n# ObtÃ©m o tamanho do diretÃ³rio do modelo quantizado de 8 bits\\nsize_quant_8bit = get_size(model_8bit_dir)\\n\\nprint(f\"Tamanho do modelo quantizado de 8 bits: {size_quant_8bit} bytes\")',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet uses the os module to get the size of a directory. It first defines a function called \"get_size\" that takes in a \"start_path\" parameter. It then uses the os.walk() method to iterate through all the files and directories in the given start path. For each file, it checks if it is a symbolic link and if not, it adds its size to the total size. Finally, it returns the total size of the directory. The file name \"fine-tuning-4bits.ipynb\" suggests that this code is used for fine-tuning a model with 4-bit precision. The context mentions the availability of the model on the HuggingFace library.'},\n",
       " {'id': '66d19978-f741-445f-b2bd-211241834fea',\n",
       "  'embedding': None,\n",
       "  'code': 'import sys\\n\\n# Example variable\\nmy_list = model_4bit_dir\\n# Checking the size of the variable\\nsize_in_bytes = sys.getsizeof(my_list)\\n\\nprint(f\"The variable is using {size_in_bytes} bytes of memory.\")',\n",
       "  'filename': 'fine-tuning-4bits.ipynb',\n",
       "  'context': 'The code snippet imports the \"sys\" library and assigns a variable \"my_list\" to a model named \"model_4bit_dir\". It then uses the \"sys.getsizeof()\" function to check the size of the \"my_list\" variable in bytes. Finally, it prints a message stating the amount of memory that the variable is using. This code may be part of a larger script or notebook file named \"fine-tuning-4bits.ipynb\" and could potentially be used to fine-tune a model available on the HuggingFace library.'},\n",
       " {'id': 'ed8ed12f-47e1-4d65-a8dc-55436b2f2bbf',\n",
       "  'embedding': None,\n",
       "  'code': '!pip install datasets # This one is for downloading our samsum dataset direclty from Hugging Face\\n!pip install peft # Both peft and trl are the libs that help us \\n!pip install trl # to configure our training methods and params\\n!pip install bitsandbytes # This one will help us to quantize the model\\n!pip install mlflow==2.11.0',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': \"The code snippet is installing multiple libraries, including datasets, peft, trl, bitsandbytes, and mlflow, using pip. These libraries will be used for fine-tuning a model on a file named 'fine-tuning-fullprec.ipynb'. The context suggests that the file will be used for training with specific methods and parameters, and the model will be quantized using bitsandbytes.\"},\n",
       " {'id': '17dac69e-1390-450b-b771-8eb33fff38f8',\n",
       "  'embedding': None,\n",
       "  'code': 'from datasets import load_dataset, Dataset, DatasetDict\\nimport os\\nimport json\\nimport re\\nfrom pprint import pprint\\nimport pandas as pd\\nimport torch\\nfrom datasets import Dataset, load_dataset\\nfrom huggingface_hub import notebook_login\\nfrom peft import LoraConfig, PeftModel, AutoPeftModelForCausalLM\\nfrom transformers import (\\n    BitsAndBytesConfig,\\n    TrainingArguments,\\n)\\nfrom trl import SFTTrainer\\nimport time\\nimport mlflow',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet is importing various libraries, including datasets, os, json, re, pprint, pandas, torch, and transformers. It then uses these libraries to load and manipulate datasets, login to HuggingFace Hub, and fine-tune a model (specifically a PeftModel and AutoPeftModelForCausalLM) with SFTTrainer and save the results using mlflow. The file name suggests that this code is for fine-tuning a model with full precision. The context mentions that the code is used in a Deep Learning workspace and that the transformers library was installed separately.'},\n",
       " {'id': '5f665a3f-b294-4966-9903-7fee11ca6c0e',\n",
       "  'embedding': None,\n",
       "  'code': '# \\'cuda:0\\' means that we want to use our GPU, if available. If not, uses CPU.\\nDEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\\n# Model name defines that we are using llama 2 with 7B parameters\\n# MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet defines the device as either \"cuda:0\" or \"cpu\" depending on the availability of a GPU. It also sets the model name as \"meta-llama/Llama-2-7b-hf\". The file name is \"fine-tuning-fullprec.ipynb\" and it is related to defining the device and model for use in a larger code or project.'},\n",
       " {'id': '36d41e4b-7103-4b77-ad03-5d0aa703ab8a',\n",
       "  'embedding': None,\n",
       "  'code': 'HF_TOKEN = \"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\"',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet assigns a Hugging Face token to the variable \"HF_TOKEN\". The file name indicates that it is used for fine-tuning and is likely related to natural language processing tasks. The context suggests that the token is used for authentication or authorization purposes.'},\n",
       " {'id': 'd81050b6-30a3-4806-aaea-2fb630314e05',\n",
       "  'embedding': None,\n",
       "  'code': 'dataset = load_dataset(\"samsum\")',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet loads a dataset called \"samsum\" using the function \"load_dataset\". The file name is \"fine-tuning-fullprec.ipynb\" and the code is likely part of a larger project involving fine-tuning a model using the samsum dataset. The context indicates that the dataset is being described.'},\n",
       " {'id': '39f618a5-580e-41ad-8bc0-db341039a2fc',\n",
       "  'embedding': None,\n",
       "  'code': \"def format_dialogue(dialogue):\\n    # Replace the '\\\\r\\\\n' with '\\\\n' to match the desired output format.\\n    formatted_dialogue = re.sub(r'\\\\r\\\\n', '\\\\n', dialogue)\\n    return formatted_dialogue\",\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet is defining a function called \"format_dialogue\" that takes in a variable called \"dialogue\". Within the function, the \"formatted_dialogue\" variable is created by using regular expressions to replace any instances of \\'\\\\r\\\\n\\' with \\'\\\\n\\'. The function then returns the updated \"formatted_dialogue\" variable. This code is likely used to clean up and standardize the formatting of dialogue in a dataset. The code is located in a file named \"fine-tuning-fullprec.ipynb\", which suggests that it is part of a larger project or notebook focused on fine-tuning a model using a full precision dataset.'},\n",
       " {'id': 'bcaa888f-08df-4cbb-9bbf-26ac321ac54b',\n",
       "  'embedding': None,\n",
       "  'code': 'dataset',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet is simply declaring a variable named \"dataset\". It is not performing any specific action with the variable, but it may be used to store or manipulate data in the future. The file name suggests that this code may be used for fine-tuning a model or algorithm, and the optional context of \"dataset\" suggests that the code may be used to load or process a specific data set for this purpose.'},\n",
       " {'id': '4d6b27a8-f0c8-44d2-8aaf-7571e681bd63',\n",
       "  'embedding': None,\n",
       "  'code': \"dataset['train'][0]\",\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': \"The code snippet is accessing the first element (index 0) in the 'train' dataset. The file name suggests that this code is used for fine-tuning a neural network model, and the 'fullprec' part indicates that the model is trained using full precision (as opposed to mixed or reduced precision). The code is most likely used to extract a sample from the training set for further analysis or model optimization.\"},\n",
       " {'id': 'd7e15083-ff48-476b-baea-ae981bd86f9d',\n",
       "  'embedding': None,\n",
       "  'code': \"dataset['train'][0]['dialogue']\",\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': \"The code snippet accesses the dialogue from the first datapoint in the 'train' section of the dataset. The file name suggests that this code is used for fine-tuning a model and the context provides additional information about the specific data being accessed.\"},\n",
       " {'id': '80f54972-90dc-4209-84dc-a13494499112',\n",
       "  'embedding': None,\n",
       "  'code': \"def format_dialogue(dialogue):\\n    # Replace the '\\\\r\\\\n' with '\\\\n' to match the desired output format.\\n    formatted_dialogue = re.sub(r'\\\\r\\\\n', '\\\\n', dialogue)\\n    return formatted_dialogue\",\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': \"The code snippet creates a function called format_dialogue that takes in a dialogue as a parameter. It uses the re.sub() function to replace any instances of '\\\\r\\\\n' (carriage return and line feed) in the dialogue with just '\\\\n' (line feed) to match the desired output format. The function then returns the newly formatted dialogue. This code is likely used to clean up dialogues in a file or program. The file name is fine-tuning-fullprec.ipynb and the context mentions using the function to format dialogues that contain '\\\\r' characters.\"},\n",
       " {'id': '44df9184-fe42-4ec7-98e8-475286e24a05',\n",
       "  'embedding': None,\n",
       "  'code': \"print(format_dialogue(dataset['train'][0]['dialogue']))\",\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet is using the \"print\" function to display the output of the \"format_dialogue\" function on the first dialogue in the train dataset. The file name indicates that this code is part of a larger notebook used for fine-tuning a model. The context suggests that this code is being used to test the functionality of the \"format_dialogue\" function.'},\n",
       " {'id': '959d3e0b-16d1-499f-8921-de3421ad0cd3',\n",
       "  'embedding': None,\n",
       "  'code': 'DEFAULT_SYSTEM_PROMPT = \"\"\"\\nBelow is a conversation between friends in a chat. Write a summary of their conversation.\\n\"\"\".strip()',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet is creating a default system prompt, which is a message that will be displayed in a chat between friends. The prompt is defined by the conversation that follows it. The file name indicates that this code may be used for fine-tuning a larger program. The context of the code is defining an input prompt, which is a message that prompts the user for input in a chat or messaging application.'},\n",
       " {'id': 'a528e9a4-c8d6-4112-8392-ab2e43347025',\n",
       "  'embedding': None,\n",
       "  'code': 'def generate_training_prompt(\\n    conversation: str, summary: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT\\n) -> str:\\n    return f\"\"\"### Instruction: {system_prompt}\\n\\n### Input:\\n{conversation.strip()}\\n\\n### Response:\\n{summary}\\n\"\"\".strip()',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet is a function named \"generate_training_prompt\" that takes in three parameters: conversation, summary, and system_prompt. The function returns a string that contains instructions and input/output prompts, using the system_prompt as a default value if none is provided. The file name is \"fine-tuning-fullprec.ipynb\" and this code is likely used for defining input prompts in a larger project or notebook.'},\n",
       " {'id': '205ec7a0-1b95-4569-bf7c-70c78f7af15d',\n",
       "  'embedding': None,\n",
       "  'code': 'def generate_text(data_point):\\n    summary = data_point[\\'summary\\']\\n    conversation_text = format_dialogue(data_point[\\'dialogue\\'])\\n    return {\\n        \"conversation\": conversation_text,\\n        \"summary\": summary,\\n        \"text\": generate_training_prompt(conversation_text, summary),\\n    }',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code creates a function called generate_text that takes in a data point as an argument. It then extracts the summary and dialogue from the data point and uses them to generate a conversation and a training prompt. The conversation and summary are returned along with the training prompt in a dictionary. This code is likely used for natural language processing tasks such as text generation or summarization. The file name suggests that the code is used for fine-tuning a model with full-precision data.'},\n",
       " {'id': '67f293f8-8b57-4a5e-949f-f8728040010c',\n",
       "  'embedding': None,\n",
       "  'code': 'def process_dataset(data: Dataset):\\n    return (\\n        data.shuffle(seed=42)\\n        .map(generate_text))',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code defines a function called \"process_dataset\" that takes in a dataset as input. The dataset is then shuffled using a seed of 42 and mapped to generate text. The file name is \"fine-tuning-fullprec.ipynb\" and the code is likely used for fine-tuning a machine learning model using a full precision approach. The context mentions defining an input prompt, indicating that the code is likely used to preprocess data for training a language model.'},\n",
       " {'id': '07569e88-d222-4fb3-b914-71b884158550',\n",
       "  'embedding': None,\n",
       "  'code': '# dataset[\"train\"] = process_dataset(dataset[\"train\"])\\ndataset[\"train\"] = process_dataset(dataset[\"train\"].select(range(1500)))\\ndataset[\"test\"] = process_dataset(dataset[\"test\"])',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet is part of a larger file named \"fine-tuning-fullprec.ipynb\" and is used to process a dataset. It takes a subset of the dataset called \"train\" and applies a function called \"process_dataset\" to it, selecting only the first 1500 entries. This processed subset is then assigned back to the \"train\" subset. The same function is applied to the \"test\" subset of the dataset as well. Overall, the code is preparing the dataset for further use, possibly for training a machine learning model.'},\n",
       " {'id': 'c5171bde-454b-4621-96d9-f234b6550adf',\n",
       "  'embedding': None,\n",
       "  'code': 'notebook_login()',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet calls the function \"notebook_login\" to perform a login action. The file name is \"fine-tuning-fullprec.ipynb\", which suggests that the code may be used for fine-tuning a machine learning model. The optional context mentions instantiating the model and its tokenizer, indicating that the code is likely creating an instance of a model and its associated tokenizer for further use.'},\n",
       " {'id': '519538a5-c51e-485c-bf96-2f4a9dd68695',\n",
       "  'embedding': None,\n",
       "  'code': 'from transformers import AutoTokenizer, AutoModelForCausalLM',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet imports the AutoTokenizer and AutoModelForCausalLM classes from the transformers library. These classes are used for fine-tuning a pre-trained language model. The code also specifies a file name, \"fine-tuning-fullprec.ipynb\", which is likely the notebook used for fine-tuning the model. The optional context mentions \"Quantization\", which could refer to the process of reducing the precision of the model\\'s parameters to improve efficiency. The code snippet is likely used for fine-tuning a pre-trained language model with the option to use quantization.'},\n",
       " {'id': 'b5c4dfa5-494e-4ce5-a701-290c4138894e',\n",
       "  'embedding': None,\n",
       "  'code': 'def create_model_and_tokenizer():\\n    # bnb_config = BitsAndBytesConfig( #BitsAndBytes is the reponsable librarie that help us to configure the desired quantization.\\n    #     load_in_4bit=True, # In this case, we are loading the model in a 4bir precision instead of its orginal precision.\\n    #     bnb_4bit_quant_type=\"nf4\", # normalized float 4 bit data type\\n    #     bnb_4bit_compute_dtype=torch.float16, # This is to use half the memory and fit the model\\n    # )\\n    # bnb_config_8 = BitsAndBytesConfig( #BitsAndBytes is the reponsable librarie that help us to configure the desired quantization.\\n    #     load_in_8_bit=True,\\n    #     bnb_4bit_quant_type=\"nf4\", # normalized float 4 bit data type\\n    #     bnb_4bit_compute_dtype=torch.float16 # This is to use half the memory and fit the model\\n    # )\\n\\n    model = AutoModelForCausalLM.from_pretrained( \\n        \"google/gemma-2b\", # Here we are defining the llama 2 7B model\\n        use_safetensors=True, #  for storing and loading tensors\\n        trust_remote_code=True, \\n        device_map=\"auto\" # and put each layer of the model depending on the available resources\\n        \\n    )\\n\\n    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\") # Here we tell we want to use the same tokenizer that the model ises\\n    tokenizer.pad_token = tokenizer.eos_token # Telling that all the padding tokens should be the same as the \\'end of sentence\\'\\n    tokenizer.padding_side = \"right\" # and the side padding is the right side\\n\\n    return model, tokenizer',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet defines a function called \"create_model_and_tokenizer\" which loads and configures a quantization library called \"BitsAndBytes\" to load the model in a 4-bit precision instead of its original precision. It then specifies the llama 2 7B model to be used and sets certain parameters for storing and loading tensors. The function also sets up a tokenizer to be used for the model and specifies the padding tokens and side. Finally, the function returns the model and tokenizer. The code is part of a file called \"fine-tuning-fullprec.ipynb\" which may be used for fine-tuning a neural network model.'},\n",
       " {'id': '5478037d-da6b-4f35-9d93-d2040a4a63ab',\n",
       "  'embedding': None,\n",
       "  'code': 'model, tokenizer = create_model_and_tokenizer()\\nmodel.config.use_cache = False # The cache is only used for generation, not for training',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet creates a model and tokenizer, and sets the model\\'s configuration to not use cache for training. This is done in the \"fine-tuning-fullprec\" notebook, which may take a few minutes to run.'},\n",
       " {'id': '4f09a027-3479-4517-aa6e-6d548b1205c0',\n",
       "  'embedding': None,\n",
       "  'code': 'lora_r = 16 # Ranking of the matrix\\nlora_alpha = 64 # Scaling facotr\\nlora_dropout = 0.1\\nlora_target_modules = [ # Selecting what layers of the model we want to use\\n    \"q_proj\",\\n    \"up_proj\",\\n    \"o_proj\",\\n    \"k_proj\",\\n    \"down_proj\",\\n    \"gate_proj\",\\n    \"v_proj\",\\n]\\n\\n\\npeft_config = LoraConfig(\\n    r=lora_r,\\n    lora_alpha=lora_alpha,\\n    lora_dropout=lora_dropout,\\n    target_modules=lora_target_modules,\\n    bias=\"none\",\\n    task_type=\"CAUSAL_LM\",\\n)',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet is setting up variables and configurations for fine-tuning a language model using the LoRA (Low-Rank Attention) approach. It sets the ranking and scaling factor for the LoRA matrix, specifies a dropout rate, and selects specific layers of the model to be used. It also creates a LoraConfig object that contains the specified settings. The code is used in the file \"fine-tuning-fullprec.ipynb\" for efficient fine-tuning of language models.'},\n",
       " {'id': '23b972b0-4ef7-4ef6-b174-99647cd349d0',\n",
       "  'embedding': None,\n",
       "  'code': \"os.environ['MLFLOW_EXPERIMENT_NAME'] = 'gemma2b-summary_task-fullpreci'\",\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet sets the environment variable \"MLFLOW_EXPERIMENT_NAME\" to the value \"gemma2b-summary_task-fullpreci\". This variable is typically used to specify the name of an experiment in a machine learning project. The file name \"fine-tuning-fullprec.ipynb\" suggests that this code is used for fine-tuning a model using full precision data. The context mentions accessing the experiment on the Monitor tab, indicating that this code is likely used for tracking and monitoring the training process of the model.'},\n",
       " {'id': 'f3179d47-16a1-45dd-867c-aa10813dc83d',\n",
       "  'embedding': None,\n",
       "  'code': 'training_arguments = TrainingArguments(\\n    per_device_train_batch_size=4, # The batch size per GPU/TPU core/CPU for training.\\n    gradient_accumulation_steps=4, # Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\\n    optim=\"paged_adamw_32bit\", #Adam Optimizer\\n    logging_steps=20, # Number of update steps between two logs if logging_strategy=\"steps\".\\n    save_steps = 20,\\n    learning_rate=1e-3, # The initial learning rate for AdamW optimizer\\n    fp16=True, # Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\\n    max_grad_norm=0.3, # Maximum gradient norm\\n    num_train_epochs=3, # Number of epochs\\n    evaluation_strategy=\"steps\", # Evaluation is done (and logged) every eval_steps\\n    eval_steps=0.2,\\n    warmup_ratio=0.05, # Ratio of total training steps used for a linear warmup from 0 to learning_rate\\n    save_strategy=\"steps\", # Save is done at the end of each steps\\n    group_by_length=True, # Whether or not to group together samples of roughly the same length in the training dataset \\n    output_dir=\\'./gemma/fullprec\\', # Path to save model checkpoints and bins\\n    report_to=\"mlflow\", # Integration to report the results and logs to\\n    save_safetensors=True, # Use safetensors saving and loading for state dicts instead of default torch.load and torch.save\\n    lr_scheduler_type=\"cosine\", # The scheduler type to use.\\n    seed=42, # Random seed that will be set at the beginning of training. \\n) ',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet defines training arguments for a model, including batch size, gradient accumulation steps, optimizer, logging and saving settings, learning rate, precision type, epoch number, evaluation and saving strategies, warmup ratio, grouping of training samples, output directory, integration for reporting results and logs, use of safetensors for saving and loading state dictionaries, scheduler type, and a random seed. This is to be used for fine-tuning a model in full precision, as indicated by the file name.'},\n",
       " {'id': 'ba8f38ad-1929-44e8-9c6c-3d2697447f88',\n",
       "  'embedding': None,\n",
       "  'code': 'trainer = SFTTrainer(\\n    model=model,\\n    train_dataset=dataset[\"train\"], # Placing our dataset on train\\n    eval_dataset=dataset[\"test\"], # and test part\\n    peft_config=peft_config, #LoRA\\n    dataset_text_field=\"text\",\\n    max_seq_length=4096, # Specifies the maximum number of tokens of the input\\n    tokenizer=tokenizer, # Model\\'s tokenizer we loaded before\\n    args=training_arguments,\\n)',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': \"The code snippet is creating an instance of the SFTTrainer class, which is used for fine-tuning a model. It takes in the model, train and test datasets, configuration for the Peft algorithm, the dataset's text field, maximum sequence length, tokenizer, and training arguments. The file name indicates that this code is used for fine-tuning a model with full precision. It is likely used for language processing tasks and the Trainer documentation can provide more information on its functionality.\"},\n",
       " {'id': 'c1db2475-05f3-401e-806f-6785634f74b0',\n",
       "  'embedding': None,\n",
       "  'code': 'start_time = time.time()\\ntrainer.train()\\nend_time = time.time()\\ntraining_duration = end_time - start_time\\nmlflow.log_metric(\"training_time\", training_duration)',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code above is recording the start and end time of a training process using the time module. It then calculates the duration of the training and logs it as a metric using the mlflow library. This code is likely part of a larger notebook file called \"fine-tuning-fullprec.ipynb\" and is used to track the training duration during a fine-tuning process.'},\n",
       " {'id': '3f26fb80-7fd1-4e92-8a65-b96af84b5a0e',\n",
       "  'embedding': None,\n",
       "  'code': 'trainer.save_model()',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet saves a trained model to a file. The file name is \"fine-tuning-fullprec.ipynb\" and the context is likely related to a training step.'},\n",
       " {'id': 'e17c4559-a169-49fe-b36a-477720250cec',\n",
       "  'embedding': None,\n",
       "  'code': \"model = PeftModel.from_pretrained(model, './gemma/fullprec/')\",\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet loads a model from a pretrained file using the PeftModel method and stores it in the \"model\" variable. The file name is \"fine-tuning-fullprec.ipynb\" and it is used to fine-tune the model. The optional context suggests that this code is being used in a larger project or program.'},\n",
       " {'id': 'f0fd7818-d275-4a9b-b498-b34c1cafc63b',\n",
       "  'embedding': None,\n",
       "  'code': 'def generate_prompt(\\n    conversation: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT\\n) -> str:\\n    return f\"\"\"### Instruction: {system_prompt}\\n\\n### Input:\\n{conversation.strip()}\\n\\n### Response:\\n\"\"\".strip()',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet is a function that generates a prompt for a conversation, using a given system prompt or a default one. The file name is \"fine-tuning-fullprec.ipynb\" and the context is creating a test dataset for a model.'},\n",
       " {'id': '96784098-7a75-4c0c-aa47-a2033adfb135',\n",
       "  'embedding': None,\n",
       "  'code': 'examples = []\\n\\nfor data_point in dataset[\"test\"].select(range(5)):\\n    summary = data_point[\\'summary\\']\\n    conversation = data_point[\\'dialogue\\']\\n    examples.append(\\n        {\\n            \"summary\": summary,\\n            \"conversation\": conversation,\\n            \"prompt\": generate_prompt(conversation),\\n        }\\n    )\\ntest_df = pd.DataFrame(examples)',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet is creating a test dataset by extracting data points from the \"test\" section of a larger dataset. It then takes the \"summary\" and \"dialogue\" elements from each data point and adds them to a new dictionary, along with a prompt generated using the \"conversation\" data. This dictionary is then appended to a list called \"examples\". Finally, the code creates a pandas dataframe called \"test_df\" using the information from the \"examples\" list. The file name is \"fine-tuning-fullprec.ipynb\" and the code is being used to create a test dataset for a model.'},\n",
       " {'id': 'bbcdbbb0-84f8-4c48-b314-74b476d3e8dd',\n",
       "  'embedding': None,\n",
       "  'code': 'test_df',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code creates a test dataset named \"test_df\" with 5 rows, in the file \"fine-tuning-fullprec.ipynb\". This dataset is used for trying a model.'},\n",
       " {'id': 'd17c6879-1c75-4850-acd7-46fbb87b81a3',\n",
       "  'embedding': None,\n",
       "  'code': 'def summarize(model, text: str):\\n    inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\\n    inputs_length = len(inputs[\"input_ids\"][0])\\n    with torch.inference_mode():\\n        # Adjust temperature to a more stable value\\n        outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.7)\\n    return tokenizer.decode(outputs[0][inputs_length:], skip_special_tokens=True)',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet defines a function called \"summarize\" that takes in a pre-trained model and a string of text as inputs. It then uses a tokenizer to convert the text into a format that the model can understand and sets the device for the model to run on. The code then generates a summary of the text using the model with specific parameters for the maximum number of new tokens and temperature. Finally, the function returns the decoded summary by removing any special tokens. The file named \"fine-tuning-fullprec.ipynb\" contains code for fine-tuning the pre-trained model, and the function in the code snippet is used to tokenize inputs and utilize the model for summarization.'},\n",
       " {'id': 'f962d0bd-63aa-4976-8a0d-e75183a01239',\n",
       "  'embedding': None,\n",
       "  'code': 'import numpy as np\\nnp.random.seed(28)',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet imports the NumPy library and sets a specific seed value for random number generation. The file name suggests that this code may be used for fine-tuning a model, possibly for machine learning or deep learning, using full precision data. The optional context may provide additional information or instructions related to the use of this code.'},\n",
       " {'id': '94150f05-0a1f-4897-af24-c4cd0732c94a',\n",
       "  'embedding': None,\n",
       "  'code': 'example = test_df.iloc[0]\\nprint(example.conversation)',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet is accessing the first row of data from a dataframe called \"test_df\" and assigning it to a variable called \"example\". Then, it is printing the value of the \"conversation\" column from the first row. This code is likely part of a larger file called \"fine-tuning-fullprec.ipynb\" which is used for fine-tuning a model or making predictions based on the data.'},\n",
       " {'id': '4973989b-aced-428a-a428-5bdb45adf7e0',\n",
       "  'embedding': None,\n",
       "  'code': 'print(example.summary)',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet is using the \"print\" function to display the summary of the variable \"example\". This variable could contain a summary of a dataset or a model. The file name is \"fine-tuning-fullprec.ipynb\", which suggests that it is a notebook file for fine-tuning a model. The optional context could provide additional information about the purpose of the code, such as what type of model is being fine-tuned and what dataset it is being trained on. Overall, the code is likely used to display the summary of a model or dataset as part of a fine-tuning process.'},\n",
       " {'id': '64831aa5-9561-4106-abf3-d5d1f3dcb6fe',\n",
       "  'embedding': None,\n",
       "  'code': 'print(example.prompt)',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet prints the value of the variable \"example.prompt\". The file name is \"fine-tuning-fullprec.ipynb\" and it is likely a notebook file. The context is describing the purpose of the code, which is to make inferences. Overall, the code is likely used to fine-tune a model or make predictions based on the given data.'},\n",
       " {'id': 'd92cfda4-1cac-4c39-8f4b-4a3a51b9f1ec',\n",
       "  'embedding': None,\n",
       "  'code': '%%time\\nsummary = summarize(model, example.prompt)',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet measures the time it takes to run the model to summarize the given prompt. It is part of a file named \"fine-tuning-fullprec.ipynb\" and is likely used for fine-tuning and evaluating the performance of the model. This code is used in the context of \"Inferences\" and is likely used to generate summaries for various prompts.'},\n",
       " {'id': 'd1c104db-3810-4fd3-8708-5784c3c39c2e',\n",
       "  'embedding': None,\n",
       "  'code': 'print(summary)',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet is using the \"print\" function to display a summary of some data or information. The file name indicates that this code is part of a Jupyter notebook file titled \"fine-tuning-fullprec\", which could suggest that the code is used for fine-tuning a machine learning model. The optional context provided mentions \"inferences\", which could mean that the code is used for making predictions or drawing conclusions based on the data. Overall, the code snippet is likely used for displaying a summary of some data in a Jupyter notebook file related to machine learning inferences or predictions.'},\n",
       " {'id': '5265dd11-12c4-42bc-abb9-1a7b1d7bafca',\n",
       "  'embedding': None,\n",
       "  'code': 'from peft import PeftModel, PeftConfig\\nfrom transformers import AutoModelForCausalLM\\n\\nconfig = PeftConfig.from_pretrained(\"morgana-rodrigues/gemma-2b-samsum\")\\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\\nmodel = PeftModel.from_pretrained(model, \"morgana-rodrigues/gemma-2b-samsum\")',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code above imports the necessary modules for fine-tuning a pre-trained Peft model, sets the Peft configuration using a specific pre-trained model, and then creates a Peft model using the same pre-trained model. The code also specifies a file name for the fine-tuning process and mentions that the model is available on the HuggingFace library. Essentially, the code is preparing and setting up the necessary components for fine-tuning the Peft model on a specific dataset.'},\n",
       " {'id': 'f79e8fa8-405d-43d1-b4d0-c0719326a602',\n",
       "  'embedding': None,\n",
       "  'code': 'import os\\n\\ndef get_size(start_path):\\n    total_size = 0\\n    for dirpath, dirnames, filenames in os.walk(start_path):\\n        for f in filenames:\\n            fp = os.path.join(dirpath, f)\\n            # skip if it is symbolic link\\n            if not os.path.islink(fp):\\n                total_size += os.path.getsize(fp)\\n\\n    return total_size\\n\\n# Caminhos dos diretÃ³rios do modelo\\nmodel_full_prec_dir = \\'/home/jovyan/.cache/huggingface/hub/models--morgana-rodrigues--gemma-2b-samsum\\'\\n\\n# ObtÃ©m o tamanho do diretÃ³rio do modelo quantizado de 8 bits\\nsize_quant_8bit = get_size(model_8bit_dir)\\n\\nprint(f\"Tamanho do modelo quantizado de 8 bits: {size_quant_8bit} bytes\")',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet imports the os module and defines a function called \"get_size\" which takes in a start path as a parameter. The function then initializes a variable \"total_size\" and uses a for loop to iterate through the directories, subdirectories, and files in the specified start path using the os.walk() function. Inside the loop, it checks if the file is a symbolic link and if not, it adds the file size to the total_size variable using the os.path.getsize() function. Finally, the function returns the total size of the specified start path.\\n\\nThe file name \"fine-tuning-fullprec.ipynb\" is referencing a Jupyter notebook file that likely contains code for fine-tuning the model. The optional context mentions that the model is also available on the HuggingFace library, implying that the code is used for fine-tuning a model from the library. The code snippet also includes a comment indicating that it is obtaining the size of a quantized 8-bit model directory, which suggests that the code may be used for evaluating the size of a model after quantization.'},\n",
       " {'id': '6da13dfc-8f23-45a1-8d91-66d8af58718b',\n",
       "  'embedding': None,\n",
       "  'code': 'import sys\\n\\n# Example variable\\nmy_list = model_full_prec_dir\\n# Checking the size of the variable\\nsize_in_bytes = sys.getsizeof(my_list)\\n\\nprint(f\"The variable is using {size_in_bytes} bytes of memory.\")',\n",
       "  'filename': 'fine-tuning-fullprec.ipynb',\n",
       "  'context': 'The code snippet imports the \"sys\" library and assigns the variable \"my_list\" with the value of \"model_full_prec_dir\". It then uses the \"sys.getsizeof()\" function to check the size of the variable in bytes and prints a message indicating the amount of memory being used. The file name suggests that this code snippet is part of a larger notebook or script related to fine-tuning a model, and the context suggests that the model may be available on a library called \"HuggingFace\".'},\n",
       " {'id': '7b1f0072-269e-4df6-a46f-aa71d7a56a38',\n",
       "  'embedding': None,\n",
       "  'code': '!pip install datasets # This one is for downloading our samsum dataset direclty from Hugging Face\\n!pip install peft # Both peft and trl are the libs that help us \\n!pip install trl # to configure our training methods and params\\n!pip install bitsandbytes # This one will help us to quantize the model\\n!pip install mlflow==2.11.0',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code above installs several libraries, including datasets, peft, trl, bitsandbytes, and mlflow, for use in fine-tuning a model. Specifically, it allows for the downloading of a dataset from Hugging Face, configuring training methods and parameters, and quantizing the model. The file name \"fine-tuning-8bits.ipynb\" suggests that this code is used for fine-tuning a model using 8-bit precision.'},\n",
       " {'id': '06a68a69-248c-4445-a756-5f9dec2c11ea',\n",
       "  'embedding': None,\n",
       "  'code': 'from datasets import load_dataset, Dataset, DatasetDict\\nimport os\\nimport json\\nimport re\\nfrom pprint import pprint\\nimport pandas as pd\\nimport torch\\nfrom datasets import Dataset, load_dataset\\nfrom huggingface_hub import notebook_login\\nfrom peft import LoraConfig, PeftModel, AutoPeftModelForCausalLM, PeftModelForCausalLM\\nfrom transformers import (\\n    BitsAndBytesConfig,\\n    TrainingArguments,\\n)\\nfrom trl import SFTTrainer\\nimport time\\nimport mlflow',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet imports various libraries and modules needed for fine-tuning a model. It then defines a file name and sets a context for the code. The code is used for fine-tuning a model using a dataset and a specific configuration, and it also includes training arguments and a trainer for the model. Additionally, it includes libraries for managing and tracking the training process. The file name is \"fine-tuning-8bits.ipynb\", indicating that the code is being run in a Jupyter notebook. The context mentions that the code is being used in a Deep Learning workspace and that the necessary libraries are already installed.'},\n",
       " {'id': '71ac1084-2e8f-45ff-8160-383d86afb696',\n",
       "  'embedding': None,\n",
       "  'code': '# \\'cuda:0\\' means that we want to use our GPU, if available. If not, uses CPU.\\nDEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\\n# Model name defines that we are using llama 2 with 7B parameters\\n# MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet defines the device to be used for computing, either the GPU if available or the CPU. It also sets the model name to \"Llama-2-7b-hf\". The file name suggests that this code is used for fine-tuning a model with 8-bit precision.'},\n",
       " {'id': '936152e4-f527-4cd2-9982-9c069dc7294e',\n",
       "  'embedding': None,\n",
       "  'code': 'HF_TOKEN = \"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\"',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet assigns the value \"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\" to the variable HF_TOKEN, which is likely a token used for authentication or authorization. The file name suggests that this code may be used for fine-tuning a model with 8-bit precision, and the context confirms that the token is related to Hugging Face, a natural language processing library.'},\n",
       " {'id': '0d1ec2b4-e204-4f02-88db-fa5c7a823ac4',\n",
       "  'embedding': None,\n",
       "  'code': 'dataset = load_dataset(\"samsum\")',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet loads a dataset named \"samsum\" using a function called load_dataset(). The file name is \"fine-tuning-8bits.ipynb\" and it is likely used for fine-tuning a model using the loaded dataset. The context suggests that the code is being used for a project involving natural language processing, specifically for summarization (based on the dataset name).'},\n",
       " {'id': '7d68f73b-9433-4c48-a818-af2d2ba18567',\n",
       "  'embedding': None,\n",
       "  'code': \"def format_dialogue(dialogue):\\n    # Replace the '\\\\r\\\\n' with '\\\\n' to match the desired output format.\\n    formatted_dialogue = re.sub(r'\\\\r\\\\n', '\\\\n', dialogue)\\n    return formatted_dialogue\",\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': \"This code snippet defines a function called format_dialogue, which takes in a parameter called dialogue. It uses the regular expression module (re) to replace any instances of '\\\\r\\\\n' (carriage return and line feed) in the dialogue with just '\\\\n' (line feed). This is done to match the desired output format. The function then returns the resulting formatted dialogue. The file name suggests that this code may be used for fine-tuning a dataset that consists of 8-bit data.\"},\n",
       " {'id': '4326337e-a6b0-4dc1-a886-d84111cc46d8',\n",
       "  'embedding': None,\n",
       "  'code': 'dataset',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet defines a variable named \"dataset.\" The file name indicates that the code may be related to fine-tuning a model using 8-bit data, and the optional context suggests that the code may be used to manipulate or analyze a dataset.'},\n",
       " {'id': 'ac798e0c-6ffc-4e7b-b0f5-921c1365592f',\n",
       "  'embedding': None,\n",
       "  'code': \"dataset['train'][0]\",\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': \"The code snippet is accessing the first element (index 0) of the 'train' subset in the dataset. The file name suggests that this code is likely part of a process for fine-tuning a machine learning model, specifically for handling 8-bit data. The optional context suggests that the code is being used to inspect a sample from the training set.\"},\n",
       " {'id': 'aa3a8e4a-6f88-404e-b6a6-5ae2ace65beb',\n",
       "  'embedding': None,\n",
       "  'code': \"dataset['train'][0]['dialogue']\",\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet is accessing the \"dialogue\" data from the first datapoint in the \"train\" dataset. The file name suggests that this code is used for fine-tuning a model, possibly for a task involving 8-bit data. The context confirms that the code is accessing the dialogue from the first datapoint.'},\n",
       " {'id': 'ce3b1b6d-6d17-48f1-9dda-210aab7d2fe3',\n",
       "  'embedding': None,\n",
       "  'code': \"def format_dialogue(dialogue):\\n    # Replace the '\\\\r\\\\n' with '\\\\n' to match the desired output format.\\n    formatted_dialogue = re.sub(r'\\\\r\\\\n', '\\\\n', dialogue)\\n    return formatted_dialogue\",\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet defines a function called \"format_dialogue\" that takes in a parameter called \"dialogue\". Within the function, it uses regular expressions to replace any instances of \"\\\\r\\\\n\" (carriage return and line feed) with just \"\\\\n\" (line feed). This results in a cleaner and more consistent format for the dialogue. The function then returns the formatted dialogue. This code is likely used in the file named \"fine-tuning-8bits.ipynb\" to clean up the dialogue before further processing or analysis is done.'},\n",
       " {'id': 'edc5b3b1-ff63-41c7-97fd-8f8e96abb807',\n",
       "  'embedding': None,\n",
       "  'code': \"print(format_dialogue(dataset['train'][0]['dialogue']))\",\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet is using the function \"format_dialogue\" to print the dialogue from the first train dialogue in the dataset. The file name is \"fine-tuning-8bits.ipynb\" and the context is testing the function on the first train dialogue. In summary, the code is printing the formatted dialogue from the first train dialogue in the dataset, as part of a larger process of fine-tuning using the specific file.'},\n",
       " {'id': '79383bec-e81b-4ef7-ba62-61a59146a573',\n",
       "  'embedding': None,\n",
       "  'code': 'DEFAULT_SYSTEM_PROMPT = \"\"\"\\nBelow is a conversation between friends in a chat. Write a summary of their conversation.\\n\"\"\".strip()',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet defines a constant variable named DEFAULT_SYSTEM_PROMPT and assigns it a multi-line string value. The string contains a prompt for a conversation between friends in a chat. The .strip() method removes any leading or trailing whitespaces from the string. The file name is \"fine-tuning-8bits.ipynb\" and the context mentions defining an input prompt. This suggests that the code is part of a larger program or notebook that involves fine-tuning a model using an input prompt.'},\n",
       " {'id': '7e2713ff-01b2-4beb-a95f-55f47da1769c',\n",
       "  'embedding': None,\n",
       "  'code': 'def generate_training_prompt(\\n    conversation: str, summary: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT\\n) -> str:\\n    return f\"\"\"### Instruction: {system_prompt}\\n\\n### Input:\\n{conversation.strip()}\\n\\n### Response:\\n{summary}\\n\"\"\".strip()',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code is a function called \"generate_training_prompt\" that takes in a conversation string, a summary string, and an optional system prompt string. It then returns a formatted string that serves as a training prompt for a language model. The file name is \"fine-tuning-8bits.ipynb\" and the context is defining the input prompt.'},\n",
       " {'id': '0fffd322-9580-4990-8bb3-80ba397fa247',\n",
       "  'embedding': None,\n",
       "  'code': 'def generate_text(data_point):\\n    summary = data_point[\\'summary\\']\\n    conversation_text = format_dialogue(data_point[\\'dialogue\\'])\\n    return {\\n        \"conversation\": conversation_text,\\n        \"summary\": summary,\\n        \"text\": generate_training_prompt(conversation_text, summary),\\n    }',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet defines a function called \"generate_text\" that takes in a data point as its input. The function then extracts the \"summary\" and \"dialogue\" values from the data point and uses them to format a conversation text. Finally, the function returns a dictionary containing the conversation text, summary, and a training prompt generated from the conversation text and summary. The file name \"fine-tuning-8bits.ipynb\" suggests that this code may be used for fine-tuning a model with 8-bit precision.'},\n",
       " {'id': 'a80d399a-e9ce-4cfb-ad3b-149d8214d643',\n",
       "  'embedding': None,\n",
       "  'code': 'def process_dataset(data: Dataset):\\n    return (\\n        data.shuffle(seed=42)\\n        .map(generate_text))',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': \"The code snippet defines a function called 'process_dataset' which takes in a dataset as its input. The function shuffles the input data using a seed of 42, and then maps the data to generate text. The file name suggests that this code is used for fine-tuning a dataset, likely for a machine learning or natural language processing task. The optional context further suggests that the code may be used to define an input prompt for the fine-tuning process.\"},\n",
       " {'id': 'd23da2d6-8436-4999-aee1-c69dcc64bada',\n",
       "  'embedding': None,\n",
       "  'code': '# dataset[\"train\"] = process_dataset(dataset[\"train\"])\\ndataset[\"train\"] = process_dataset(dataset[\"train\"].select(range(1500)))\\ndataset[\"test\"] = process_dataset(dataset[\"test\"])',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code is part of a file called \"fine-tuning-8bits.ipynb\" and is used to process a dataset. The first line of code takes a subset of the dataset called \"train\", applies a function called \"process_dataset\" to it, and then saves the processed dataset back into the \"train\" subset. The second line of code also takes a subset of the dataset called \"test\", applies the \"process_dataset\" function to it, and saves the processed dataset back into the \"test\" subset. This process is repeated for both the \"train\" and \"test\" subsets of the dataset.'},\n",
       " {'id': '6a3d761c-ebf4-49d5-b9eb-009bdb8b220f',\n",
       "  'embedding': None,\n",
       "  'code': 'notebook_login()',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet is calling the function \"notebook_login\" to log into a notebook. The file name is \"fine-tuning-8bits.ipynb\", indicating that the code is likely related to fine-tuning a model with 8-bit precision. The optional context suggests that the code is used for instantiating the model and its tokenizer, which may involve setting specific parameters and loading saved files.'},\n",
       " {'id': '0885028c-bad5-4b3f-bd39-82d23c3f6a8d',\n",
       "  'embedding': None,\n",
       "  'code': 'from transformers import AutoTokenizer, AutoModelForCausalLM',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet is importing the AutoTokenizer and AutoModelForCausalLM modules from the transformers library. The file name suggests that this code is used for fine-tuning a model with 8-bit precision. The optional context mentions quantization, which is the process of converting floating-point numbers to fixed-point numbers. This suggests that the code is used for converting a model to a lower precision format for faster and more efficient inference. Additionally, the tokenizer and model modules are likely used for natural language processing tasks.'},\n",
       " {'id': '3ce297d7-2ad8-4405-8bc6-1c402e53ac54',\n",
       "  'embedding': None,\n",
       "  'code': 'def create_model_and_tokenizer():\\n\\n    bnb_config_8 = BitsAndBytesConfig( #BitsAndBytes is the reponsable librarie that help us to configure the desired quantization.\\n        load_in_8_bit=True,\\n        bnb_4bit_quant_type=\"nf4\", # normalized float 4 bit data type\\n        bnb_4bit_compute_dtype=torch.float16 # This is to use half the memory and fit the model\\n    )\\n\\n    model = AutoModelForCausalLM.from_pretrained( \\n        \"google/gemma-2b\", # Here we are defining the llama 2 7B model\\n        use_safetensors=True, #  for storing and loading tensors\\n        quantization_config=bnb_config_8, # with the desired quantization \\n        trust_remote_code=True, \\n        device_map=\"auto\" # and put each layer of the model depending on the available resources\\n        \\n    )\\n\\n    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\") # Here we tell we want to use the same tokenizer that the model ises\\n    tokenizer.pad_token = tokenizer.eos_token # Telling that all the padding tokens should be the same as the \\'end of sentence\\'\\n    tokenizer.padding_side = \"right\" # and the side padding is the right side\\n\\n    return model, tokenizer',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet creates a function called \"create_model_and_tokenizer\" that is used to configure quantization, tokenizer, and model for fine-tuning a model. The function uses the BitsAndBytes library to set the desired quantization, defines the model to be used (llama 2 7B) and its corresponding tokenizer, and specifies the padding and device mapping. The function then returns the configured model and tokenizer. This code is likely part of a larger project, as it is stored in a Jupyter Notebook file named \"fine-tuning-8bits.ipynb\".'},\n",
       " {'id': 'ba306bb6-c87e-44ba-b389-d941285bff01',\n",
       "  'embedding': None,\n",
       "  'code': 'model, tokenizer = create_model_and_tokenizer()\\nmodel.config.use_cache = False # The cache is only used for generation, not for training',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet is creating a model and tokenizer, and then disabling the use of cache for training. This code is likely used for fine-tuning a model for 8-bit data, as indicated by the file name. The note at the end suggests that it may take some time to complete.'},\n",
       " {'id': 'eb9ce9aa-6d52-4305-8a0d-3223adbd539c',\n",
       "  'embedding': None,\n",
       "  'code': 'lora_r = 16 # Ranking of the matrix\\nlora_alpha = 64 # Scaling facotr\\nlora_dropout = 0.1\\nlora_target_modules = [ # Selecting what layers of the model we want to use\\n    \"q_proj\",\\n    \"up_proj\",\\n    \"o_proj\",\\n    \"k_proj\",\\n    \"down_proj\",\\n    \"gate_proj\",\\n    \"v_proj\",\\n]\\n\\n\\npeft_config = LoraConfig(\\n    r=lora_r,\\n    lora_alpha=lora_alpha,\\n    lora_dropout=lora_dropout,\\n    target_modules=lora_target_modules,\\n    bias=\"none\",\\n    task_type=\"CAUSAL_LM\",\\n)',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet initializes and configures the LoRA (Low-Rank Attention) model for fine-tuning. It sets the ranking, scaling factor, dropout rate, and targeted layers for the model. The code also specifies the LoRA configuration for a specific task type and bias setting. The file name suggests that this code may be used in a notebook for fine-tuning a model with 8-bit precision.'},\n",
       " {'id': '83a8529e-c79c-452f-82a1-d8a5b9739290',\n",
       "  'embedding': None,\n",
       "  'code': \"os.environ['MLFLOW_EXPERIMENT_NAME'] = 'gemma2b-summary_task-quant8-6k'\",\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': \"The code snippet sets an environment variable called 'MLFLOW_EXPERIMENT_NAME' to the value 'gemma2b-summary_task-quant8-6k'. This variable is likely used for tracking and organizing experiments in a machine learning workflow. The associated file, 'fine-tuning-8bits.ipynb', is a notebook file that likely contains code for fine-tuning a machine learning model. The optional context suggests that the code is used to define a name for the experiment, making it easier to access and track in the future.\"},\n",
       " {'id': '70d83c38-c7db-4bce-ab12-63dab875bc4a',\n",
       "  'embedding': None,\n",
       "  'code': 'training_arguments = TrainingArguments(\\n    per_device_train_batch_size=4, # The batch size per GPU/TPU core/CPU for training.\\n    gradient_accumulation_steps=4, # Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\\n    optim=\"paged_adamw_32bit\", #Adam Optimizer\\n    logging_steps=20, # Number of update steps between two logs if logging_strategy=\"steps\".\\n    save_steps = 20,\\n    learning_rate=1e-3, # The initial learning rate for AdamW optimizer\\n    fp16=True, # Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\\n    max_grad_norm=0.3, # Maximum gradient norm\\n    num_train_epochs=3, # Number of epochs\\n    evaluation_strategy=\"steps\", # Evaluation is done (and logged) every eval_steps\\n    eval_steps=0.2,\\n    warmup_ratio=0.05, # Ratio of total training steps used for a linear warmup from 0 to learning_rate\\n    save_strategy=\"steps\", # Save is done at the end of each steps\\n    group_by_length=True, # Whether or not to group together samples of roughly the same length in the training dataset \\n    output_dir=\\'./gemma/gemma-8bit\\', # Path to save model checkpoints and bins\\n    report_to=\"mlflow\", # Integration to report the results and logs to\\n    save_safetensors=True, # Use safetensors saving and loading for state dicts instead of default torch.load and torch.save\\n    lr_scheduler_type=\"cosine\", # The scheduler type to use.\\n    seed=42, # Random seed that will be set at the beginning of training. \\n) ',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code above is setting up training arguments for a fine-tuning task using the Hugging Face Trainer. It specifies parameters such as batch size, gradient accumulation steps, optimizer, learning rate, precision (32-bit vs 16-bit), and number of epochs. It also sets up evaluation and saving strategies, warmup ratio, and a random seed. The code is being used in the context of fine-tuning a model on a dataset with a specific file name and output directory. The results and logs will be reported to MLflow, and safetensors will be used for saving and loading model state dictionaries.'},\n",
       " {'id': 'ac58d58d-a862-4df1-9291-c71200a03b55',\n",
       "  'embedding': None,\n",
       "  'code': 'trainer = SFTTrainer(\\n    model=model,\\n    train_dataset=dataset[\"train\"], # Placing our dataset on train\\n    eval_dataset=dataset[\"test\"], # and test part\\n    peft_config=peft_config, #LoRA\\n    dataset_text_field=\"text\",\\n    max_seq_length=4096, # Specifies the maximum number of tokens of the input\\n    tokenizer=tokenizer, # Model\\'s tokenizer we loaded before\\n    args=training_arguments,\\n)',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet is used for training a model, specifically the SFTTrainer which is imported from the Hugging Face library. The trainer is initialized with a specific model, train and test datasets, PEFT configuration, dataset text field, maximum sequence length, tokenizer, and training arguments. This code is likely used for fine-tuning a pre-trained model for a specific task. The file name suggests that it is specifically used for fine-tuning a model using 8-bit precision.'},\n",
       " {'id': 'd247910c-41e3-4b03-ac39-aa262301c24c',\n",
       "  'embedding': None,\n",
       "  'code': 'start_time = time.time()\\ntrainer.train()\\nend_time = time.time()\\ntraining_duration = end_time - start_time\\nmlflow.log_metric(\"training_time\", training_duration)',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet is measuring the duration of a training process using the time.time() function. It starts by setting a start time and then calls a trainer.train() function. After the training is completed, it sets an end time and calculates the duration by subtracting the start time from the end time. The duration is then logged as a metric using the mlflow.log_metric() function. This code is likely part of a larger project, as indicated by the file name \"fine-tuning-8bits.ipynb\", and is used to track the training time for a specific model or algorithm.'},\n",
       " {'id': '877ef8e4-22f8-4975-9689-643dcdf20fc4',\n",
       "  'embedding': None,\n",
       "  'code': 'trainer.save_model()',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet is saving a trained model, indicated by the \"trainer\" object, in the file named \"fine-tuning-8bits.ipynb\". This code is most likely used in the context of a training step, where the model is being fine-tuned. The save_model() function allows the trained model to be saved for future use or reference.'},\n",
       " {'id': '2e8d6657-42c1-4262-b41f-83eff863697d',\n",
       "  'embedding': None,\n",
       "  'code': \"model = PeftModelForCausalLM.from_pretrained(model, './gemma/gemma-8bit')\",\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet is loading a pretrained PeftModel for Causal Language Modeling by specifying the model name and the file path where the model is located. The file name is \"fine-tuning-8bits.ipynb\" and the code is likely being used for fine-tuning the model with 8-bit precision. The optional context suggests that the code is part of a larger code file or notebook.'},\n",
       " {'id': '9566b19d-d74c-412e-9085-11e9060c2218',\n",
       "  'embedding': None,\n",
       "  'code': 'def generate_prompt(\\n    conversation: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT\\n) -> str:\\n    return f\"\"\"### Instruction: {system_prompt}\\n\\n### Input:\\n{conversation.strip()}\\n\\n### Response:\\n\"\"\".strip()',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet is a function called \"generate_prompt\" that takes in two parameters, a conversation string and a system prompt string. It returns a formatted string that includes the system prompt, the conversation input, and a placeholder for a response. The file name is \"fine-tuning-8bits.ipynb\" and the context is creating a test dataset for a model.'},\n",
       " {'id': '148ca06f-83ea-4363-8541-89a57b813a3b',\n",
       "  'embedding': None,\n",
       "  'code': 'examples = []\\n\\nfor data_point in dataset[\"test\"].select(range(5)):\\n    summary = data_point[\\'summary\\']\\n    conversation = data_point[\\'dialogue\\']\\n    examples.append(\\n        {\\n            \"summary\": summary,\\n            \"conversation\": conversation,\\n            \"prompt\": generate_prompt(conversation),\\n        }\\n    )\\ntest_df = pd.DataFrame(examples)',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet is creating a test dataset by selecting the first 5 data points from the \"test\" subset of a larger dataset. It then extracts the summary and conversation data from each data point and appends it to a list called \"examples\". Finally, it uses the \"generate_prompt\" function to generate a prompt based on the conversation data, and adds all of the data to a pandas DataFrame called \"test_df\". This code is likely part of a larger process of fine-tuning a model, as indicated by the file name and context provided.'},\n",
       " {'id': 'b52071bb-18e8-46eb-b28e-157a8b9c38ac',\n",
       "  'embedding': None,\n",
       "  'code': 'test_df',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet creates a variable called \"test_df\". The file name is \"fine-tuning-8bits.ipynb\", and there is an additional function to tokenize inputs and use a model, although the specific details and purpose of this function are not given.'},\n",
       " {'id': 'b6b39bbe-4abf-4d60-93f9-83dc3988dec0',\n",
       "  'embedding': None,\n",
       "  'code': 'def summarize(model, text: str):\\n    inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\\n    inputs_length = len(inputs[\"input_ids\"][0])\\n    with torch.inference_mode():\\n        # Adjust temperature to a more stable value\\n        outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.7)\\n    return tokenizer.decode(outputs[0][inputs_length:], skip_special_tokens=True)',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet defines a function called summarize that takes in a model and text as input. It then uses a tokenizer to convert the text into a PyTorch tensor and sets the device to be used. The code then generates a summary using the model, with a maximum of 256 new tokens and a temperature of 0.7. Finally, the function returns the decoded output of the summary. The file name suggests that this code may be used for fine-tuning a model with 8-bit precision. The context mentions a function to tokenize inputs and use the model, suggesting that this code may be part of a larger process for using a pre-trained model for text summarization.'},\n",
       " {'id': '657df8e3-d1f1-47ad-a6a1-4e78ffd4daa4',\n",
       "  'embedding': None,\n",
       "  'code': 'import numpy as np\\nnp.random.seed(28)',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code imports the numpy library and sets a seed value of 28 for generating random numbers. The file name suggests that the code is related to fine-tuning a model with 8-bit precision, and the context indicates that it is used for making inferences.'},\n",
       " {'id': '00ef7978-86ec-4308-bb8d-6ad295986b9f',\n",
       "  'embedding': None,\n",
       "  'code': 'example = test_df.iloc[0]\\nprint(example.conversation)',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet loads a file named \"fine-tuning-8bits.ipynb\" and retrieves the first row of data from a dataframe named \"test_df\". It then prints the \"conversation\" column from the retrieved row. This is likely part of a larger process of fine-tuning a machine learning model using 8-bit precision.'},\n",
       " {'id': '510796f3-dd80-4660-b6f9-563d15c68e1d',\n",
       "  'embedding': None,\n",
       "  'code': 'print(example.summary)',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet is printing the summary of an example. The file name is \"fine-tuning-8bits.ipynb\", which suggests that the code is related to fine-tuning a model with 8-bit precision. The context mentions \"inferences\", indicating that the code is used to make predictions or draw conclusions from the model.'},\n",
       " {'id': '40140889-e8b2-4cb4-a2e5-f3b11bd3f587',\n",
       "  'embedding': None,\n",
       "  'code': 'print(example.prompt)',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet is printing the value of the variable \"example.prompt\". The file name is \"fine-tuning-8bits.ipynb\" and it is likely a notebook file. The context is related to inferences. Overall, the code is likely used to print out a value in a notebook file related to inferences.'},\n",
       " {'id': '8cd5450d-304c-4c34-a4b0-c9f86c56c227',\n",
       "  'embedding': None,\n",
       "  'code': '%%time\\nsummary = summarize(model, example.prompt)',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet measures and displays the time it takes to summarize a prompt using a pre-trained model. This is done in the context of fine-tuning the model using 8-bit precision.'},\n",
       " {'id': '3b2b404b-8a4b-4ef2-879b-f02325fd9446',\n",
       "  'embedding': None,\n",
       "  'code': 'print(summary)',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet is using the print function to display the summary of a file named \"fine-tuning-8bits.ipynb\". It is likely part of a larger code written in a Jupyter Notebook, and the context suggests that it is related to making inferences.'},\n",
       " {'id': '2cc3e9f2-0255-4e11-9c91-b7db1e0085c7',\n",
       "  'embedding': None,\n",
       "  'code': 'from peft import PeftModel, PeftConfig\\nfrom transformers import AutoModelForCausalLM\\n\\nconfig = PeftConfig.from_pretrained(\"morgana-rodrigues/gemma-2b-quant-8bit\")\\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\\nmodel = PeftModel.from_pretrained(model, \"morgana-rodrigues/gemma-2b-quant-8bit\")',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet is importing the necessary libraries to use PeftModel and AutoModelForCausalLM, and then configuring and loading a pre-trained model from the HuggingFace library called \"morgana-rodrigues/gemma-2b-quant-8bit\". It then fine-tunes this model using the training data in the file \"fine-tuning-8bits.ipynb\" and saves the updated model in the same location.'},\n",
       " {'id': 'dbcdd343-6e32-49db-b6d6-07f6350423ac',\n",
       "  'embedding': None,\n",
       "  'code': 'import os\\n\\ndef get_size(start_path):\\n    total_size = 0\\n    for dirpath, dirnames, filenames in os.walk(start_path):\\n        for f in filenames:\\n            fp = os.path.join(dirpath, f)\\n            # skip if it is symbolic link\\n            if not os.path.islink(fp):\\n                total_size += os.path.getsize(fp)\\n\\n    return total_size\\n\\n# Caminhos dos diretÃ³rios do modelo\\nmodel_8bit_dir = \\'/home/jovyan/.cache/huggingface/hub/models--morgana-rodrigues--gemma-2b-quant-8bit\\'\\n\\n# ObtÃ©m o tamanho do diretÃ³rio do modelo quantizado de 8 bits\\nsize_quant_8bit = get_size(model_8bit_dir)\\n\\nprint(f\"Tamanho do modelo quantizado de 8 bits: {size_quant_8bit} bytes\")\\n',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet is a function that uses the os module to get the total size of a given directory. It loops through all subdirectories and files in the directory and adds their sizes to a total size variable, skipping any symbolic links. The file name is specified as \"fine-tuning-8bits.ipynb\" and the code is being used to get the size of a specific directory, which is then printed out.'},\n",
       " {'id': 'cbf3b3ea-3548-48e1-8128-4b46d63ffb6e',\n",
       "  'embedding': None,\n",
       "  'code': 'import sys\\n\\n# Example variable\\nmy_list = model_8bit_dir\\n# Checking the size of the variable\\nsize_in_bytes = sys.getsizeof(my_list)\\n\\nprint(f\"The variable is using {size_in_bytes} bytes of memory.\")',\n",
       "  'filename': 'fine-tuning-8bits.ipynb',\n",
       "  'context': 'The code snippet is importing the \"sys\" library and then creating a variable called \"my_list\" with the value of \"model_8bit_dir\". The size of the variable is then checked using the \"getsizeof\" function from the \"sys\" library and stored in a variable called \"size_in_bytes\". Finally, the code prints a statement using the \"f-string\" method to display the size of the variable in bytes. The file name is \"fine-tuning-8bits.ipynb\" and it is related to a model available on the HuggingFace library.'},\n",
       " {'id': '9f7d8ea2-9a75-40f5-ba5c-90887f3c892d',\n",
       "  'embedding': None,\n",
       "  'code': '!pip install webvtt-py\\n!pip install pandas',\n",
       "  'filename': 'summarization-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is installing two packages, webvtt-py and pandas, using the pip command. These packages are necessary for the development of an example in the Jupyter Notebook file named \"summarization-with-langchain.ipynb\". The context explains that the example will be handling transcripts in the webvtt format and thus requires the webvtt-py library. This code is part of the process of configuring the environment for the example.'},\n",
       " {'id': '5556831c-3a12-4d23-8090-a1cc641a6deb',\n",
       "  'embedding': None,\n",
       "  'code': 'import os\\nos.environ[\"HF_HOME\"] = \"/home/jovyan/local/hugging_face\"\\nos.environ[\"HF_HUB_CACHE\"] = \"/home/jovyan/local/hugging_face/hub\"',\n",
       "  'filename': 'summarization-with-langchain.ipynb',\n",
       "  'context': 'The code snippet uses the \"os\" library to set two environment variables: \"HF_HOME\" and \"HF_HUB_CACHE\". These variables specify the location of the local HuggingFace cache directory. The file name, \"summarization-with-langchain.ipynb\", suggests that this code is used for summarization tasks with the use of a language model. The context provides additional information that this code is used to configure the HuggingFace cache so that models can be downloaded and stored locally for future use. This is a desired feature for AI Studio and the GenAI addon.'},\n",
       " {'id': '10b68744-ffd8-41bb-b177-ac41b1a0649b',\n",
       "  'embedding': None,\n",
       "  'code': 'import webvtt\\nimport pandas as pd\\n\\ndata = {\\n    \"id\": [],\\n    \"speaker\": [],\\n    \"content\": [],\\n    \"start\": [],\\n    \"end\": []\\n}\\n\\nfor caption in webvtt.read(\\'data/I_have_a_dream.vtt\\'):\\n    line = caption.text.split(\":\")\\n    while len(line) < 2:\\n        line = [\\'\\'] + line\\n    data[\"id\"].append(caption.identifier)\\n    data[\"speaker\"].append(line[0].strip())\\n    data[\"content\"].append(line[1].strip())\\n    data[\"start\"].append(caption.start)\\n    data[\"end\"].append(caption.end)\\n    \\ndf = pd.DataFrame(data)\\n\\ndf.head()\\n    \\n    ',\n",
       "  'filename': 'summarization-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is importing two libraries, webvtt and pandas, and creating an empty dictionary called \"data\" with five keys: \"id\", \"speaker\", \"content\", \"start\", and \"end\". It then uses a for loop to iterate through the captions in a .vtt file and split the text into a list, using \":\" as the delimiter. If the list has less than two items, an empty string is added to the beginning. The data from each caption is then appended to the corresponding key in the data dictionary. Finally, the data is converted into a Pandas DataFrame and the first five rows are displayed. This code is used to extract data from a .vtt file and organize it in a Pandas DataFrame for further analysis.'},\n",
       " {'id': '564de3fc-375a-4770-a342-bb91d94de3d5',\n",
       "  'embedding': None,\n",
       "  'code': 'import pandas as pd\\n\\nwith open(\"data/I_have_a_dream.txt\") as file:\\n    lines = file.read()\\n\\ndata = {\\n    \"id\": [],\\n    \"speaker\": [],\\n    \"content\": [],\\n    \"start\": [],\\n    \"end\": []\\n}\\n\\nfor line in lines.split(\"\\\\n\"):\\n    if line.strip() != \"\":\\n        data[\"id\"].append(\"\")\\n        data[\"speaker\"].append(\"\")\\n        data[\"content\"].append(line.strip())\\n        data[\"start\"].append(\"\")\\n        data[\"end\"].append(\"\")        \\n        \\ndf = pd.DataFrame(data)\\n\\ndf.head()',\n",
       "  'filename': 'summarization-with-langchain.ipynb',\n",
       "  'context': 'The code snippet uses the pandas library to import a text file named \"I_have_a_dream.txt\" and stores its contents in the variable \"lines\". It then creates an empty dictionary called \"data\" with keys for \"id\", \"speaker\", \"content\", \"start\", and \"end\". It then loops through each line in the \"lines\" variable, removes any extra spaces, and adds the line to the \"content\" key in the \"data\" dictionary. It also adds empty strings to the other keys in the dictionary. Finally, it creates a pandas DataFrame using the \"data\" dictionary and returns the first five rows using the .head() function. This code is used to load text data into a structured format for further analysis or processing.'},\n",
       " {'id': '06dfa670-98f8-4678-b5e1-4ae85c745caa',\n",
       "  'embedding': None,\n",
       "  'code': \"import numpy as np\\nfrom sentence_transformers import SentenceTransformer\\nfrom scipy.spatial.distance import cosine\\n\\nembedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\\nembeddings = embedding_model.encode(df.content)\\n\",\n",
       "  'filename': 'summarization-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is importing necessary libraries and defining a sentence embedding model. It then encodes the content from a file or dataframe using the embedding model. The file name indicates that this code is used for text summarization using the LangChain algorithm. The context provides more details on how the code is identifying relevant topics by grouping small blocks of text based on their semantic distance. This approach can be customized by adjusting parameters such as the number of topics or the method for identifying breaks between chunks of text.'},\n",
       " {'id': 'aab283a4-769a-4045-8af4-db112cfcf7d5',\n",
       "  'embedding': None,\n",
       "  'code': 'class SemanticSplitter():\\n    def __init__ (self, content, embedding_model, method=\"number\", partition_count = 10, quantile = 0.9):\\n        self.content = content\\n        self.embedding_model = embedding_model\\n        self.partition_count = partition_count\\n        self.quantile = quantile\\n        self.embeddings = embedding_model.encode(content)\\n        self.distances = [cosine(embeddings[i - 1], embeddings[i]) for i in range(1, len(embeddings))]\\n        self.breaks = []\\n        self.centroids = []\\n        self.load_breaks(method=method)\\n\\n    def centroid_distance(self, embedding_id, centroid_id):\\n        return cosine(self.embeddings[embedding], self.centroid[centroid])\\n\\n    def adjust_neighbors(self):\\n        self.breaks = []\\n\\n    def load_breaks(self, method = \\'number\\'):\\n        if method == \\'number\\':\\n            if self.partition_count > len(self.distances):\\n                self.partition_count = len(self.distances)\\n            self.breaks = np.sort(np.argpartition(self.distances, self.partition_count - 1)[0:self.partition_count - 1])\\n        elif method == \\'quantiles\\':\\n            threshold = np.quantile(self.distances, self.quantile)\\n            self.breaks = [i for i, v in enumerate(self.distances) if v >= threshold]\\n        else:\\n            self.breaks = []\\n\\n    def get_centroid(self, beginning, end):\\n        return embedding_model.encode(\\'\\\\n\\'.join(self.content[beginning : end]))\\n    \\n    def load_centroids(self):\\n        if len(self.breaks) == 0:\\n            self.centroids = [self.get_centroid(0, len(self.content))]\\n        else:\\n            self.centroids = []\\n            beginning = 0\\n            for break_position in self.breaks:\\n                self.centroids += [self.get_centroid(beginning, break_position + 1)]\\n                beginning = break_position + 1\\n            self.centroids += [self.get_centroid(beginning, len(self.content))]\\n\\n    def get_chunk(self, beginning, end):\\n        return \\'\\\\n\\'.join(self.content[beginning : end])\\n    \\n    def get_chunks(self):\\n        if len(self.breaks) == 0:\\n            return [self.get_chunk(0, len(self.content))]\\n        else:\\n            chunks = []\\n            beginning = 0\\n            for break_position in self.breaks:\\n                chunks += [self.get_chunk(beginning, break_position + 1)]\\n                beginning = break_position + 1\\n            chunks += [self.get_chunk(beginning, len(self.content))]\\n        return chunks\\n        \\n    ',\n",
       "  'filename': 'summarization-with-langchain.ipynb',\n",
       "  'context': 'The code above is defining a class called SemanticSplitter which takes in a content (transcription), an embedding model, a method (either \"number\" or \"quantiles\"), partition count, and quantile. It then encodes the content using the embedding model and calculates the distances between each embedding using cosine similarity. Based on the chosen method, it either identifies the breaks in the content as the number of chunks specified or using a quantile threshold. It then loads the centroids (representative embeddings) for each chunk and splits the content into chunks based on the identified breaks. This process allows for the content to be grouped into relevant topics for easier summarization.'},\n",
       " {'id': '6d23b2ae-e80c-4b55-91b6-b1605e75aa28',\n",
       "  'embedding': None,\n",
       "  'code': 'chunk_separator = \"\\\\n *-* \\\\n\"\\n\\nsplitter = SemanticSplitter(df.content, embedding_model, method=\"number\", partition_count=6)\\nchunks = chunk_separator.join(splitter.get_chunks())',\n",
       "  'filename': 'summarization-with-langchain.ipynb',\n",
       "  'context': 'The code snippet defines a chunk separator and then uses a semantic splitter to split the content of a data frame into smaller chunks based on a specified method and partition count. The chunks are then joined using the chunk separator. This process is used for semantic chunking of a transcript, which involves grouping small blocks of text into relevant topics for the purpose of summarization. The code also uses a semantic embedding model to identify breaks between chunks based on their semantic distance. This method can be customized by choosing the number of topics or method for identifying breaks. The code is part of a file named \"summarization-with-langchain.ipynb\" which likely contains other steps and information related to summarization using the LangChain platform.'},\n",
       " {'id': '870a8537-4388-447e-b52b-eebc430c9d14',\n",
       "  'embedding': None,\n",
       "  'code': '### Code to access model through OpenAI service\\n\\nimport os\\nfrom langchain_openai import OpenAI\\n\\nimport yaml\\nwith open(\\'secrets.yaml\\') as file:\\n    secrets = yaml.safe_load(file)\\nos.environ[\"OPENAI_API_KEY\"] = secrets[\"OpenAI\"]\\nllm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")\\n',\n",
       "  'filename': 'summarization-with-langchain.ipynb',\n",
       "  'context': \"The code snippet is accessing a Language Model (LLM) through OpenAI's service. It first imports the necessary packages and sets the OpenAI API key using a secrets file. Then, it loads the LLM model and creates a summarization prompt. The snippet also mentions other options for loading the model, such as using a Hugging Face rest API or loading it locally. The file name suggests that the code is used for summarization with langchain, and the context explains the purpose and benefits of summarizing text in chunks.\"},\n",
       " {'id': '1c3302ef-378a-4762-87d5-65aaf3a818b4',\n",
       "  'embedding': None,\n",
       "  'code': '### Alternate code to connect to Hugging Face models\\n#from langchain_huggingface import HuggingFaceEndpoint\\n\\n#import yaml\\n#with open(\\'secrets.yaml\\') as file:\\n#    secrets = yaml.safe_load(file)\\n#huggingfacehub_api_token = secrets[\"HuggingFace\"]\\n\\n#repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\\n#llm = HuggingFaceEndpoint(\\n#   huggingfacehub_api_token=huggingfacehub_api_token,\\n#   repo_id=repo_id,\\n#)\\n',\n",
       "  'filename': 'summarization-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is an alternate way to connect to Hugging Face models, which are natural language processing models. The snippet imports a library called \"langchain_huggingface\" and a file called \"secrets.yaml\" which contains API keys for Hugging Face. It then sets up a connection to a specific model repository on Hugging Face and creates an endpoint for accessing the model. \\n\\nThe file name associated with this code is \"summarization-with-langchain.ipynb\", indicating that it is likely a Jupyter notebook containing code for summarizing text using a language model. The context of the code snippet explains that the purpose of the code is to summarize text in smaller chunks, and it outlines four different options for loading and using a language model, including using a cloud API, connecting to a Hugging Face API, loading the model locally, or loading it from a file.'},\n",
       " {'id': '76a799b0-7cc5-4a96-9023-b23fd4753a51',\n",
       "  'embedding': None,\n",
       "  'code': '#from langchain_huggingface import HuggingFacePipeline\\n#from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\\n\\n#model_id = \"mistralai/Mistral-7B-v0.1\"\\n#tokenizer = AutoTokenizer.from_pretrained(model_id)\\n#model = AutoModelForCausalLM.from_pretrained(model_id)\\n#pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100, device=0)\\n#llm = HuggingFacePipeline(pipeline=pipe)',\n",
       "  'filename': 'summarization-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is importing necessary libraries and defining a pipeline for text generation using a specific model. The file name indicates that the code is related to summarization using the LangChain platform. The context suggests that the code is used to summarize large texts by breaking them into smaller chunks and using a LLM model. The different options for loading the model suggest that the code can be used with different platforms and methods, including cloud APIs, Hugging Face rest APIs, locally downloaded models, and project assets.'},\n",
       " {'id': '37f5c12e-8555-400b-aea7-c30a72ac968b',\n",
       "  'embedding': None,\n",
       "  'code': '### Alternate code to load local models. \\n###This specific example requires the project to have an asset call Llama7b, associated with the cloud S3 URI s3://dsp-demo-bucket/LLMs (public bucket)\\n\\n#from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\\n#from langchain_community.llms import LlamaCpp\\n\\n#callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\\n\\n#llm = LlamaCpp(\\n            #model_path=\"/home/jovyan/datafabric/Llama7b/ggml-model-f16-Q5_K_M.gguf\",\\n            #n_gpu_layers=64,\\n            #n_batch=512,\\n            #n_ctx=4096,\\n            #max_tokens=1024,\\n            #f16_kv=True,  \\n            #callback_manager=callback_manager,\\n            #verbose=False,\\n            #stop=[],\\n            #streaming=False,\\n            #temperature=0.4,\\n        #)',\n",
       "  'filename': 'summarization-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is loading a local LlamaCpp model for use in summarization. The model is located in the file \"ggml-model-f16-Q5_K_M.gguf\" and is associated with the cloud S3 URI s3://dsp-demo-bucket/LLMs. The code also sets certain parameters for the model, such as the number of GPU layers, batch size, context size, and maximum number of tokens. It also includes options for callbacks, verbosity, and temperature. This code is used in the file \"summarization-with-langchain.ipynb\" to summarize chunks of text separately, which can be useful for large texts or when covering multiple topics in a conversation. The code also mentions other options for loading the model, such as using a cloud API or connecting to a Hugging Face rest API.'},\n",
       " {'id': '22446ed5-4de3-4977-8d14-369f5d4c3d8a',\n",
       "  'embedding': None,\n",
       "  'code': \"prompt_template = '''\\nThe following text is an excerpt of a transcription:\\n\\n### \\n{context} \\n###\\n\\nPlease, produce a single paragraph summarizing the given excerpt.\\n'''\",\n",
       "  'filename': 'summarization-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is creating a prompt template that will be used to produce a single paragraph summary of a given excerpt. The file name indicates that the code is used for summarization with LangChain, and the context provides information on how the LangChain model will be used to summarize chunks of text. The code snippet also lists four different options for using the LangChain model, including calling an API, connecting to a Hugging Face API, loading the model locally, or loading the model from a file.'},\n",
       " {'id': 'e929780f-2a3a-43b8-b949-b5f0688d1152',\n",
       "  'embedding': None,\n",
       "  'code': 'from operator import itemgetter\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema import StrOutputParser\\nfrom langchain_core.runnables import RunnableLambda, RunnablePassthrough\\n\\n\\ndef join_summaries(summaries):\\n    return \"\\\\n\".join(list(summaries.values()))\\n\\ndef break_chunks(chunks):\\n    return chunks.split(chunk_separator)\\n\\nlambda_join = RunnableLambda(join_summaries)\\nlambda_break = RunnableLambda(break_chunks)\\n\\nprompt = ChatPromptTemplate.from_template(prompt_template)\\n\\nchain = lambda_break | {f\"summary_{i}\" : itemgetter(i) | prompt | llm  for (i, _) in enumerate(RunnablePassthrough())} | lambda_join | StrOutputParser()\\n\\n',\n",
       "  'filename': 'summarization-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is importing necessary modules and functions for summarization using LangChain. It defines a function to join multiple summaries into a single string and another function to break a string into separate chunks. Then, it creates a prompt template from a given template and sets up a parallel chain. The parallel chain uses the break_chunks function to split the input into separate chains. Each chain in the parallel chain uses itemgetter to get a specific element, personalizes the prompt template, and uses LLM inference to summarize the chunk. Finally, the individual summaries are merged into a single one using the join_summaries function. The file name is \"summarization-with-langchain.ipynb\" and the context explains how this code is used in the overall process of summarizing a transcript.'},\n",
       " {'id': 'af1e34e5-d922-40bc-b109-f10f41208dfd',\n",
       "  'embedding': None,\n",
       "  'code': 'import promptquality as pq\\n\\nimport yaml\\nwith open(\\'secrets.yaml\\') as file:\\n    secrets = yaml.safe_load(file)\\nos.environ[\\'GALILEO_API_KEY\\'] = secrets[\"Galileo\"]\\ngalileo_url = \"https://console.hp.galileocloud.io/\"\\npq.login(galileo_url)',\n",
       "  'filename': 'summarization-with-langchain.ipynb',\n",
       "  'context': 'The code above imports the necessary libraries, loads a YAML file containing secrets, sets an environmental variable with a specific API key, and connects to the Galileo console using the Prompt Quality library. This is done in preparation for the next step, which involves using a language model to summarize text.'},\n",
       " {'id': '46195311-8133-4bbe-bf56-56da19bf4b4e',\n",
       "  'embedding': None,\n",
       "  'code': '###Important observation: This code is working on version 0.64.2 of promptquality, which comes pre-installed in local-genai workspace. \\n###From version 0.65 on, changes in the format of custom scorers might cause the application to crash\\n\\nimport evaluate\\nimport time\\nimport json\\n\\ndef rouge_executor(row) -> float:\\n    print(json.loads(row.node_input))\\n    print(json.loads(row.node_output))\\n    rouge = evaluate.load(\"rouge\")\\n    reference = json.loads(row.node_input)[\"content\"]\\n    prediction =  json.loads(row.node_output)[\"content\"]\\n    rouge_values = rouge.compute(predictions =[prediction], references = [reference])\\n    return rouge_values[\"rougeL\"]\\n\\ndef rouge_aggregator(scores, indices) -> dict:\\n    if len(scores) == 0:\\n        return {\\'Average RougeL\\': sum(scores)/len(scores)}\\n    else:\\n        return {\\'Average RougeL\\': 0}\\n\\nrouge_scorer = pq.CustomScorer(name=\\'RougeL\\', executor=rouge_executor, aggregator=rouge_aggregator)',\n",
       "  'filename': 'summarization-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is creating a custom scorer for measuring the quality of summarization. It imports the necessary libraries (evaluate, time, and json) and defines two functions - rouge_executor and rouge_aggregator. The rouge_executor function takes in a row of data and extracts the reference and prediction content from it, then uses the HuggingFace implementation of ROUGE to compute the ROUGE-L score for the prediction. The rouge_aggregator function takes in a list of scores and indices, and returns the average ROUGE-L score. The code also creates a CustomScorer object named \"rouge_scorer\" with the name \"RougeL\" and uses the two defined functions as the executor and aggregator for the scorer. The file name associated with this code is \"summarization-with-langchain.ipynb\" and it is meant to be used in the context of connecting the created chain and quality metrics to Galileo.'},\n",
       " {'id': 'ae728e02-cbf3-4c88-af4e-81455db51fa8',\n",
       "  'embedding': None,\n",
       "  'code': 'summaries = chain.invoke(chunks)\\n\\npartitioned_run =  pq.EvaluateRun(\\n    project_name = \"AIStudio_summarization_template\",\\n    run_name = \"Test4 partitioned script\",\\n    scorers=[pq.Scorers.toxicity, pq.Scorers.sexist, rouge_scorer]\\n)\\n\\nstart_time = time.time()\\nresponse = chain.invoke(chunks)\\ntotal_time = int((time.time() - start_time) * 1000000)\\npartitioned_run.add_workflow(input=chunks, output=response, duration_ns= total_time) \\npartitioned_run.add_llm_step(input=chunks, output=response, duration_ns= total_time, model=\\'local\\')\\n\\npartitioned_run.finish()\\n\\n',\n",
       "  'filename': 'summarization-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is creating a chain for summarization and then running it on a given input. It also creates a partitioned run and adds the workflow and metrics to it. The file name is related to the summarization process using a language chain. The context provides additional information on how the code is connecting the metrics to Galileo and mentions two alternative ways to do so.'},\n",
       " {'id': 'b838849d-6f82-4bee-bc51-76dbb53b7f6e',\n",
       "  'embedding': None,\n",
       "  'code': '### THIS CODE IS NOT WORKING YET, AS GALILEO DOES NOT SUPPORT LISTS AS THE OUTPUT OF CHAIN NODES \\n\\n#summarization_callback =  pq.GalileoPromptCallback(\\n#    project_name = \"AIStudio_summarization_template\",\\n#    run_name = \"Partitioned transcript\",\\n#    scorers=[pq.Scorers.toxicity, pq.Scorers.sexist, rouge_scorer]\\n#)\\n\\n#summaries = chain.invoke(chunks, config={\"callbacks\": [summarization_callback]})\\n\\n',\n",
       "  'filename': 'summarization-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is attempting to create a summarization callback using the pq.GalileoPromptCallback function. This callback will be used to measure the quality of the summarization in a project called \"AIStudio_summarization_template\" and run named \"Partitioned transcript\". The callback will use three scorers - toxicity, sexist, and rouge_scorer - to measure the quality. The code also includes a chain.invoke function to run the chain and connect the metrics to Galileo. The file name is \"summarization-with-langchain.ipynb\" and the context explains the steps taken to connect to Galileo and measure the quality of the summarization using a customized run or a langchain callback.'},\n",
       " {'id': '120c27a8-cf3d-4f05-becb-718ee5b8fdf8',\n",
       "  'embedding': None,\n",
       "  'code': '%pip install --upgrade --quiet  feedparser newspaper3k listparser\\n!pip install PyMuPDF\\n',\n",
       "  'filename': 'text-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is installing several libraries, including feedparser, newspaper3k, listparser, and PyMuPDF. These libraries may be necessary for connecting with Galileo and using models for text generation. The file name indicates that the code may be related to generating text using a language model. The context provides additional information that this code is setting up the necessary environment for connecting with Galileo and using models for text generation.'},\n",
       " {'id': '29a2b92d-ef0d-4cbf-8a40-95a19b7bfafe',\n",
       "  'embedding': None,\n",
       "  'code': 'import requests\\nimport xml.etree.ElementTree as ET\\nfrom langchain_community.document_loaders import PyMuPDFLoader\\n',\n",
       "  'filename': 'text-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is used for text generation with the Langchain platform. It imports the necessary libraries and modules, including requests, xml.etree.ElementTree, and PyMuPDFLoader, and specifies the file name as \"text-generation-with-langchain.ipynb\". It then executes a search for relevant scientific papers using the arXiv API, based on a given query and maximum number of results. The results of the search are processed and information about each paper, such as title and PDF link, are extracted. The PDF file is then downloaded and the text is extracted using the PyMuPDF library. The extracted text is consolidated and presented in a string format for later use in the text generation process.'},\n",
       " {'id': '680121ca-b24b-437f-859b-7359f0ce3760',\n",
       "  'embedding': None,\n",
       "  'code': 'def search_arxiv_and_extract_text(query, max_results=1):\\n    \"\"\"\\n    Searches arXiv for articles based on a query and extracts the text from the associated PDF.\\n\\n    Parameters:\\n    - query (str): The search term to use in the arXiv search.\\n    - max_results (int): The maximum number of results to return from the search. Default is 1.\\n    \\n    Returns:\\n    - papers (list): A list of dictionaries containing \\'title\\' and \\'text\\' for each article.\\n    \"\"\"\\n    url = f\\'http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={max_results}\\'\\n    response = requests.get(url)\\n    \\n    papers = []  \\n    \\n    if response.status_code == 200:\\n        root = ET.fromstring(response.content)\\n        \\n        for entry in root.findall(\\'{http://www.w3.org/2005/Atom}entry\\'):\\n            title = entry.find(\\'{http://www.w3.org/2005/Atom}title\\').text\\n            pdf_url = None\\n            \\n            for link in entry.findall(\\'{http://www.w3.org/2005/Atom}link\\'):\\n                if link.attrib.get(\\'type\\') == \\'application/pdf\\':\\n                    pdf_url = link.attrib[\\'href\\']\\n                    break\\n            \\n            if pdf_url:\\n                pdf_path = f\"temp_{title[:50].strip().replace(\\' \\', \\'_\\')}.pdf\"\\n                pdf_downloaded = download_pdf(pdf_url, pdf_path)\\n                \\n                if pdf_downloaded:\\n                    loader = PyMuPDFLoader(pdf_path)\\n                    docs = loader.load()\\n                    text = \"\\\\n\".join([doc.page_content for doc in docs])\\n                    \\n                    papers.append({\\n                        \\'title\\': title.strip(),\\n                        \\'text\\': text\\n                    })\\n                    print(f\"Text extracted from the article \\'{title.strip()}\\':\\\\n{text[:500]}...\")\\n                else:\\n                    print(f\"Error downloading article PDF \\'{title.strip()}\\'.\")\\n            else:\\n                arxiv_url = entry.find(\\'{http://www.w3.org/2005/Atom}id\\').text\\n                print(f\"No PDF link found for the article \\'{title.strip()}\\'. You can view it online here: {arxiv_url}\")\\n    else:\\n        print(\"Error accessing arXiv.\")\\n    \\n    return papers\\n\\ndef download_pdf(pdf_url, output_path):\\n    \"\"\"\\n    Downloads a PDF file from a URL and saves it locally.\\n\\n    Parameters:\\n    - pdf_url (str): The URL of the PDF to be downloaded.\\n    - output_path (str): The path where the PDF will be saved locally.\\n    \"\"\"\\n    response = requests.get(pdf_url)\\n    if response.status_code == 200:\\n        with open(output_path, \\'wb\\') as f:\\n            f.write(response.content)\\n        return True\\n    else:\\n        print(f\"Error downloading PDF: {response.status_code}\")\\n        return False',\n",
       "  'filename': 'text-generation-with-langchain.ipynb',\n",
       "  'context': \"The code snippet is a part of a larger pipeline that searches for relevant scientific papers on the arXiv platform and extracts the text from their associated PDFs. It takes in a query (search term) and a maximum number of results as inputs, and then uses the arXiv API to search for papers that match the query. It processes the response and downloads the PDFs of the matching papers, and then uses the PyMuPDF library to extract the text from the PDFs. The extracted text is returned in a list of dictionaries, with each dictionary containing the paper's title and extracted text.\"},\n",
       " {'id': 'ec4faa47-a6fd-4f69-9160-6b3bb30194af',\n",
       "  'embedding': None,\n",
       "  'code': 'query = \"RAG\"  \\npapers = search_arxiv_and_extract_text(query, max_results=1)',\n",
       "  'filename': 'text-generation-with-langchain.ipynb',\n",
       "  'context': 'The code above performs a search for relevant scientific papers on the arXiv platform using the provided query term. It then sends an HTTP request to the arXiv API and processes the response to extract information about the papers found. After downloading the PDF files for the papers, the code uses the PyMuPDF library to extract the text from the papers and presents it in a string format. This text can be used in later stages for generating embeddings. The code is part of a pipeline for text generation and is stored in a file named \"text-generation-with-langchain.ipynb\".'},\n",
       " {'id': 'cf2f1fe4-a86e-494f-87fa-16a4ebc15154',\n",
       "  'embedding': None,\n",
       "  'code': 'papers',\n",
       "  'filename': 'text-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is part of a pipeline for generating text using the LangChain model, and is specifically focused on searching for relevant scientific papers. The file name indicates that this code is written in the Jupyter Notebook format. The context provides some background information on the overall pipeline.\\n\\nThe code performs the following steps:\\n1. Allows the user to provide a query (term or topic of interest) and specify the maximum number of results to be returned.\\n2. Sends a request to the arXiv API with the query and retrieves information about matching papers.\\n3. Processes the response to extract the title and PDF link for each paper.\\n4. Downloads the PDFs of the papers using the retrieved links.\\n5. Extracts the text from the downloaded PDFs using the PyMuPDF library and consolidates it into a string format.'},\n",
       " {'id': '4e4790b3-1440-46a7-b1a1-783b95305ffa',\n",
       "  'embedding': None,\n",
       "  'code': '# Import the Document class to structure the text and its metadata \\n# Import the RecursiveCharacterTextSplitter to split the text into smaller parts\\nfrom langchain.schema import Document\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# Creates a list of Document objects from the scientific articles in the `papers` variable.\\n# Each `Document` is created with the article content and a metadata dictionary containing the title.\\ndocuments = [Document(page_content=paper[\\'text\\'], metadata={\"title\": paper[\\'title\\']}) for paper in papers]\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=400)\\nsplits = text_splitter.split_documents(documents)\\n\\n',\n",
       "  'filename': 'text-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet imports the necessary modules and classes from LangChain to structure and process text data. It then creates a list of Document objects from a variable containing scientific articles, with each Document containing the article content and title metadata. The RecursiveCharacterTextSplitter is then used to split the Document objects into smaller chunks, with a specified chunk size and overlap. These chunks are then converted into embeddings using the HuggingFaceEmbeddings class. Overall, the code is part of a text generation process using LangChain, involving document creation, text splitting, and embedding generation.'},\n",
       " {'id': 'f514e42a-7d76-47bb-8390-fca836991f9b',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_huggingface import HuggingFaceEmbeddings\\n\\nembeddings= HuggingFaceEmbeddings()\\n\\n',\n",
       "  'filename': 'text-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is importing the HuggingFaceEmbeddings library and creating an instance of it. The HuggingFaceEmbeddings class is used for generating embeddings, which are vector representations of text. The file name suggests that it is used for text generation, and the context confirms this by mentioning the steps involved in processing and generating embeddings from text. Specifically, the code snippet is used to transform text into Document objects, segment them into smaller chunks, and then convert those chunks into embeddings using the HuggingFaceEmbeddings class. This process is likely part of a larger text generation project being done with the LangChain platform.'},\n",
       " {'id': '7a10cd70-f1f5-46b5-a97b-e99f7fea61b4',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.vectorstores import Chroma\\n#Our vector database\\nvectordb = Chroma.from_documents(documents=splits, embedding=embeddings)\\n',\n",
       "  'filename': 'text-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet imports the Chroma vector storage library and creates a vector database named \"vectordb\" by storing the embeddings of the given text chunks. This is done using the Chroma.from_documents function. The file name suggests that this code is part of a text generation project using the Langchain library. The context explains that this step involves storing and retrieving vector data and implementing a similarity search function. The code snippet specifically deals with the storage and retrieval of embeddings in the vector database using Chroma, and defines a function for performing similarity searches within the database.'},\n",
       " {'id': '03c7fe56-975a-4d4a-9c71-76e26c653bfe',\n",
       "  'embedding': None,\n",
       "  'code': 'retriever = vectordb.as_retriever()\\n',\n",
       "  'filename': 'text-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet initializes a retriever object using the vectordb library, which is used for storing and retrieving vector data. The file name suggests that this code is used for text generation with a tool called LangChain. The context explains that the code is part of a pipeline for storing generated embeddings in a vector database and implementing a similarity search function. This involves storing the embeddings in the database using Chroma, configuring a retriever, and defining a function for performing similarity searches in the database. The query_vectordb function takes a query as input and returns the top k most similar results from the database.'},\n",
       " {'id': '50e74355-02e0-4ac8-9ae3-15891e72a8e7',\n",
       "  'embedding': None,\n",
       "  'code': 'def query_vectordb(vectordb, query, k=10):\\n    \"\"\"\\n    Performs a similarity search in a vector database.\\n    Parameters:\\n    - vectordb: The vector database where the embeddings are stored.\\n    - query: The query that will be used to search for the most similar embeddings in the database.\\n    - k: The number of most similar results that should be returned (default is 10).\\n    \"\"\"\\n    return vectordb.similarity_search(query, k=k)',\n",
       "  'filename': 'text-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet defines a function called \"query_vectordb\" that performs a similarity search in a vector database. It takes in three parameters - the vector database, the query, and an optional parameter for the number of results to be returned. The function then calls the \"similarity_search\" method on the vector database with the given query and returns the top k most similar results. This code is used in the context of implementing a retriever for a text generation pipeline, where the generated embeddings are stored in a vector database and can be retrieved using this function. The file \"text-generation-with-langchain.ipynb\" likely contains the entire code for this pipeline.'},\n",
       " {'id': 'ae4cee16-a435-45ad-b471-7306dcd8f35a',\n",
       "  'embedding': None,\n",
       "  'code': 'query = \"LLM\"\\nquery_vectordb(vectordb, query)',\n",
       "  'filename': 'text-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is part of a text generation pipeline that stores generated embeddings in a vector database and implements a similarity search function for specific queries. The file name suggests that this code is part of a larger project involving text generation and language modeling. The context provides further information about the steps involved in this specific part of the pipeline, including embedding storage, retriever configuration, and the definition of a similarity search function. The query_vectordb function specifically performs similarity searches in the vector database, taking a query as input and returning the top k most similar results.'},\n",
       " {'id': '673c77c9-4357-4a68-b118-c6882305966c',\n",
       "  'embedding': None,\n",
       "  'code': 'import getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")',\n",
       "  'filename': 'text-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is importing the modules \"getpass\" and \"os\" to be used in the program. It then checks if there is an environment variable called \"OPENAI_API_KEY\", and if not, prompts the user to enter their OpenAI API key. The file name is \"text-generation-with-langchain.ipynb\", which suggests that the code is related to generating text using LangChain. The context indicates that the code is part of a larger process, where the Galileo and OpenAI APIs are being used for login. Ultimately, this code is setting up the necessary environment for the program to use the OpenAI API for text generation.'},\n",
       " {'id': '8024d8d8-fa9f-484b-8ce8-2807b2b3bcc0',\n",
       "  'embedding': None,\n",
       "  'code': \"os.environ['GALILEO_API_KEY'] = ''  # Replace with your API key\\nos.environ['GALILEO_CONSOLE_URL'] = 'https://console.hp.galileocloud.io/'\\nGALILEO_PROJECT_NAME = 'Academic Script'\\nconfig = pq.login(os.environ['GALILEO_CONSOLE_URL'])\",\n",
       "  'filename': 'text-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet sets the environment variables for the Galileo API key and console URL, and assigns a project name. Then, it uses the pq.login function to log in using the console URL. The file name suggests that this code is used for text generation with the Langchain model.'},\n",
       " {'id': 'caea2d78-83f5-45ca-80d7-bcec978a8db6',\n",
       "  'embedding': None,\n",
       "  'code': 'from typing import List\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.schema.runnable import RunnablePassthrough\\nfrom langchain.schema import StrOutputParser\\nimport promptquality as pq\\nfrom promptquality import NodeRow, NodeType\\nimport uuid\\n\\ndef format_docs(docs: List[Document]) -> str:\\n    \"\"\"\\n    Function that formats the contents of a list of documents into a single string.\\n    Parameters:\\n    - docs (List[Document]): List of Document objects, where each document contains a textual content.\\n\\n    Returns:\\n    - str: A single string that joins the content of all documents, separated by two line breaks.\\n    \\n    \"\"\"\\n    formatted_docs = \"\\\\n\\\\n\".join([d.page_content for d in docs])\\n    return formatted_docs\\n\\ntemplate = \"\"\"You are tasked with analyzing a scientific paper and responding to a series of steps or questions based on the paper\\'s content. \\nYour goal is to provide accurate, contextual responses for each step, drawing from the paper\\'s information and your own knowledge when necessary.\\nHere is the paper you will be analyzing:\\n    {context}\\n\\n1. Read the step carefully.\\n2. Search for relevant information in the paper that addresses the step.\\n3. If the paper contains information directly related to the step, use that information to formulate your response.\\n4. If the paper does not contain information directly related to the step, but the topic is related to the paper\\'s content, use your own knowledge to provide a response that is consistent with the paper\\'s context and subject matter.\\n    Question: {question}\\n    \"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\nmodel = ChatOpenAI()\\n\\nchain = {\\n    \"context\": lambda inputs: format_docs(query_vectordb(vectordb, inputs[\\'query\\'])), \\n    \"question\": RunnablePassthrough()\\n} | prompt | model | StrOutputParser()\\n\\n',\n",
       "  'filename': 'text-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is setting up an environment for text generation using the Langchain library. It imports necessary modules and defines a function for formatting a list of documents into a single string. It also creates a template for a chat prompt that will be used for responding to a series of steps or questions based on a scientific paper. The \"context\" and \"question\" variables are placeholders for inputs that will be provided later. The code then creates a chain that will handle the inputs and pass them through the prompt and model to generate a response. The file name is \"text-generation-with-langchain.ipynb\" and the context is setting up the necessary environment for using the Galileo and OpenAI APIs.'},\n",
       " {'id': '13d2a847-71f9-4af7-b88b-1e34d4761aac',\n",
       "  'embedding': None,\n",
       "  'code': '\"\"\"\\nInstantiate your local model with `llama.cpp`\\n\\n\"\"\"\\n### Alternate code to load local models. \\n###This specific example requires the project to have an asset call Llama7b, associated with the cloud S3 URI s3://dsp-demo-bucket/LLMs (public bucket)\\n\\n# Instantiate your local model with `llama.cpp`\\n\\n#from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\\n#from langchain_community.llms import LlamaCpp\\n\\n# Initialize the callback manager, which handles streaming output\\n#callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\\n\\n# Configure the local model using `LlamaCpp`\\n#llm_local = LlamaCpp(\\n#    model_path=\"/home/jovyan/datafabric/Llama7b/ggml-model-f16-Q5_K_M.gguf\",  # Path to the model\\n#    n_gpu_layers=64,  # Number of model layers to be processed on the GPU\\n#    n_batch=512,  # Batch size for processing\\n#    n_ctx=4096,  # Context size for tokenization\\n#    max_tokens=1024,  # Maximum number of tokens for generation\\n#    f16_kv=True,  # Use 16-bit precision for key-value storage\\n#    callback_manager=callback_manager,  # Callback manager for streaming output\\n#    verbose=False,  # Sets verbosity (whether detailed logs should be shown)\\n#    stop=[],  # Tokens to stop the generation\\n#    streaming=False,  # Sets whether the output should be streamed in real-time\\n#    temperature=0.4  # Controls the randomness of the model\\'s responses\\n#)\\n\\n# Combine the prompt with the configured local model (`llm_local`)\\n#llm_chain = prompt | llm_local\\n',\n",
       "  'filename': 'text-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is instantiating a local model using the LlamaCpp library. It is configuring the model with specific parameters such as model path, number of layers, batch size, context size, maximum tokens, and temperature. It also sets up a callback manager for streaming output and allows for the use of 16-bit precision for key-value storage. The code also combines a prompt with the configured local model. This configuration allows for text generation using the model. The file name indicates that this code may be used for text generation using Langchain technology.'},\n",
       " {'id': '85684070-a184-4dee-91e4-837f9b0b7168',\n",
       "  'embedding': None,\n",
       "  'code': '\"\"\"\\nChain with Local model\\n\"\"\"\\n\\n#from typing import List\\n#from langchain.prompts import ChatPromptTemplate\\n#from langchain.schema.runnable import RunnablePassthrough\\n#from langchain.schema import StrOutputParser\\n#import uuid\\n\\n#def format_docs(docs: List[Document]) -> str:\\n#    return \"\\\\n\\\\n\".join([doc.page_content for doc in docs])\\n\\n#template = \"\"\"You are a Python wizard tasked with generating code for a Jupyter Notebook (.ipynb) based on the given context.\\n#Your answer should consist of just the Python code, without any additional text or explanation.\\n\\n#Context:\\n#{context}\\n\\n#Question: {question}\\n#\"\"\"\\n\\n#prompt = ChatPromptTemplate.from_template(template)\\n\\n#model = llm_local()\\n\\n#chain = {\\n#    \"context\": lambda inputs: format_docs(retriever(inputs[\\'query\\'], collection)),  \\n#    \"question\": RunnablePassthrough()  \\n#} | prompt | model | StrOutputParser()\\n',\n",
       "  'filename': 'text-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet sets up a chain for generating code for a Jupyter Notebook based on a given context. It imports necessary libraries and defines a function for formatting documents. It also sets up a template for the prompt and configures a local model. The chain then uses the context and question inputs to generate code using the prompt, model, and output parser. The resulting code is then stored in a .ipynb file.'},\n",
       " {'id': '19457de7-0e21-4f5c-9402-82ecd103dacd',\n",
       "  'embedding': None,\n",
       "  'code': 'import time\\n\\ndef run_and_approve(variable_name, template, question, vectordb, k=10):\\n    \"\"\"\\n    Function that executes a chain of prompts to generate text based on a template and a question,\\n    with the option of manually approving the result.\\n\\n    Parameters:\\n    - variable_name (str): Name of the variable to be used when printing the result.\\n    - template (str): Prompt template that will be used as context to generate the text.\\n    - question (str): Specific task that the model must perform, such as \"create a title\" or \"generate an introduction\".\\n    - vectordb: Vector database used to search for documents related to the context.\\n    - k (int, optional): Number of documents to be retrieved from the vector database. The default is 5.\\n\\n    Returns:\\n    - str: The text generated and approved by the user based on the template and the question.\\n    \"\"\"\\n    while True:\\n        # Configure a new prompt_handler for each step\\n        prompt_handler = pq.GalileoPromptCallback(\\n            scorers=[\\n                pq.Scorers.context_adherence_plus,\\n                pq.Scorers.correctness,\\n                pq.Scorers.prompt_perplexity\\n            ]\\n        )\\n\\n        # Execute the chain in batch with the template as query and a specific question\\n        result = chain.batch([{\"query\": template, \"question\": question}], config=dict(callbacks=[prompt_handler]))\\n\\n        if not result:\\n            continue\\n\\n        # Print the variable name and result\\n        print(f\"{variable_name}: {result[0]}\")\\n\\n        approval = input(\"Approve the result? (y/n): \").strip().lower()\\n        if approval == \\'y\\':\\n            # Finalize and send the prompt to Galileo\\n            prompt_handler.finish()\\n            return result[0]  # Return the approved result\\n\\n        print(\"Result not approved, generating again...\\\\n\")\\n\\n# Example execution for each cell\\n\\n\\ntitle_template = \"\"\"\\nGenerate a title for the presentation that is clear, concise, and reflects the content. Add a subtitle if needed.\\n\"\"\"\\ntitle_result = run_and_approve(\"title_template\", title_template, question=\"create a title\", vectordb=vectordb)\\n\\n# Introduction\\nintroduction_template = \"\"\"\\nGenerate an introduction that includes:\\n- Contextualization of the general theme.\\n- Relevance of the topic, both academically and practically.\\n- A brief literature review.\\n- A clear definition of the research problem.\\n- The specific objectives of the research.\\n- Hypotheses (if applicable).\\n\"\"\"\\nintroduction_result = run_and_approve(\"introduction_template\", introduction_template, question=\"generate an introduction\", vectordb=vectordb)\\n\\n# Methodology\\nmethodology_template = \"\"\"\\nGenerate the methodology section, including:\\n- Research Design (e.g., experimental, descriptive, exploratory).\\n- Sample and Population details.\\n- Data Collection methods.\\n- Instruments used for data collection.\\n- Data Analysis techniques.\\n\"\"\"\\nmethodology_result = run_and_approve(\"methodology_template\", methodology_template, question=\"generate the methodology\", vectordb=vectordb)\\n\\n#  Results\\nresults_template = \"\"\"\\nGenerate the results section, including:\\n- Presentation of Data with visual aids like graphs and tables.\\n- Initial Interpretation of the data.\\n- Comparison with Hypotheses (if applicable).\\n\"\"\"\\nresults_result = run_and_approve(\"results_template\", results_template, question=\"generate the results\", vectordb=vectordb)\\n\\n# Conclusion\\nconclusion_template = \"\"\"\\nGenerate the conclusion of the study, including:\\n- Synthesis of Results.\\n- Response to the Research Problem.\\n- Study Contributions.\\n- Final Reflection on the study\\'s impact or practical recommendations.\\n\"\"\"\\nconclusion_result = run_and_approve(\"conclusion_template\", conclusion_template, question=\"generate the conclusion\", vectordb=vectordb)\\n\\n# References\\nreferences_template = \"\"\"\\nGenerate the list of references for the study, ensuring that:\\n- All sources cited in the presentation are included.\\n- The references are formatted according to a specific style (APA, MLA, Chicago).\\n\"\"\"\\nreferences_result = run_and_approve(\"references_template\", references_template, question=\"generate the references\", vectordb=vectordb)\\n\\n# Combine the results into a final script\\nfinal_script = f\"{title_result}\\\\n\\\\n{introduction_result}\\\\n\\\\n{methodology_result}\\\\n\\\\n{results_result}\\\\n\\\\n{conclusion_result}\\\\n\\\\n{references_result}\"\\nprint(\"Final Script:\\\\n\", final_script)\\n',\n",
       "  'filename': 'text-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is a function called \"run_and_approve\" that is used to generate text based on a template and a specific task or question. It uses a vector database to retrieve relevant context and allows for manual approval or rejection of the generated output. The function also provides a mechanism to regenerate the text if it is not approved. This function is used in a notebook called \"text-generation-with-langchain.ipynb\" to generate text for various sections of a presentation, such as the title, introduction, methodology, results, conclusion, and references. The final generated text is then combined into a final script.'},\n",
       " {'id': 'fac7ded2-83fc-4577-805c-b623b33dfa53',\n",
       "  'embedding': None,\n",
       "  'code': '%pip install --upgrade --quiet  GitPython',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is using the pip package manager to install the library GitPython, with the options to upgrade and do so quietly. The file name indicates that this code is being used for code generation with LangChain, and it is being done in the context of configuring the environment for a project. This likely involves installing various libraries, such as LangChain, Huggingface models, OpenAI, and ChromaDB, to enable code generation and storage of embeddings. The code is being executed in a Jupyter notebook (with the .ipynb extension) and is the first step in setting up the environment for the project.'},\n",
       " {'id': '0218bc56-b1d7-4602-b2bb-684a6069a3c5',\n",
       "  'embedding': None,\n",
       "  'code': 'import os\\nimport git\\nimport nbformat\\nimport uuid\\n\\n# Function to clone GitHub repository\\ndef clone_repo(repo_url, clone_dir=\"./temp_repo\"):\\n    if not os.path.exists(clone_dir):\\n        os.makedirs(clone_dir)\\n    git.Repo.clone_from(repo_url, clone_dir)\\n    print(f\"Repository cloned in: {clone_dir}\")\\n\\n# Function to find all .ipynb notebooks in a directory\\ndef find_all_notebooks(directory):\\n    notebooks = []\\n    for root, dirs, files in os.walk(directory):\\n        for file in files:\\n            if file.endswith(\".ipynb\"):\\n                notebooks.append(os.path.join(root, file))\\n    return notebooks #return code\\n\\n# Function to extract code and context from notebooks\\ndef extract_code_and_context(notebook_path):\\n    with open(notebook_path, \\'r\\', encoding=\\'utf-8\\') as f:\\n        notebook = nbformat.read(f, as_version=4)\\n\\n    extracted_data = []\\n    for cell in notebook[\\'cells\\']:\\n        if cell[\\'cell_type\\'] == \\'markdown\\':\\n            context = \\'\\'.join(cell[\\'source\\'])\\n        elif cell[\\'cell_type\\'] == \\'code\\':\\n            cell_data = {\\n                \"id\": str(uuid.uuid4()),  \\n                \"embedding\": None,        \\n                \"code\": \\'\\'.join(cell[\\'source\\']),\\n                \"filename\": os.path.basename(notebook_path),\\n                \"context\": context if \\'context\\' in locals() else \\'\\'\\n            }\\n            extracted_data.append(cell_data)\\n\\n    return extracted_data\\n',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code above performs a series of operations to clone a GitHub repository, locate all Jupyter Notebook files within the cloned directory, and extract code and context from each notebook. It includes functions to clone the repository, find all notebooks, and extract code and context from each notebook file. The extracted data is then stored in a list for further processing. This code is typically used for code generation tasks using a language model.'},\n",
       " {'id': '311ab44d-7468-43b7-9dfe-f06a39f1c9ed',\n",
       "  'embedding': None,\n",
       "  'code': '# Cloning the repository and performing the extraction\\nrepo_url = \"https://github.com/sergiopaniego/RAG_local_tutorial.git\"  #your repository\\nclone_repo(repo_url)\\n\\n# Locate and process notebooks\\nclone_dir = \"./temp_repo\"\\nnotebooks = find_all_notebooks(clone_dir)\\n\\nall_extracted_data = []\\n\\nfor notebook in notebooks:\\n    print(f\"Extracting data from: {notebook}\")\\n    extracted_data = extract_code_and_context(notebook)  \\n    all_extracted_data.extend(extracted_data)\\n\\nprint(\"Extraction completed.\")',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is performing the process of cloning a GitHub repository, searching for Jupyter Notebook files, and extracting both code and context from these notebooks. The file name \"code-generation-with-langchain.ipynb\" suggests that this code is specifically for generating code using LangChain, a natural language processing tool. The optional context provides additional information and background on the overall process of the code, including the steps involved.'},\n",
       " {'id': '397e72ee-d0c3-4fc7-90ad-0af6a886bedc',\n",
       "  'embedding': None,\n",
       "  'code': 'all_extracted_data',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The given code snippet is part of a process for generating code using the LangChain framework. The code snippet is used to extract code and context from Jupyter Notebook files (.ipynb) in a specified repository. The code first clones the desired GitHub repository into a temporary directory, then searches for all Jupyter Notebook files within the cloned directory. After locating the notebooks, the code then extracts the code and any associated markdown context from each notebook. The extracted code and context are then stored in a list for later use in the code generation process.'},\n",
       " {'id': 'b4fed8af-8e94-4d07-bc98-409a19a2e4b8',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_core.prompts import PromptTemplate\\nfrom langchain_openai import OpenAI',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is used to generate explanatory metadata for extracted code snippets using a language model (LLM). It defines a prompt template with placeholders for the code snippet, file name, and optional context, which is then used to create a PromptTemplate object. The OpenAI LLM is then used to process the information and generate responses. Finally, the function update_context_with_llm is used to iterate through the extracted code and replace the original context field with the generated explanations. This ultimately enriches the data structure with clear explanations for each code snippet, making it easier to understand and use the information later. This process is used in a Jupyter notebook with the file name \"code-generation-with-langchain.ipynb\" during the second step of generating metadata with the LLM.'},\n",
       " {'id': '63522bf7-9848-49f8-911e-f65810374aa4',\n",
       "  'embedding': None,\n",
       "  'code': 'os.environ[\"OPENAI_API_KEY\"] = \"\" #your api key',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is setting the environment variable \"OPENAI_API_KEY\" to an empty string, which is likely used for authentication purposes. The file name is \"code-generation-with-langchain.ipynb\", indicating that the code is part of a notebook for generating code using a language model. The code is using a language model to generate explanations and metadata for code snippets, using a defined prompt template that includes the code snippet, file name, and optional context as placeholders. The OpenAI language model is authenticated using the API key and is used to process the information and generate responses. The function \"update_context_with_llm\" iterates through the extracted code and uses the language model to replace the original context with the generated explanation. This ultimately enriches the original data structure with clear explanations for each code snippet, making it easier to understand and use in the future.'},\n",
       " {'id': '2389af38-8a58-48f8-892d-ac451fe7fbb0',\n",
       "  'embedding': None,\n",
       "  'code': 'template = \"\"\"\\nYou will receive three pieces of information: a code snippet, a file name, and an optional context. Based on this information, explain in a clear, summarized and concise way what the code snippet is doing.\\n\\nCode:\\n{code}\\n\\nFile name:\\n{filename}\\n\\nContext:\\n{context}\\n\\nDescribe what the code above does.\\n\"\"\"\\n\\nprompt = PromptTemplate.from_template(template)\\n\\n',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is used to generate explanatory metadata for extracted code, using a language model. It defines a prompt template that includes placeholders for the code snippet, file name, and optional context. The OpenAI LLM is then used to process this information and generate responses, which are used to update the context field in a data structure containing the extracted code. This ultimately enriches the data structure by providing clear explanations for each code snippet, making it easier to understand and use the information later.'},\n",
       " {'id': '64965c45-f6b8-4414-a37f-cc05634ffe46',\n",
       "  'embedding': None,\n",
       "  'code': 'llm = OpenAI()\\n\\nllm_chain = prompt | llm\\n',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet uses a language model (LLM) to generate descriptions and explanatory metadata for each extracted code snippet. It does this by defining a prompt template that contains placeholders for the code snippet, the file name, and an optional context. The OpenAI LLM, authenticated with an API key, is then used to process this information and generate responses. The function update_context_with_llm iterates through the data structure containing the code snippets, runs the language model for each item, and replaces the original context field with the explanation generated by the AI. This helps enrich the original data structure by providing clear explanations for each code snippet, making it easier to understand and use the information later.'},\n",
       " {'id': '8705ef11-3593-4faa-b260-9a6f725c157b',\n",
       "  'embedding': None,\n",
       "  'code': '### Alternate code to load local models. \\n###This specific example requires the project to have an asset call Llama7b, associated with the cloud S3 URI s3://dsp-demo-bucket/LLMs (public bucket)\\n\\n# from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\\n# from langchain_community.llms import LlamaCpp\\n\\n# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\\n\\n# llm_local = LlamaCpp(\\n            # model_path=\"/home/jovyan/datafabric/Llama7b/ggml-model-f16-Q5_K_M.gguf\",\\n            # n_gpu_layers=64,\\n            # n_batch=512,\\n            # n_ctx=4096,\\n            # max_tokens=1024,\\n            # f16_kv=True,  \\n            # callback_manager=callback_manager,\\n            # verbose=False,\\n            # stop=[],\\n            # streaming=False,\\n            # temperature=0.4,\\n        # )\\n\\n# llm_chain = prompt | llm_local\\n',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is using the LlamaCpp and StreamingStdOutCallbackHandler libraries to load a local model (Llama7b) for the purpose of generating metadata. The model is associated with a specific S3 URI and the code specifies various parameters such as number of GPU layers, batch size, and maximum tokens. The code also includes a callback manager and sets a temperature value for the generated metadata. Finally, the code executes a prompt and feeds it into the local model for metadata generation. The file \"code-generation-with-langchain.ipynb\" likely contains additional code for the metadata generation process.'},\n",
       " {'id': '65c48b56-c050-459b-868f-b3c827e851ce',\n",
       "  'embedding': None,\n",
       "  'code': 'import httpcore\\n\\ndef update_context_with_llm(data_structure):\\n    updated_structure = []\\n    \\n    for item in data_structure:\\n        code = item[\\'code\\']\\n        filename = item[\\'filename\\']\\n        context = item[\\'context\\']\\n        \\n        try:\\n            # Try calling an LLM to generate code explanation\\n            response = llm_chain.invoke({\\n                \"code\": code, \\n                \"filename\": filename, \\n                \"context\": context\\n            })\\n            \\n            # Update item with LLM response\\n            item[\\'context\\'] = response.strip()\\n        \\n        except httpcore.ConnectError as e:\\n            # API or model connection specific error\\n            print(f\"Connection error processing file {filename}:The connection to the API or model has been corrupted. Details: {str(e)}\")\\n            # Keep the original context in case of error\\n            item[\\'context\\'] = context\\n        \\n        except httpcore.ProtocolError as e:\\n            # Protocol error, similar to the original error mentioned\\n            print(f\"Protocol error when processing the file {filename}: {str(e)}\")\\n            # Keep the original context\\n            item[\\'context\\'] = context\\n        \\n        except Exception as e:\\n            # Other general errors\\n            print(f\"Error processing the file {filename}: {str(e)}\")\\n            # Keep the original context\\n            item[\\'context\\'] = context\\n        \\n        # Add the updated item (or not) to the structure\\n        updated_structure.append(item)\\n    \\n    return updated_structure\\n',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': \"The code is a function that takes in a data structure as an input and loops through each item in the structure. For each item, it extracts the code, file name, and optional context. It then tries to call an LLM (language model) to generate an explanation for the code, using the extracted information. If the LLM call is successful, the item's context is updated with the response from the LLM. If there is a connection or protocol error, the original context is kept. Any other general errors are also caught and the original context is kept. Finally, the updated item is added to a new data structure and returned as the output. The code is likely used for generating metadata for code snippets using a local model with LlamaCPP.\"},\n",
       " {'id': '0d40f604-9597-4ffa-8f27-e4c68972391e',\n",
       "  'embedding': None,\n",
       "  'code': 'updated_data = update_context_with_llm(all_extracted_data)',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet updates the context with the updated data using a function called \"update_context_with_llm\", which takes in a variable called \"all_extracted_data\". This code is likely part of a larger code generation process and is specifically designed for use with a local model and LlamaCPP. The file name suggests that this code is used for code generation, likely in a Jupyter notebook file. The context provides additional information about the code and mentions the use of a local model with LlamaCPP for generating metadata. Overall, the code is updating context with new data for code generation using a specific model and programming language.'},\n",
       " {'id': '0f0d4a41-cdbc-4daa-85ef-b8db768ad9da',\n",
       "  'embedding': None,\n",
       "  'code': 'updated_data',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is updating data and the file name is related to code generation using Langchain. The optional context is about generating metadata with a local model using LlamaCPP. This code updates data related to metadata generation using a local model with LlamaCPP.'},\n",
       " {'id': 'f95c4c05-bad2-4015-9fcd-cfc5f53d669e',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_huggingface import HuggingFaceEmbeddings\\n\\nembeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet uses the HuggingFace embeddings model to generate embedding vectors for the context extracted from each code snippet. It iterates through the previously extracted data structure and updates each item with the new embedding vector. The code then converts the updated data structure into a Pandas DataFrame, making it easier to manipulate and use the data in the future. This process is repeated for each code snippet, with the results stored in the DataFrame for easy visualization and further processing. Overall, the code is used to generate embeddings and structure data for code snippets using the HuggingFace embeddings model.'},\n",
       " {'id': '7c7c8e7c-f16a-427d-a047-20ebc1b2a1d8',\n",
       "  'embedding': None,\n",
       "  'code': \"def update_embeddings(data_structure):\\n    updated_structure = []\\n    for item in data_structure:\\n        context = item['context']\\n\\n        # Generate the embedding for the context\\n        embedding_vector = embeddings.embed_query(context)\\n\\n        # Update the item with the new embedding\\n        item['embedding'] = embedding_vector\\n        updated_structure.append(item)\\n    \\n    return updated_structure\\n\",\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is a function called \"update_embeddings\" that takes in a data structure as input and returns an updated version of that structure. It uses an embeddings model to generate embedding vectors for the context extracted from each code snippet. The function iterates through the data structure and for each item, it generates an embedding vector from the context and updates the item with this new embedding. The updated items are then added to a new list, which is returned as the updated structure. The file name is \"code-generation-with-langchain.ipynb\" and the optional context is a step-by-step explanation of how the embeddings are generated and used to update the data structure. The final result is a DataFrame that contains the code snippets, their metadata, and the generated embedding vectors for easy visualization and further processing.'},\n",
       " {'id': '9f67f7c3-7d80-439d-b633-5a8512cc49f6',\n",
       "  'embedding': None,\n",
       "  'code': 'updated_structure = update_embeddings(all_extracted_data)',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is part of a larger project, likely a Jupyter Notebook, called \"code-generation-with-langchain.\" It relates to Step 3 of the project, which involves generating embedding vectors and structuring data. The file name suggests that the project may be related to generating code using a \"langchain\" (a chain of languages).\\n\\nThe code snippet uses the HuggingFace embeddings model \"all-MiniLM-L6-v2\" to generate embedding vectors that represent the context of code snippets. It also defines a function called \"update_embeddings\" which iterates through previously extracted data and uses the embeddings model to generate embedding vectors for each code snippet\\'s context. These vectors are then inserted into the data structure for each item. After updating the data structure, the code converts the list of code snippets and their metadata into a Pandas DataFrame for easier manipulation and visualization. Essentially, the code is using a specific embeddings model to generate embedding vectors for code snippets, and then organizing and storing this data in a DataFrame for further use.'},\n",
       " {'id': '24c5715d-cc69-4210-bfd0-66997f4e9d72',\n",
       "  'embedding': None,\n",
       "  'code': 'import pandas as pd\\ndef to_dataframe_row(embedded_snippets: list):\\n    \"\"\"\\n    Helper function to convert a list of embedded snippets into a dataframe row\\n    in dictionary format.\\n\\n    Args:\\n        embedded_snippets: List of dictionaries containing Snippets to be converted\\n\\n    Returns:\\n        List of Dictionaries suitable for conversion to a DataFrame\\n    \"\"\"\\n    outputs = []\\n    for snippet in embedded_snippets:\\n        output = {\\n            \"ids\": snippet[\\'id\\'],\\n            \"embeddings\": snippet[\\'embedding\\'],\\n            \"code\": snippet[\\'code\\'],\\n            \"metadatas\": {\\n                \"filenames\": snippet[\\'filename\\'],\\n                \"context\": snippet[\\'context\\'],\\n            },\\n        }\\n        outputs.append(output)\\n    return outputs\\n\\n\\n',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is a helper function that takes in a list of embedded snippets and converts them into a list of dictionaries suitable for conversion to a DataFrame. It does this by iterating through the list of snippets and extracting relevant information such as IDs, embeddings, code, and metadata. The code also uses an embeddings model to generate embedding vectors for the context extracted from each code snippet. The file name associated with this code is \"code-generation-with-langchain.ipynb\". The code snippet is used in the context of generating embeddings and structuring data for code snippets.'},\n",
       " {'id': 'ec6ddbd7-c0d2-43ab-9989-6c32734bbefb',\n",
       "  'embedding': None,\n",
       "  'code': 'rows = to_dataframe_row(updated_structure)\\ndf = pd.DataFrame(rows)',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is generating embedding vectors for code snippets and their respective context using the HuggingFace embeddings model. It iterates through the previously extracted data structure, generates an embedding vector for the context field using the model, and updates the data structure with the new embedding vector. Then, it converts the updated data structure into a format suitable for a Pandas DataFrame, which is used to store the results for further manipulation and visualization. The file name suggests that this code is part of a larger process involving code generation using the Langchain platform.'},\n",
       " {'id': '0c7dbfb7-1447-4bfc-a89b-29cbbd54b5e6',\n",
       "  'embedding': None,\n",
       "  'code': 'df',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is used to generate embedding vectors for code snippets using a HuggingFace model. The function \"update_embeddings\" iterates through the data structure and uses the embedding model to create vectors for the context of each code snippet. The data is then converted into a Pandas DataFrame, which makes it easier to manipulate and use in the future. The end result is a DataFrame containing the code snippets, their associated metadata, and the generated embedding vectors.'},\n",
       " {'id': '7477deee-3407-4d2d-b49f-960fd6cca115',\n",
       "  'embedding': None,\n",
       "  'code': 'import chromadb\\n\\nchroma_client = chromadb.Client()\\n\\ncollection = chroma_client.get_or_create_collection(name=\"my_collection\")\\n\\nids = df[\"ids\"].tolist()\\ndocuments = df[\"code\"].tolist()\\nmetadatas = df[\"metadatas\"].tolist()\\nembeddings_list = df[\"embeddings\"].tolist()\\n\\ncollection.upsert(\\n    documents=documents,\\n    ids=ids,\\n    metadatas=metadatas,\\n    embeddings=embeddings_list  \\n)\\n\\nprint(\"Documents added successfully!!\")\\n',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is connecting to a vector database system called ChromaDB and creating a collection to store code snippets and their corresponding metadata. It then extracts data from a DataFrame, including unique identifiers, code snippets, metadata, and embedding vectors, and inserts them into the collection using an upsert method. The code also includes a function to query the collection and retrieve related documents. The file name suggests that this code is used for code generation with the use of LangChain, and the context further explains the specific steps and functions being implemented.'},\n",
       " {'id': '0f793bcd-0275-4dc9-809b-2297bdcad261',\n",
       "  'embedding': None,\n",
       "  'code': 'document_count = collection.count()\\nprint(f\"Total documents in the collection: {document_count}\")',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is used to interact with a vector database system called ChromaDB in order to store and retrieve code snippets and their respective metadata. The code first creates a collection within the database, and then inserts the code snippets, unique identifiers, metadata, and embedding vectors into the collection. It then performs a query to search for documents related to a specific query text and returns the top 5 results. Finally, a function called \"retriever\" is implemented to query the collection using a provided string and return a list of Document objects containing the code snippets and their metadata. This allows for easy retrieval and analysis of the documents. The file name suggests that this code snippet is part of a larger code generation project using LangChain.'},\n",
       " {'id': 'b667e13a-620c-45a8-8aaa-8fd59470dd69',\n",
       "  'embedding': None,\n",
       "  'code': 'results = collection.query(\\n    query_texts=[\"!pip install\"],\\n    n_results=5,  \\n)',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is part of a larger code generation process using LangChain and is stored in a file named \"code-generation-with-langchain.ipynb\". It is used to retrieve code snippets and metadata from a database using ChromaDB. The code first initializes a ChromaDB client and creates (or retrieves) a collection named \"my_collection\" to store the documents and their corresponding embeddings. Then, it extracts data from a DataFrame, including the code snippets, their unique identifiers, and metadata such as the file name and context. These documents and their information are then inserted into the collection using the upsert method. Finally, a query is performed on the collection to search for code snippets related to the query text \"!pip install\" and return the top 5 most relevant results. The code also implements a retriever function that takes a query string, the collection, and the number of results to return as parameters. This function executes a query in the collection, creates Document objects for each result, and returns a list of these objects for easy retrieval and analysis in the future. Overall, the code snippet is used to store and query code snippets and their metadata in a database using ChromaDB.'},\n",
       " {'id': '5addcbe2-520d-425f-84f6-50044e202562',\n",
       "  'embedding': None,\n",
       "  'code': 'results',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is used to store and query code snippets and their metadata in a vector database system called ChromaDB. It first creates a connection to the database and then creates a collection to store the documents and their embeddings. The code then extracts data from a DataFrame, including unique identifiers, code snippets, and metadata, and inserts them into the collection. Finally, a query is performed to retrieve the most relevant documents based on a given query string. The code also includes a function called \"retriever\" which can be used to query the collection and return a list of Document objects containing the code snippet and its metadata.'},\n",
       " {'id': 'f2e58e76-ecf8-4cc9-b2ce-01987559828f',\n",
       "  'embedding': None,\n",
       "  'code': \"from langchain.schema import Document\\nfrom typing import List\\n\\n\\ndef retriever(query: str, collection, top_n: int = 10) -> List[Document]:\\n    results = collection.query(\\n        query_texts=[query],\\n        n_results=top_n\\n    )\\n    \\n    documents = [\\n        Document(\\n            page_content=str(results['documents'][i]),\\n            metadata=results['metadatas'][i] if isinstance(results['metadatas'][i], dict) else results['metadatas'][i][0]  # Corrigir o metadado se for uma lista\\n        )\\n        for i in range(len(results['documents']))\\n    ]\\n    \\n    return documents\\n\",\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet creates a function called \"retriever\" that takes in a query string, a collection, and an optional number of results to return. It then uses the ChromaDB database system to search for documents related to the query text and returns a list of Document objects containing the code snippets and their metadata. This function is used to query the collection for relevant documents, making it easier to retrieve and analyze them in the future. The code also includes steps for creating the collection, inserting documents and their associated metadata into it, and querying the collection for relevant results. This process is used to store and query code snippets and their metadata using ChromaDB.'},\n",
       " {'id': '2a6654b0-b9ef-44a0-aaf3-9a93918e15f0',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_core.runnables import RunnablePassthrough\\nfrom langchain.schema.output_parser import StrOutputParser\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom langchain_openai import ChatOpenAI\\n\\n\\ndef format_docs(docs: List[Document]) -> str:\\n    return \"\\\\n\\\\n\".join([doc.page_content for doc in docs])\\n\\ntemplate = \"\"\"You are a Python wizard tasked with generating code for a Jupyter Notebook (.ipynb) based on the given context.\\nYour answer should consist of just the Python code, without any additional text or explanation.\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\"\"\"\\n\\nprompt = ChatPromptTemplate.from_template(template)\\n\\nmodel = ChatOpenAI() \\n\\nchain = {\\n    \"context\": lambda inputs: format_docs(retriever(inputs[\\'query\\'], collection)), \\n    \"question\": RunnablePassthrough()\\n} | prompt | model | StrOutputParser()\\n\\n\\n',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet imports necessary libraries and modules for code generation using LangChain, including a language model from OpenAI. It defines a function for formatting documents and creates a template for generating code in a Jupyter Notebook. It then sets up a processing chain that takes in a context and question, and uses the language model to generate code based on the provided prompt. The code also includes functions for cleaning and printing the generated code, interacting with Galileo for quality evaluation, and dynamically creating new code cells in the Jupyter Notebook. After executing the chain, the code processes the results and inserts them into the notebook.'},\n",
       " {'id': '03c70404-0985-4b10-83e0-5794f4cbc567',\n",
       "  'embedding': None,\n",
       "  'code': '# def format_docs(docs: List[Document]) -> str:\\n    # return \"\\\\n\\\\n\".join([doc.page_content for doc in docs])\\n\\n# template = \"\"\"You are a Python wizard tasked with generating code for a Jupyter Notebook (.ipynb) based on the given context.\\n# Your answer should consist of just the Python code, without any additional text or explanation.\\n\\n# Context:\\n# {context}\\n\\n# Question: {question}\\n# \"\"\"\\n\\n# prompt = ChatPromptTemplate.from_template(template)\\n# model = llm_local() \\n\\n# chain = {\\n    # \"context\": lambda inputs: format_docs(retriever(inputs[\\'query\\'], collection)), \\n    # \"question\": RunnablePassthrough()\\n# } | prompt | model | StrOutputParser()',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet defines a function called \"format_docs\" that takes in a list of documents and returns a string consisting of the page content of each document separated by two line breaks. It also defines a template for generating code for a Jupyter Notebook and creates a prompt using that template. It then creates a local model and defines a chain which uses the \"format_docs\" function to format the context and passes through the question. Finally, it runs the model and parses the output as a string. The file name indicates that this code is used for generating code using a language chain in a Jupyter Notebook. The context specifies that a local model using LlamaCPP will be used, and the question is a runnable passthrough.'},\n",
       " {'id': 'c67dffb8-6cee-4e8c-bc5c-cc92639b4883',\n",
       "  'embedding': None,\n",
       "  'code': 'def clean_and_print_code(result: str):\\n    clean_code = result.replace(\"```python\", \"\").replace(\"```\", \"\").strip()\\n    \\n    print(clean_code)',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is a function called \"clean_and_print_code\" that takes in a string parameter named \"result\". The function removes any instances of \"```python\" and \"```\" from the string, as well as any leading or trailing whitespace. Then, the function prints the resulting cleaned code. This code is likely used for cleaning and formatting code before printing it out. The file name is \"code-generation-with-langchain.ipynb\" and the code is being used in the context of running a local model using LlamaCPP.'},\n",
       " {'id': 'dcb236fa-5845-4085-a122-f37e5f12cb2b',\n",
       "  'embedding': None,\n",
       "  'code': 'import promptquality as pq\\n\\nos.environ[\\'GALILEO_API_KEY\\'] = \"htMRukWlQyvOEDMnAUYQUTQnEZL6_3ubALGkhn6ph70\" #your api Key\\ngalileo_url = \"https://console.hp.galileocloud.io/\"\\npq.login(galileo_url)\\n',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': \"The code imports the promptquality library and sets the Galileo API key in the operating system's environment. It then sets the Galileo URL and uses the promptquality library to login. The file name suggests that the code is used for code generation with Langchain, and the context indicates that it is used for running a local model using LlamaCPP.\"},\n",
       " {'id': '4579e9d4-3748-4029-bd56-c7d622b8a0a1',\n",
       "  'embedding': None,\n",
       "  'code': 'from IPython.display import display, Markdown\\nfrom IPython import get_ipython\\nfrom IPython.display import display, Code\\n\\n\\nprompt_handler = pq.GalileoPromptCallback(\\n    scorers=[\\n        pq.Scorers.context_adherence_plus,  # groundedness\\n        pq.Scorers.correctness,             # factuality\\n        pq.Scorers.prompt_perplexity        # perplexity \\n    ]\\n)\\n\\n# Example of inputs to run the chain\\ninputs = [\\n    {\"query\": \"instantiate the LLM model and the Embedding model\", \"question\": \"create code llm model and the embedding model\"},\\n\\n]\\n#How to create a vector bank?\\n#create code a chromadb vector database\\n\\nresults = chain.batch(inputs, config=dict(callbacks=[prompt_handler]))\\n\\n# Publish run results\\nprompt_handler.finish()\\n',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code above imports necessary modules for displaying markdown and retrieving the IPython instance. It then sets up a GalileoPromptCallback object with a list of scorers to evaluate the performance of the generated code. An example input with a query and question is provided and then passed to a chain object for batch processing. The results are then published and the prompt handler finishes the process.'},\n",
       " {'id': '51e480ed-207a-4184-98b2-e73de1acc00e',\n",
       "  'embedding': None,\n",
       "  'code': 'import json\\nfrom IPython.core.getipython import get_ipython\\n\\ndef create_new_code_cell_from_output(output):\\n    \"\"\"\\n    Creates a new code cell in Jupyter Notebook from an output,\\n    dealing with different output formats.\\n\\n    Args:\\n        output: The output to be inserted into the new cell. It can be a string, a dictionary\\n                or another type of object.\\n    \"\"\"\\n\\n    shell = get_ipython()\\n\\n    if isinstance(output, dict):\\n        code = output[\\'cells\\'][0][\\'source\\']\\n        code = \\'\\'.join(code)\\n    else:\\n        code = str(output)\\n\\n    clean_code = code.strip()\\n\\n    shell.set_next_input(clean_code, replace=False)\\n\\nfor result in results:\\n    try:\\n        output = json.loads(result)\\n        create_new_code_cell_from_output(output)\\n    except json.JSONDecodeError:\\n        # If it\\'s not JSON, just treat it as a string of code\\n        create_new_code_cell_from_output(result)\\n',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is creating a new code cell in Jupyter Notebook from a given output, which can be a string, dictionary, or other type of object. It first imports the JSON library and gets the IPython shell. Then, it checks if the output is a dictionary and extracts the code from it. If not, it converts the output to a string. After cleaning up the code, it sets it as the next input in the notebook. Finally, it loops through a list of results and tries to convert them to JSON, using the create_new_code_cell_from_output function. If the result cannot be converted, it is treated as a string of code and added to a new code cell. The file name suggests that this code may be used for code generation with LangChain, a language model that can generate code based on a given query and question.'},\n",
       " {'id': 'dc3c35f8-b752-4f6a-992f-67fab742d3aa',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.llms import Ollama\\nfrom langchain_community.embeddings import OllamaEmbeddings\\n\\nmodel = Ollama(model=\"llama3\")\\nembeddings = OllamaEmbeddings(model=\"llama3\")',\n",
       "  'filename': 'code-generation-with-langchain.ipynb',\n",
       "  'context': 'The code snippet imports two modules, \"langchain_community.llms\" and \"langchain_community.embeddings\", and creates an object of the class Ollama from the \"llms\" module. It also creates an object of the class OllamaEmbeddings from the \"embeddings\" module. These objects are assigned to the variables \"model\" and \"embeddings\" respectively. The file name suggests that this code is related to code generation using the language model LLM in the context of the Langchain community.'},\n",
       " {'id': '6bf719ad-29f4-498d-ba7b-fe511fdf262d',\n",
       "  'embedding': None,\n",
       "  'code': 'pip install PyPDF',\n",
       "  'filename': 'chatbot-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is using the pip install command to install the PyPDF library. This library allows for the manipulation and extraction of data from PDF documents. The file name is \"chatbot-with-langchain.ipynb\", which suggests that this code is likely part of a chatbot program that uses the Langchain platform. The context mentions configuring the environment and installing necessary libraries, specifically mentioning the need for a connector to work with PDF documents. Therefore, the code is likely being used to set up the environment for the chatbot program to be able to connect with the Langchain platform and manipulate PDF documents.'},\n",
       " {'id': '1255a126-cd5f-4e12-808b-bb860965c1d0',\n",
       "  'embedding': None,\n",
       "  'code': 'import os\\nos.environ[\"HF_HOME\"] = \"/home/jovyan/local/hugging_face\"\\nos.environ[\"HF_HUB_CACHE\"] = \"/home/jovyan/local/hugging_face/hub\"',\n",
       "  'filename': 'chatbot-with-langchain.ipynb',\n",
       "  'context': 'The code sets the environmental variables \"HF_HOME\" and \"HF_HUB_CACHE\" to specific directories, \"/home/jovyan/local/hugging_face\" and \"/home/jovyan/local/hugging_face/hub\" respectively. This allows for the local caching of models downloaded from Hugging Face, even after the workspace is closed. This configuration is meant for use with a chatbot project, as indicated by the file name \"chatbot-with-langchain.ipynb\".'},\n",
       " {'id': '543d49b4-029c-4980-9fb4-c4fb7307e974',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.document_loaders import WebBaseLoader\\nfrom langchain_community.document_loaders import PyPDFLoader',\n",
       "  'filename': 'chatbot-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is importing two document loaders from different modules, WebBaseLoader from \"langchain.document_loaders\" and PyPDFLoader from \"langchain_community.document_loaders\". The file name is \"chatbot-with-langchain.ipynb\". The context explains that the code is part of a step in a process, specifically for data loading. The code is using the Langchain framework to extract content from a local PDF file containing product documentation. It also mentions that there are examples provided for using Web Loaders to load data from web pages.'},\n",
       " {'id': '9bc1f07d-3670-4891-bdf4-b2f541c37409',\n",
       "  'embedding': None,\n",
       "  'code': 'file_path = (\\n    \"data/AIStudioDoc.pdf\"\\n)\\npdf_loader = PyPDFLoader(file_path)\\npdf_data = pdf_loader.load()\\n\\n#loader1 = WebBaseLoader(\"https://www.hp.com/us-en/workstations/ai-studio.html\") # If you want to change the knowledge base, just modify this link.\\n#data1 = loader1.load()\\n\\n#loader2 = WebBaseLoader(\"https://zdocs.datascience.hp.com/docs/aistudio\")\\n#data2 = loader2.load()',\n",
       "  'filename': 'chatbot-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is setting a file_path variable to the path of a PDF file called \"AIStudioDoc.pdf\" in the \"data\" folder. It then creates a PyPDFLoader object using the file path and calls the load() function to extract the data from the PDF file. The code also includes two examples of using WebBaseLoader objects to load data from web pages, but these lines have been commented out. The file name is \"chatbot-with-langchain.ipynb\" and this code is part of a larger process of data loading using the Langchain framework.'},\n",
       " {'id': '5ac93c49-75ce-43e7-a591-134a27b62751',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.text_splitter import RecursiveCharacterTextSplitter\\n',\n",
       "  'filename': 'chatbot-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is importing the RecursiveCharacterTextSplitter function from the langchain package. The file name indicates that it is related to a chatbot using the langchain library. The context suggests that the code is used to split loaded documents into smaller chunks for adding to a vector database. This process is important for creating specific and manageable text inputs for the chatbot to use.'},\n",
       " {'id': 'cf4c4cb2-1499-47d5-b2d9-b13e0bff5b66',\n",
       "  'embedding': None,\n",
       "  'code': 'text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\\nsplits = text_splitter.split_documents(pdf_data)',\n",
       "  'filename': 'chatbot-with-langchain.ipynb',\n",
       "  'context': 'The code snippet creates a RecursiveCharacterTextSplitter object, which is used to split a given document into smaller chunks of text. The chunk size and overlap are specified as parameters. Then, the code calls the split_documents function to split a document (pdf_data) into smaller chunks using the previously defined object. The file name, \"chatbot-with-langchain.ipynb\", suggests that this code may be used in the creation of a chatbot with language chain functionality.'},\n",
       " {'id': '623d383d-8f2d-48c3-bf18-f17166b922a4',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain_community.embeddings import HuggingFaceEmbeddings\\nfrom langchain.vectorstores import Chroma\\n',\n",
       "  'filename': 'chatbot-with-langchain.ipynb',\n",
       "  'context': 'The code snippet imports two libraries, \"langchain_community.embeddings\" and \"langchain.vectorstores\". It then uses these libraries to transform text into embeddings and store them in a vector database. This allows for similarity search and retrieval of documents. The file name suggests that this code is part of a chatbot that uses the \"langchain\" technology for language processing and retrieval.'},\n",
       " {'id': '0de72f1a-c702-4668-b395-721e3e97c8f4',\n",
       "  'embedding': None,\n",
       "  'code': 'embedding = HuggingFaceEmbeddings()',\n",
       "  'filename': 'chatbot-with-langchain.ipynb',\n",
       "  'context': 'The code snippet creates an instance of the HuggingFaceEmbeddings class, which is used to convert text into numerical embeddings. The associated file name suggests that this code is part of a chatbot program, and the embeddings are likely being used to process and analyze natural language input. The optional context suggests that the embeddings are being stored in a vector database for future use in similarity searches and document retrieval.'},\n",
       " {'id': 'cb3cf709-3cf3-4e88-b9a4-b03777714968',\n",
       "  'embedding': None,\n",
       "  'code': 'vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\\nretriever = vectordb.as_retriever()\\n',\n",
       "  'filename': 'chatbot-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is creating a vector database and a retriever for a chatbot. The vector database is using documents that have been split into smaller pieces and an embedding to store text as numerical vectors. This allows for similarity search and proper retrieval of documents based on their embeddings. The file name suggests this code is part of a chatbot project that utilizes a language model called \"langchain.\"'},\n",
       " {'id': 'a4e7a8f1-1340-435f-9314-c56143ffe887',\n",
       "  'embedding': None,\n",
       "  'code': '#import os\\n#from langchain_openai import OpenAI\\n#with open(\\'secrets.yaml\\') as file:\\n#    secrets = yaml.safe_load(file)\\n\\n#os.environ[\"OPENAI_API_KEY\"] = secrets[\"OpenAI\"]\\n#llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")\\n',\n",
       "  'filename': 'chatbot-with-langchain.ipynb',\n",
       "  'context': 'The code snippet imports the \"os\" and \"langchain_openai\" libraries and sets up a connection to the OpenAI API using a specified API key from a \"secrets.yaml\" file. It then initializes a language model (LLM) using the \"gpt-3.5-turbo-instruct\" model name. The file name is \"chatbot-with-langchain.ipynb\" and the code is related to setting up and using the LLM model. The context suggests that the code is a part of a larger project that involves choosing between different options for the LLM model, such as local or cloud-based options, and ensuring the proper assets and API keys are in place.'},\n",
       " {'id': '044a9d48-0499-4e3b-8835-a64032d1b79f',\n",
       "  'embedding': None,\n",
       "  'code': '### Alternate code to use cloud models from Hugging Face\\n\\n#import yaml\\n#with open(\\'secrets.yaml\\') as file:\\n    #secrets = yaml.safe_load(file)\\n#huggingfacehub_api_token = secrets[\"HuggingFace\"]\\n#repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\\n#llm = HuggingFaceEndpoint(\\n   #huggingfacehub_api_token=huggingfacehub_api_token,\\n   #repo_id=repo_id,\\n#)\\n',\n",
       "  'filename': 'chatbot-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is setting up a connection to use cloud models from Hugging Face. It is importing the necessary libraries and loading a secrets file to retrieve the HuggingFace API token. It also specifies the repository ID for the specific model to be used. Finally, the code creates an instance of the HuggingFaceEndpoint class, passing in the API token and repository ID. The file name, \"chatbot-with-langchain.ipynb\", suggests that this code is being used in the context of a chatbot project, specifically for selecting a language model to be used.'},\n",
       " {'id': '2e025feb-be1b-420f-ba2a-da4bb43dbacc',\n",
       "  'embedding': None,\n",
       "  'code': '###Alternate code to load local models\\n###This specific example requires the project to have an asset call Llama7b, associated with the cloud S3 URI s3://dsp-demo-bucket/LLMs (public bucket)\\n\\nfrom langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\\nfrom langchain_community.llms import LlamaCpp\\n\\ncallback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\\n\\nllm = LlamaCpp(\\n            model_path=\"/home/jovyan/datafabric/Llama7b/ggml-model-f16-Q5_K_M.gguf\",\\n            n_gpu_layers=30,\\n            n_batch=512,\\n            n_ctx=4096,\\n            max_tokens=1024,\\n            f16_kv=True,  \\n            callback_manager=callback_manager,\\n            verbose=False,\\n            stop=[],\\n            streaming=False,\\n            temperature=0.2,\\n        )    ',\n",
       "  'filename': 'chatbot-with-langchain.ipynb',\n",
       "  'context': 'The code above is an alternate method for loading local models for a chatbot. It specifically uses the LlamaCpp model from the langchain_community.llms library and sets the parameters for the model such as the model path, number of GPU layers, batch size, context size, maximum tokens, and temperature. It also includes a callback manager and sets certain options for the model, such as being verbose and stopping at certain points. This code snippet is used in conjunction with the chatbot-with-langchain.ipynb file, which is part of a larger project that allows for different options for the LLM model, including local and cloud options.'},\n",
       " {'id': 'fbf5e404-5bb7-4fc1-b094-99feb1ea331c',\n",
       "  'embedding': None,\n",
       "  'code': 'from langchain.prompts import ChatPromptTemplate\\nfrom langchain.schema import StrOutputParser\\nfrom langchain.schema.runnable import RunnablePassthrough\\nfrom typing import List\\nfrom langchain.schema.document import Document\\n\\ndef format_docs(docs: List[Document]) -> str:\\n    return \"\\\\n\\\\n\".join([d.page_content for d in docs])\\n\\ntemplate = \"\"\"You are an virtual Assistant for a Data Science platform called AI Studio. Answer the question based on the following context:\\n\\n    {context}\\n\\n    Question: {query}\\n    \"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\nchain = {\"context\": retriever | format_docs, \"query\": RunnablePassthrough()} | prompt | llm | StrOutputParser()',\n",
       "  'filename': 'chatbot-with-langchain.ipynb',\n",
       "  'context': 'The code snippet is creating a chatbot using the LangChain library. It imports necessary modules and defines a function to format documents. It also sets up a template for the chat prompt and creates a chain that takes in a question and context, formats the context documents, uses a Hugging Face chat model to answer the question, and then formats the output as a string. The file name indicates that this code may be used in a Jupyter notebook. The optional context provides information on how the chain will be used in the overall process.'},\n",
       " {'id': '458df7ef-0c50-491d-89e1-ba8c1ffa9d2e',\n",
       "  'embedding': None,\n",
       "  'code': 'import promptquality as pq\\n\\nos.environ[\\'GALILEO_API_KEY\\'] = \"9zjBwRIhyWo4zzkdsJhvg2y-NTT92qjEQmt2DIFmCFg\" #your api Key\\ngalileo_url = \"https://console.hp.galileocloud.io/\"\\npq.login(galileo_url)',\n",
       "  'filename': 'chatbot-with-langchain.ipynb',\n",
       "  'context': 'The code imports the library \"promptquality\" as \"pq\" and sets an environment variable for the Galileo API key. It then sets the Galileo URL and uses the Prompt Quality library to log in. The file name is \"chatbot-with-langchain.ipynb\" and the context suggests it is part of a larger project involving a chatbot and language translation.'},\n",
       " {'id': 'c2cf0769-da8c-4d1f-bed8-b2a6b8d79f88',\n",
       "  'embedding': None,\n",
       "  'code': '# Create callback handler\\nprompt_handler = pq.GalileoPromptCallback(\\n    project_name=\"AIStudio_Chatbot_template\",\\n    scorers=[pq.Scorers.context_adherence_luna, pq.Scorers.correctness, pq.Scorers.toxicity, pq.Scorers.sexist]\\n)\\n\\n# Run your chain experiments across multiple inputs with the galileo callback\\ninputs = [\\n    \"What is AI Studio\",\\n    \"How to create projects in AI Studio?\"\\n    \"How to monitor experiments?\",\\n    \"What are the different workspaces available?\",\\n    \"What, exactly, is a workspace?\",\\n    \"How to share my experiments with my team?\",\\n    \"Can I access my Git repository?\",\\n    \"Do I have access to files on my local computer?\",\\n    \"How do I access files on the cloud?\",\\n    \"Can I invite more people to my team?\"\\n]\\nchain.batch(inputs, config=dict(callbacks=[prompt_handler]))\\n\\n# publish the results of your run\\nprompt_handler.finish()',\n",
       "  'filename': 'chatbot-with-langchain.ipynb',\n",
       "  'context': 'The code creates a callback handler for a chatbot project in AI Studio. It then runs chain experiments with inputs and uses the callback to log the results in Galileo. Finally, it publishes the results and finishes the operation. The file name suggests that the code is used for a language chain chatbot project.'},\n",
       " {'id': '9e161238-6055-4c5c-a881-9dda1b3cddd6',\n",
       "  'embedding': None,\n",
       "  'code': \"# These are all the libraries and frameworks we're going to use\\nfrom datasets import load_dataset, load_metric\\nfrom datetime import datetime\\nimport mlflow\\nimport mlflow.pytorch\\nimport torch\\nfrom tqdm.autonotebook import tqdm\\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, pipeline\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is importing necessary libraries and frameworks for a training process, including datasets, datetime, mlflow, torch, tqdm, and transformers. It also sets up a file named \"Training.ipynb\" for the training process.'},\n",
       " {'id': 'b66ae89e-b2e2-43c2-8a53-7550949e0982',\n",
       "  'embedding': None,\n",
       "  'code': 'start_time_all_execution = datetime.now() # This variable is to help us to see in how much time this notebook will run',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet creates a variable called \"start_time_all_execution\" that stores the current time. This variable is used to track the amount of time it takes for the notebook named \"Training.ipynb\" to run.'},\n",
       " {'id': '93d078a8-5bfe-4d14-87d7-5de939ea6aa5',\n",
       "  'embedding': None,\n",
       "  'code': 'squad_dataset = load_dataset(\"squad\") # Downloading the dataset\\nsquad_dataset',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet downloads the SQuAD dataset from the HuggingFace datasets repository and assigns it to a variable called \"squad_dataset\". The file name is \"Training.ipynb\" and the context provides information about the SQuAD dataset and where it can be found.'},\n",
       " {'id': '4c7275d8-970e-4eea-afe9-dea2cff05f6a',\n",
       "  'embedding': None,\n",
       "  'code': \"squad_train_dataset = squad_dataset['train']\\nsquad_train_dataset\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is creating a variable called \"squad_train_dataset\" which stores the value of the \"train\" key from the \"squad_dataset\" dictionary. The file name is \"Training.ipynb\" and the context suggests that this code is being used to access and manipulate data for a training dataset.'},\n",
       " {'id': '43235eb4-e8f2-44a6-9724-8ac32b0e6938',\n",
       "  'embedding': None,\n",
       "  'code': 'index_input = 1\\n\\nsquad_train_dataset[index_input]',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code above is setting a variable called \"index_input\" to the value of 1. It then uses this variable to access a specific item in a dataset called \"squad_train_dataset\". The file name associated with this code is \"Training.ipynb\" and the context suggests that this code is part of a larger dataset with 87599 rows, each of which corresponds to a question-answer input for a model. Essentially, the code is retrieving a specific input from the dataset for use in a model.'},\n",
       " {'id': 'f5165ee0-1ea6-497e-8b08-8e9be93bfa22',\n",
       "  'embedding': None,\n",
       "  'code': \"print(squad_train_dataset[index_input]['title'])\\nprint(squad_train_dataset[index_input]['context'])\\nprint(squad_train_dataset[index_input]['question'])\\nprint(squad_train_dataset[index_input]['answers'])\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is printing out the title, context, question, and answers from the squad_train_dataset, using the index_input as the specified index. This code is likely used to access and display specific information from the dataset. The file name is \"Training.ipynb\" and the context provides more information about the input structure of the dataset.'},\n",
       " {'id': '6e58411c-6930-4d74-bdc0-5d11f7c54693',\n",
       "  'embedding': None,\n",
       "  'code': \"# Checking in the train dataset if we have just one answer for each question\\nsquad_train_dataset.filter(lambda x: len(x['answers']['text']) != 1)\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': \"The code snippet is checking the train dataset to see if each question has only one answer. It does this by using the filter method and checking the length of the 'answers' key's 'text' value. The file name suggests that this code is part of a training process. The context explains that the 'answers' key is a dictionary with two keys, 'text' and 'answer_start', and that there can be more than one answer for each question in the dataset. The code is filtering out any inputs that have more than one answer.\"},\n",
       " {'id': 'a354bc16-2965-473b-9d32-9e96a9c730e2',\n",
       "  'embedding': None,\n",
       "  'code': 'model_checkpoint_bbc = \"distilbert-base-cased\" #\"bert-base-cased\" is a larger option if you want to test!\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint_bbc) # getting the model\\'s tokenizer ',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': \"The code snippet loads a pre-trained BERT model from the HuggingFace Hub, specifically the distilbert-base-cased model. The model's tokenizer is then obtained using the AutoTokenizer function. The file name Training.ipynb suggests that this code is used for training the model.\"},\n",
       " {'id': 'c4601e65-7b00-4b22-bb9c-577681fc63cb',\n",
       "  'embedding': None,\n",
       "  'code': \"# getting just context and the question\\n#print(index_input) # uncoment to remember the index\\n\\ncontext = squad_train_dataset[index_input]['context'] \\nquestion = squad_train_dataset[index_input]['question']\\nprint(context)\\nprint(question)\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is extracting data from a training dataset, specifically the context and question for a particular index. The commented line is a reminder to the user of the index used. The file name indicates that this code is part of a training process. The context is a sample from the train dataset exploration. Overall, the code is printing the context and question for a specific index in the training dataset.'},\n",
       " {'id': '211d370a-9189-4837-8552-998d679e9a90',\n",
       "  'embedding': None,\n",
       "  'code': 'inputs = tokenizer(question, context) # Decoding the inputs\\ninputs',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is using a tokenizer to decode the inputs of a question and context and store them in a variable called \"inputs\". The file name is \"Training.ipynb\" and the context is explaining the order in which the inputs are passed to the tokenizer.'},\n",
       " {'id': '20febf75-340c-42f6-88aa-12a95730ca38',\n",
       "  'embedding': None,\n",
       "  'code': 'inputs.keys()',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet uses the method \"keys()\" to extract the keys from a dictionary. The file name is \"Training.ipynb\" and the context suggests that the code is being used to check the keys of a dictionary.'},\n",
       " {'id': '83342ce3-cadd-46cc-b212-c96c8c1e7ec9',\n",
       "  'embedding': None,\n",
       "  'code': \"print(inputs['input_ids'][0:27])\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is using the print function to display the first 26 items of the input_ids list from the inputs dictionary. This code is located in a file named Training.ipynb and is likely part of a larger code that is being used for training purposes.'},\n",
       " {'id': '4c7e4358-feb5-4e0d-ad8d-fd124e40cadc',\n",
       "  'embedding': None,\n",
       "  'code': \"tokenizer.decode(inputs['input_ids'][0:27])\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet uses the specified tokenizer to decode the input_ids of the first 27 elements in the inputs dictionary. This is done within the file named \"Training.ipynb\", and may be used to provide context or understanding for the numbers in question.'},\n",
       " {'id': '7013264f-b16c-47db-9fdf-5dbdf05dfa08',\n",
       "  'embedding': None,\n",
       "  'code': 'sentences = 0 \\nfor i in context:\\n    if str(i) == \\'.\\':\\n        sentences += 1\\n\\nprint(context)\\nprint(f\"Total number of words: {len(context)}\")\\nprint(f\"Total number of sentences: {sentences}\")',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is counting the number of sentences in a given context by iterating through each character and checking if it is a period. If it is, the sentence count is increased by one. The code also prints the context, total number of words, and total number of sentences. It is likely used for training purposes and the file name is \"Training.ipynb\".'},\n",
       " {'id': '0b73d6bc-6b9f-4d9d-8d73-eb215a39ea23',\n",
       "  'embedding': None,\n",
       "  'code': 'inputs = tokenizer(\\n    question,\\n    context, \\n    max_length=100,\\n    truncation=\"only_second\",\\n    stride=50,\\n    return_overflowing_tokens=True\\n)',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is using a tokenizer function to preprocess inputs for a natural language processing task. The function takes in a question and context, and performs tokenization by splitting the text into smaller chunks based on a predefined maximum length. The truncation parameter specifies that only the second input (context) will be truncated, and the stride parameter determines how much overlap there is between the context windows. The final parameter, return_overflowing_tokens, ensures that any overlapping tokens are also returned. This code is likely part of a training process, as indicated by the file name, which is a Jupyter Notebook (ipynb) used for data analysis and training models.'},\n",
       " {'id': '0c31b3f7-d37a-4641-8075-4502d3ca4ea9',\n",
       "  'embedding': None,\n",
       "  'code': 'print(inputs)\\nprint(inputs.keys())',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet uses the print function to display the contents of the inputs variable and its keys. The file name associated with the code is Training.ipynb, and it is used to indicate that the code is part of a training or learning process. The purpose of the code may be to examine and analyze the input data and its associated keys for further development or understanding.'},\n",
       " {'id': 'ba407334-f4c6-4cd6-9286-8d579374a88f',\n",
       "  'embedding': None,\n",
       "  'code': 'print(f\\'Total number of lists: {len(inputs[\"input_ids\"])}\\')',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': \"This code snippet prints the total number of lists in the 'input_ids' variable, using string formatting to display the result. It is likely part of a larger training process, as indicated by the file name 'Training.ipynb'. The context suggests that the 'input_ids' variable has been modified to contain a list of lists.\"},\n",
       " {'id': 'ce8f5ec3-3429-46ed-82d7-abc0c0262ff7',\n",
       "  'embedding': None,\n",
       "  'code': 'for id in inputs[\"input_ids\"]:\\n    print(tokenizer.decode(id))',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is using a for loop to iterate through a list of input IDs stored in the variable \"inputs.\" For each ID, it is using a function called \"decode\" from the \"tokenizer\" library to convert the ID into a human-readable form and then printing the result. The file name is \"Training.ipynb,\" which suggests that this code is part of a training process for a machine learning model. The optional context mentions decoding lists, indicating that the input IDs are likely encoded data that needs to be decoded for further analysis. Overall, the code is decoding a list of input IDs and printing the results, likely for training a machine learning model.'},\n",
       " {'id': 'cc24db38-6b59-4f4f-9689-d20dfa58dee8',\n",
       "  'embedding': None,\n",
       "  'code': 'question_samples = squad_train_dataset[:3][\"question\"] # Getting the first 3 questions\\ncontext_samples = squad_train_dataset[:3][\"context\"] #  and contexts\\n\\nfor i in question_samples:\\n    print(i)',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet retrieves the first three questions and contexts from a dataset called squad_train_dataset and prints them out. This is done using a for loop to iterate through the question_samples variable. The file name is \"Training.ipynb\" and the code is being used to demonstrate a new key by passing multiple input samples to a tokenizer.'},\n",
       " {'id': 'b64e132e-610b-444e-8fec-3c0f0a19b6aa',\n",
       "  'embedding': None,\n",
       "  'code': 'print(context) # You can check if you want but those questions are from the same context, so no need to print all of the 3.',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet prints the context, which is used to show that the questions are from the same context but there is no need to print all three. The file name is \"Training.ipynb\" and the context is used to pass multiple input samples to the tokenizer for a better demonstration of a new key.'},\n",
       " {'id': 'f3d24a28-e477-4822-a5bb-f3c885d1f953',\n",
       "  'embedding': None,\n",
       "  'code': 'inputs = tokenizer(\\n    question_samples, \\n    context_samples,\\n    max_length=100,\\n    truncation=\"only_second\",\\n    stride=50,\\n    return_overflowing_tokens=True,\\n    return_offsets_mapping=True\\n)',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is using the tokenizer function to tokenize the input data, which includes question_samples and context_samples. The maximum length of the tokens is set to 100, and any tokens that exceed this length will be truncated. The truncation method used is \"only_second\", which means only the context samples will be truncated. The stride is set to 50, which determines the amount of overlap between tokens. The function also returns any overflowing tokens and the start and end character for each token using the return_overflowing_tokens and return_offsets_mapping arguments. This code is likely used in a training notebook (Training.ipynb) to set up the tokenizer for a natural language processing task.'},\n",
       " {'id': '166138ad-8da5-4d69-9e72-7fc3766418b5',\n",
       "  'embedding': None,\n",
       "  'code': 'print(len(inputs[\"input_ids\"]))',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is printing the length of the \"input_ids\" in the inputs dictionary. The file name is \"Training.ipynb\" and the context is referring to a specific task of counting the number of windows with 3 inputs.'},\n",
       " {'id': '77ce7478-284e-4f83-b05b-c5b27c55955b',\n",
       "  'embedding': None,\n",
       "  'code': 'inputs[\"overflow_to_sample_mapping\"]',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code is mapping the overflow variable from a training file (Training.ipynb) to a specific input, in order to determine the amount of splitting that occurred for each input. This information is helpful in analyzing the data and understanding the training process.'},\n",
       " {'id': '0ed854d2-b7d8-413f-a8e2-bc699c6c8f7d',\n",
       "  'embedding': None,\n",
       "  'code': 'for id in inputs[\"input_ids\"]:\\n    print(tokenizer.decode(id))',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is a for loop that iterates through each id in the \"input_ids\" list, which is likely a part of a larger dataset. For each id, the code uses a tokenizer to decode it and then prints the result. This code is most likely a part of a larger training process, as indicated by the file name \"Training.ipynb\". The context suggests that the samples are being divided into windows, and the code is being used to process and print the decoded ids for each window.'},\n",
       " {'id': '4ea4a778-2909-4e36-b143-3bb98577aa20',\n",
       "  'embedding': None,\n",
       "  'code': '# print(question, \"\\\\n\", context) #descoment this cell if you don\\'t remember them',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is a comment that prints out a question and context, and prompts the user to uncomment the code if they do not remember the question or context. The file name is \"Training.ipynb\" and the context suggests that this is a file related to training and demonstrating a tokenizer. The code is used to retrieve a question and context for the demonstration.'},\n",
       " {'id': '519c14d4-65bc-469a-b1a8-d325e83601c6',\n",
       "  'embedding': None,\n",
       "  'code': 'inputs = tokenizer(\\n    question,\\n    context, \\n    max_length=100,\\n    truncation=\"only_second\",\\n    stride=50,\\n    return_overflowing_tokens=True,\\n    return_offsets_mapping=True\\n)\\n\\ninputs.keys()',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet uses the tokenizer to process text inputs, including a question and a context, with a maximum length of 100 characters. It uses the \"only_second\" truncation method and a stride of 50. It also returns any overflowing tokens and offsets mapping. The file name suggests that this code is used for training a model. The context suggests that this code is used for demonstrating how the tokenizer works with a single input. The input keys represent the different parts of the processed text.'},\n",
       " {'id': '11459cdd-cf57-49ac-b167-9e5755d314a4',\n",
       "  'embedding': None,\n",
       "  'code': \"# inputs['offset_mapping']\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': \"The code snippet accesses the 'offset_mapping' key from a dictionary called 'inputs'. The file name is 'Training.ipynb'. The code prints the value associated with the 'offset_mapping' key.\"},\n",
       " {'id': 'a95460a2-d07e-4484-a7d6-200b19ec9391',\n",
       "  'embedding': None,\n",
       "  'code': 'print(f\"Total number of lists: {len(inputs[\\'offset_mapping\\'])}\")',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is using the \"print\" function to display the total number of lists in the \"offset_mapping\" list, which is a part of the \"inputs\" dictionary. The \"len\" function is used to find the length of the \"offset_mapping\" list and the \"f\" character before the string allows for formatting of the output. This code is most likely a part of the \"Training\" file, which is a notebook file used for training a model or conducting a data analysis. The optional context suggests that the code is working with a list of lists of tuples, indicating that it is used for organizing and storing data.'},\n",
       " {'id': '20eb0243-7088-4739-8257-0d5e4d10d5e0',\n",
       "  'embedding': None,\n",
       "  'code': \"print(tokenizer.decode(inputs['input_ids'][0])) # Taking the firts window\\nprint(inputs['offset_mapping'][0])\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet prints the decoded first window of input IDs and the offset mapping of the first input. This is used for training and the offset mapping indicates the location of each token in the original sample. This code is found in a file named Training.ipynb.'},\n",
       " {'id': '281cb1ec-d9e5-4681-a74f-1a3c2ba8a24c',\n",
       "  'embedding': None,\n",
       "  'code': 'for id in inputs[\"input_ids\"]:\\n    print(tokenizer.decode(id))',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is a for loop that iterates through a list of input ids and prints out the decoded version of each id using the tokenizer. This code is likely part of a larger training process and is used to work with long contexts by splitting them into multiple windows. The file name \"Training.ipynb\" suggests that this code is being used for training a model.'},\n",
       " {'id': 'eb8aa63e-941a-4a23-92d2-7c28b2fa72bf',\n",
       "  'embedding': None,\n",
       "  'code': \"answer = squad_train_dataset[index_input]['answers']\\nprint(answer)\\n#print(context) # uncoment here to remember the original context\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code above retrieves the answer from the squad_train_dataset at the specified index_input. It then prints the answer. The commented out line indicates that the original context can be printed if needed. The file name suggests that this code is used for training a model. The context provides additional information about the data in the dataset.'},\n",
       " {'id': 'f2f1a182-bdd8-4069-9b46-ad26bf1ba666',\n",
       "  'embedding': None,\n",
       "  'code': \"print(inputs.sequence_ids(0)) # Getting the first window\\nprint(tokenizer.decode(inputs['input_ids'][0])) # first window context\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is using the sequence_ids method to print out the first window of a given input. It then uses the tokenizer to decode the input_ids of the first window and prints out the context of that window. This is likely used in the context of a training file, named Training.ipynb.'},\n",
       " {'id': '2993c0c5-06e7-45dc-992c-d9734076b3c6',\n",
       "  'embedding': None,\n",
       "  'code': 'sequence_ids = inputs.sequence_ids(0) # Getting the first window\\n\\nwind_ctx_start = sequence_ids.index(1) # Getting the first occurence of 1, which means the index where the context begins\\nwind_ctx_end = (len(sequence_ids) - sequence_ids[::-1].index(1) - 1) # Getting the index of the last 1, where the context ends\\n\\nwind_ctx_start, wind_ctx_end',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is obtaining the start and end indices of a context within a given window. The variable \"sequence_ids\" represents the sequence of IDs within the window, and the code finds the first occurrence of the value 1, which indicates the start of the context. The \"wind_ctx_start\" variable stores the index of the first occurrence of 1. The \"wind_ctx_end\" variable finds the index of the last occurrence of 1, representing the end of the context. The file name \"Training.ipynb\" suggests that this code is part of a training process or program. The optional context provides additional information about the purpose of the code, which is to determine the start and end indices of an answer within a given window.'},\n",
       " {'id': '04ac9115-6687-4644-a3fb-162ea3278be4',\n",
       "  'embedding': None,\n",
       "  'code': \"print(answer)\\nans_start_char = answer['answer_start'][0]\\nans_end_char = ans_start_char + len(answer['text'][0]) # the length of the text plus 515 is the final char of the answer\\n\\nprint((ans_start_char, ans_end_char))\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is used to print the starting and ending character positions of an answer within a given context. The file name suggests that it is used for training, most likely for a natural language processing task. The context refers to the original context in which the answer is located.'},\n",
       " {'id': 'a2ff783b-15a4-4cc6-b005-d63c8a59217c',\n",
       "  'embedding': None,\n",
       "  'code': \"offset = inputs['offset_mapping'][0] # First windows\\n# print(offset) # uncoment to remember the offset\\n# print(tokenizer.decode(inputs['input_ids'][0])) # and how they correspond to the original sentences\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is used for training a model. It retrieves the first set of offset mappings from the input, which is used to identify the character positions within the context. The code can also print the offset and the corresponding original sentences. This code is found in the file \"Training.ipynb\" and is used to help understand and use the offset mapping for training.'},\n",
       " {'id': 'c27f69e6-5597-4b77-8111-a269a8d19b1d',\n",
       "  'embedding': None,\n",
       "  'code': 'start_idx = 0\\nend_idx = 0\\n\\nif offset[wind_ctx_start][0] > ans_start_char or offset[wind_ctx_end][1] < ans_end_char:\\n    print(\"target is (0,0)\")\\nelse:\\n    i = wind_ctx_start\\n    for start_end_char in offset[wind_ctx_start:]:\\n        start, end = start_end_char\\n        if start == ans_start_char:\\n            start_idx = i\\n\\n        if end == ans_end_char:\\n            end_idx = i \\n            break\\n\\n        i += 1\\n    \\nstart_idx, end_idx',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet initializes two variables, start_idx and end_idx, to 0. It then checks if the offset at the start and end of the window context are greater than or less than the answer start and end characters. If they are, it prints the target as (0,0). If not, it iterates through the window context, comparing each start and end character to the answer start and end characters. If they match, the start_idx or end_idx is updated accordingly. The code then returns the values of start_idx and end_idx. This code most likely relates to training a model on a data set, as indicated by the file name \"Training.ipynb\".'},\n",
       " {'id': '2f7933db-b873-4d28-b7bf-ad9a8671780d',\n",
       "  'embedding': None,\n",
       "  'code': \"input_ids = inputs['input_ids'][0]\\n# tokenizer.decode(input_ids) # uncoment to visualize\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is extracting the input IDs from a dictionary called \"inputs\". It takes the first sample and assigns its input IDs to the variable \"input_ids\". This is then used to decode the input using a tokenizer. The file name indicates that this code is part of a training notebook, likely used for training a model on a dataset. The context suggests that the code is being used to check if the model\\'s predicted answers match the actual answers.'},\n",
       " {'id': 'c409c6f5-2437-4b81-81c3-d72dce7591b8',\n",
       "  'embedding': None,\n",
       "  'code': '# Placing the start_idx and end_idx and decoding.\\nprint(input_ids[start_idx:end_idx+1])\\nprint(tokenizer.decode(input_ids[start_idx : end_idx + 1]))',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet in the file \"Training.ipynb\" prints the input_ids from the start_idx to the end_idx+1, and then decodes them using a tokenizer. This is used to compare the input_ids with the expected answers.'},\n",
       " {'id': 'f26a6b0d-f734-4722-9c6e-dee159888feb',\n",
       "  'embedding': None,\n",
       "  'code': \"answer['text']\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': \"The code snippet accesses the value stored in the 'text' key in the 'answer' dictionary. This code is likely found in a file named 'Training.ipynb' and is used to retrieve the real answer from a larger set of data.\"},\n",
       " {'id': 'b39f4656-43af-48ac-9f57-158964741c80',\n",
       "  'embedding': None,\n",
       "  'code': 'def find_asnwer_token_idx(\\n        ctx_start,\\n        ctx_end,\\n        ans_start_char,\\n        ans_end_char,\\n        offset\\n):\\n\\n    start_idx = 0\\n    end_idx = 0\\n\\n    if offset[ctx_start][0] > ans_start_char or offset[ctx_end][1] < ans_end_char:\\n        pass #answer does not exist\\n    else:\\n        i = ctx_start\\n        # aligning the indices of the answers within the context windows\\n        for start_end_char in offset[ctx_start:]:\\n            start, end = start_end_char\\n            if start == ans_start_char:\\n                start_idx = i\\n\\n            if end == ans_end_char:\\n                end_idx = i \\n                break\\n\\n            i += 1\\n    return start_idx, end_idx\\n        ',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code above is a function that takes in parameters related to a context and an answer, and returns the indices of the answer within the context. It first checks if the answer exists within the given context, and if not, returns a pass statement. If the answer does exist, it then iterates through the context to align the indices of the answer within the context window. It does so by comparing the start and end characters of the answer to the start and end characters of the context. Finally, it returns the start and end indices of the answer within the context.'},\n",
       " {'id': 'f793c5bc-d54e-42ca-ba7c-50e0e332380c',\n",
       "  'embedding': None,\n",
       "  'code': '# now applying to the whole dataset\\nstart_idxs = []\\nend_idxs = []\\n\\nfor i, offset in enumerate(inputs[\"offset_mapping\"]):\\n    sequence_ids = inputs.sequence_ids(i)\\n\\n    ctx_start = sequence_ids.index(1)\\n    ctx_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1\\n\\n    start_idx, end_idx = find_asnwer_token_idx(\\n        ctx_start,\\n        ctx_end,\\n        ans_start_char,\\n        ans_end_char,\\n        offset\\n    )\\n\\n    start_idxs.append(start_idx)\\n    end_idxs.append(end_idx)\\n\\nstart_idxs, end_idxs',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is a loop that iterates through a dataset and performs a series of operations to find the start and end indices of a specific answer token. It uses the sequence_ids and offset_mapping inputs to determine the context of the answer token and then calls the find_answer_token_idx function to find the start and end indices. These indices are then appended to two lists, start_idxs and end_idxs. The file name is Training.ipynb and the code is being used to create a function.'},\n",
       " {'id': 'd86f0cdc-df13-4283-8a9b-2a9c63ec09e3',\n",
       "  'embedding': None,\n",
       "  'code': 'for q in squad_dataset[\"train\"][\"question\"][:1000]:\\n    if q.strip() != q:\\n        print(q)',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is iterating through the first 1000 questions in the \"train\" portion of a dataset called \"squad_dataset\". It then checks each question to see if it contains any extra white spaces at the beginning or end. If it does, it prints out the question. This code is likely part of a larger training file, possibly used for data preprocessing or cleaning before training a machine learning model.'},\n",
       " {'id': 'f9a9de8e-4e5a-442a-9b35-3417380d017f',\n",
       "  'embedding': None,\n",
       "  'code': '# Defining some fixed args\\nmax_length = 384 # Indicated by Google\\nstride = 128',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is defining two fixed arguments, \"max_length\" and \"stride\", with values of 384 and 128, respectively. These values are specified by Google. The file name is \"Training.ipynb\". The context mentions a tokenizer function and mentions adding a part for handling extra white spaces. \\n\\nIn summary, the code snippet is setting fixed arguments and the file name suggests that it is related to training, possibly for natural language processing tasks. The context suggests that the code is creating a function to tokenize data and handle extra white spaces.'},\n",
       " {'id': '0d02eeeb-0fb8-4844-8476-644504ed243e',\n",
       "  'embedding': None,\n",
       "  'code': 'def tokenize_fn_train(batch):\\n    questions = [q.strip() for q in batch[\\'question\\']]\\n\\n    inputs = tokenizer(\\n        questions, \\n        batch[\\'context\\'],\\n        max_length=max_length,\\n        truncation=\"only_second\",\\n        stride=stride,\\n        return_overflowing_tokens=True,\\n        return_offsets_mapping=True,\\n        padding=\"max_length\"\\n    )\\n\\n    # We won\\'t use those guys so let\\'s kick them off (remove them)\\n    offset_mapping = inputs.pop(\"offset_mapping\")\\n    orig_sample_idxs = inputs.pop(\"overflow_to_sample_mapping\")\\n\\n    # From the original dataset\\n    answers = batch[\\'answers\\']\\n    start_idxs, end_idxs = [], []\\n\\n    # Loops we just saw\\n    for i, offset in enumerate(offset_mapping):\\n        sample_idx = orig_sample_idxs[i]\\n        answer = answers[sample_idx]\\n\\n        ans_start_char = answer[\\'answer_start\\'][0]\\n        ans_end_char = ans_start_char + len(answer[\\'text\\'][0])\\n\\n        sequence_ids = inputs.sequence_ids(i)\\n\\n        # Aligning the indexes\\n        ctx_start = sequence_ids.index(1)\\n        ctx_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1\\n\\n        start_idx, end_idx = find_asnwer_token_idx(\\n            ctx_start,\\n            ctx_end,\\n            ans_start_char,\\n            ans_end_char,\\n            offset\\n        )\\n\\n        start_idxs.append(start_idx)\\n        end_idxs.append(end_idx)\\n\\n    inputs[\"start_positions\"] = start_idxs\\n    inputs[\"end_positions\"] = end_idxs\\n\\n    return inputs',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is a function called \"tokenize_fn_train\" that is used for tokenizing a batch of data. It takes in a batch of questions and contexts and uses a tokenizer to preprocess the data. The tokenizer also includes parameters such as maximum length, truncation, and padding. The code then removes unnecessary data and aligns the remaining data, including start and end positions for the answers. The function returns the tokenized inputs along with the start and end positions of the answers. This code is used in a file called \"Training.ipynb\" and is used for training a model.'},\n",
       " {'id': 'ecfdcf65-58f5-4b69-89cb-37a0fa9a4fff',\n",
       "  'embedding': None,\n",
       "  'code': 'train_dataset = squad_train_dataset.map(\\n    tokenize_fn_train,\\n    batched=True,\\n    remove_columns=squad_train_dataset.column_names\\n)',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is tokenizing the train dataset by applying a function called \"tokenize_fn_train\" to it. This is being done in a file called \"Training.ipynb\". The function is being applied in a batched manner and any unnecessary columns in the dataset are being removed.'},\n",
       " {'id': '3f52a8a5-b53a-4270-8404-b625fa2648e6',\n",
       "  'embedding': None,\n",
       "  'code': '# The actual train dataset ir a little bit  bigger than the original \\n# Because we\\'ve expanded the context in windows\\nprint(f\\'Processed dataset: {len(train_dataset)}\\\\nOriginal dataset: {len(squad_dataset[\"train\"])}\\')',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is printing the lengths of the processed and original train datasets, and it is also mentioning that the processed dataset is slightly bigger because the context has been expanded in windows. The file name is likely referring to a training notebook, and the context indicates that the code is related to tokenizing the train dataset.'},\n",
       " {'id': 'c63458ac-5b97-488e-98e7-c8ff8bbe31f1',\n",
       "  'embedding': None,\n",
       "  'code': 'def tokenize_fn_validation(batch):\\n    questions = [q.strip() for q in batch[\\'question\\']]\\n\\n    inputs = tokenizer(\\n        questions, \\n        batch[\\'context\\'],\\n        max_length=max_length,\\n        truncation=\"only_second\",\\n        stride=stride,\\n        return_overflowing_tokens=True,\\n        return_offsets_mapping=True,\\n        padding=\"max_length\"\\n    )\\n\\n    orig_sample_idxs = inputs.pop(\"overflow_to_sample_mapping\")\\n    sample_ids = []\\n\\n    for i in range(len(inputs[\"input_ids\"])):\\n        # Getting the corresponding ID from the original samples (thei identify the questions and contexts remember?) \\n        sample_idx = orig_sample_idxs[i]\\n        sample_ids.append(batch[\"id\"][sample_idx])\\n\\n        sequence_ids = inputs.sequence_ids(i) # 1:context | 0:question | (0,0): special tokens\\n        offset = inputs[\"offset_mapping\"][i] # getting the sequence_ids for this sample\\n\\n        # Modifying the original offset_mapping \\n        # When it is (0,0) or 0 replace with None\\n        # And get just the context\\n        inputs[\"offset_mapping\"][i] = [\\n            x if sequence_ids[j] == 1 else None for j, x in enumerate(offset)\\n        ]\\n\\n    inputs[\"sample_id\"] = sample_ids\\n    return inputs',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is a function named \"tokenize_fn_validation\" that takes in a batch of data as input. It first strips any leading or trailing spaces from the \"question\" column in the batch. Then, it uses a tokenizer to tokenize the \"questions\" and the \"context\" columns in the batch. The tokenizer also sets a maximum length, truncates the context only, and returns any overflow tokens and offset mappings. \\n\\nNext, it creates a list of original sample indices and a list of sample IDs. For each input in the batch, it gets the corresponding ID from the original samples and appends it to the sample IDs list. It also retrieves the sequence IDs and offset mappings for each input. The offset mappings are then modified by replacing any (0,0) or 0 values with None, and only keeping the context portion of the offset. \\n\\nFinally, the function adds the sample IDs to the inputs and returns them. This code is used in a file called \"Training.ipynb\" to create a similar function for the validation dataset.'},\n",
       " {'id': 'c911de12-9377-43d1-a027-936600974b38',\n",
       "  'embedding': None,\n",
       "  'code': 'validation_dataset = squad_dataset[\"validation\"].map(\\n    tokenize_fn_validation,\\n    batched=True,\\n    remove_columns=squad_dataset[\"validation\"].column_names\\n)\\n\\nprint(f\\'Processed dataset: {len(validation_dataset)}\\\\nOriginal dataset: {len(squad_dataset[\"validation\"])}\\')',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code creates a validation dataset by mapping a function called \"tokenize_fn_validation\" to the \"squad_dataset\" validation data. It also removes any unnecessary columns and prints the length of the processed and original datasets. This code is likely part of a larger file called \"Training.ipynb\" that is used for training a machine learning model.'},\n",
       " {'id': '6576dbfc-8ed8-416d-ab07-e5419ec6f528',\n",
       "  'embedding': None,\n",
       "  'code': 'metric = load_metric(\"squad\")',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet loads a metric called \"squad\" and assigns it to a variable called \"metric\". This metric can then be used for a problem in the context of a training notebook file.'},\n",
       " {'id': '3550b4a7-8431-4242-bfee-8b7665b949d9',\n",
       "  'embedding': None,\n",
       "  'code': \"# making some examples\\n\\npred_answers = [\\n    {'id': '1', 'prediction_text': 'Strawberry'},\\n    {'id': '2', 'prediction_text': 'Agriculture industry'},\\n    {'id': '3', 'prediction_text': 'Red'}\\n]\\n\\ntrue_answers = [\\n    {'id': '1', 'answers': {'text': ['Strawberry'], 'answer_start': [80]}},\\n    {'id': '2', 'answers': {'text': ['Agroindustry'], 'answer_start': [65]}},\\n    {'id': '3', 'answers': {'text': ['Red'], 'answer_start': [100]}}\\n]\\n\\n# checking the metrics\\n\\nmetric.compute(predictions=pred_answers, references=true_answers)\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is creating two lists, \"pred_answers\" and \"true_answers\", which contain dictionaries with information about predictions and true answers for a given problem. The file \"Training.ipynb\" is used to train a model and the context mentions using a metric called \"squad\" for this purpose. The code also includes a step to check the metrics by using the \"compute\" function, which takes in the predictions and true answers as parameters.'},\n",
       " {'id': 'd47a3798-0bef-44b1-a3c6-8ac70876fcbf',\n",
       "  'embedding': None,\n",
       "  'code': 'small_validation_dataset = squad_dataset[\"validation\"].select(range(100)) # Getting just the first 100 samples from the validation set \\ntrained_checkpoint = \"distilbert-base-cased-distilled-squad\" # model trained in q&a\\n\\ntokenizer2 = AutoTokenizer.from_pretrained(trained_checkpoint) # new tokenizer from distilbert-base-cased-distilled-squad\\n\\n# Here, since the tokenizer is a global variable \\n# And we\\'re training it with another model trained in q&a\\n# We\\'re temporarily exchanging this global variable for the tokenizer2\\noriginal_tokenizer = tokenizer\\ntokenizer = tokenizer2',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet selects the first 100 samples from the validation set and stores them in a variable. It also loads a trained model for question and answer tasks and creates a new tokenizer from that model. The code then replaces the original tokenizer with this new tokenizer. This is done as part of training a model to transform logits into answers, in a file named \"Training.ipynb\".'},\n",
       " {'id': 'de143da2-0e7d-4702-b797-61163b3ed08c',\n",
       "  'embedding': None,\n",
       "  'code': 'small_validation_processed = small_validation_dataset.map( # Now, we can use this new tokenizer from distilbert-base-cased-distilled-squad\\n    tokenize_fn_validation,                                 # and map it into our small validation dataset using the function tokenize_fn_validation\\n    batched=True,\\n    remove_columns=squad_dataset[\"validation\"].column_names\\n)',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is using a tokenizer from the distilbert-base-cased-distilled-squad model to map the small validation dataset. This is done using the function \"tokenize_fn_validation\". The code also specifies to process the dataset in batches and remove columns from the \"validation\" dataset in the \"squad_dataset\" file. The file name is \"Training.ipynb\" and the code is part of a larger training process.'},\n",
       " {'id': 'd40e3a8d-6004-4440-a93e-aad98f43bc98',\n",
       "  'embedding': None,\n",
       "  'code': 'tokenizer = original_tokenizer',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet assigns the value of \"original_tokenizer\" to the variable \"tokenizer\". This is done in the file named \"Training.ipynb\". The context suggests that the purpose of this code is to retrieve the original tokenizer from the distilbert-base-cased model.'},\n",
       " {'id': '879fbc6e-8023-48a3-84de-22ba6278f115',\n",
       "  'embedding': None,\n",
       "  'code': 'small_model_inputs =  small_validation_processed.remove_columns([\\'sample_id\\', \\'offset_mapping\\']) # unused columns\\nsmall_model_inputs.set_format(\"torch\")',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is removing the columns \\'sample_id\\' and \\'offset_mapping\\' from the \\'small_validation_processed\\' dataset and then setting the format of the remaining columns to \"torch\". This is likely being done in the file \"Training.ipynb\" in order to optimize the dataset for processing on a GPU.'},\n",
       " {'id': 'ae53c8d4-925e-4b39-ad04-c90cb3d20edf',\n",
       "  'embedding': None,\n",
       "  'code': '# Setting the GPU as current device \\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet sets the device to be used for computations to either GPU if it is available, or to CPU if not. This allows for faster processing if a GPU is available. The file name suggests that this code is used for training a model and the context clarifies that the inputs are moved to the designated device for training. Essentially, this code ensures that the appropriate device is used for the training process.'},\n",
       " {'id': '6bfe8992-9525-48c4-a501-bf33f8a24bce',\n",
       "  'embedding': None,\n",
       "  'code': 'small_model_inputs_gpu = {\\n    k: small_model_inputs[k].to(device) for k in small_model_inputs.column_names\\n}\\n# All the data will come from the GPU now',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is creating a dictionary called \"small_model_inputs_gpu\" by iterating through the keys of the \"small_model_inputs\" dictionary and assigning the corresponding values to the GPU device. This means that all the data in the \"small_model_inputs\" dictionary will now be stored and processed on the GPU. This code is likely used for training a model on a GPU, as indicated by the file name \"Training.ipynb\".'},\n",
       " {'id': '775ed5fb-12e2-4ac7-b13a-009a5d8a45e3',\n",
       "  'embedding': None,\n",
       "  'code': 'trained_model =  AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(device)',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet initializes a trained model for question answering, using the distilbert-base-cased-distilled-squad model that was previously downloaded and saved as \"trained_checkpoint\". The model is then set to use the specified device (likely a GPU) for faster processing. This code is likely part of a larger training process, as indicated by the file name \"Training.ipynb\".'},\n",
       " {'id': '9488d115-5a14-47bd-9f08-313ef458567a',\n",
       "  'embedding': None,\n",
       "  'code': \"with torch.no_grad(): # This is just saying that we're not using any compution gradient (like we're not training)\\n    outputs = trained_model(**small_model_inputs_gpu) # passing the inputs to distilbert-base-cased-distilled-squad and getting the outputs\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': \"The code snippet is using the torch library to disable computation gradient, indicating that the model is not being trained. It then passes a set of inputs to a pre-trained model, distilbert-base-cased-distilled-squad, and obtains the outputs. The file name suggests that this code is part of a larger file related to training. Overall, the code is retrieving the model's output without performing any training.\"},\n",
       " {'id': 'bb221370-09bc-4b4e-a8d9-399a89be5319',\n",
       "  'embedding': None,\n",
       "  'code': 'outputs',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is likely part of a larger program or training session contained in the file \"Training.ipynb\". It is designed to output specific information, most likely related to the training session or program. The optional context suggests that the outputs may be used to evaluate the success of the training.'},\n",
       " {'id': '0f287f44-799c-4e47-8d61-49af58cbf16d',\n",
       "  'embedding': None,\n",
       "  'code': \"# Here, we're getting the logits, moving back to CPU and formatting as a numpy array (we don't need them in the tensor format anymore)\\nstart_logits = outputs.start_logits.cpu().numpy()\\nend_logits = outputs.end_logits.cpu().numpy()\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is converting the logits (output values from a neural network) into a numpy array and moving it from the GPU to the CPU. This is done in order to use the logits for further processing, as they are no longer needed in their original tensor format. The file name is \"Training.ipynb\" and the context is possibly within a training notebook or script.'},\n",
       " {'id': 'fef89120-de44-4c0d-89d7-cccdb493766c',\n",
       "  'embedding': None,\n",
       "  'code': 'small_validation_processed[\"sample_id\"][:3] # remember that small_validation_processed was processed by distilbert-base-cased-distilled-squad tokenizer',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is accessing the first three elements of the \"sample_id\" column in the \"small_validation_processed\" file, which was processed by the distilbert-base-cased-distilled-squad tokenizer. The file name is \"Training.ipynb\". The context is reminding the user of the format of the IDs in the \"small_validation_processed\" file.'},\n",
       " {'id': '8bcdd349-562e-477a-a050-3ef201d5f149',\n",
       "  'embedding': None,\n",
       "  'code': 'validation_dataset[\"sample_id\"][:3]',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is accessing the first three rows of the \"sample_id\" column in the validation dataset. The file name is \"Training.ipynb\" and this code is likely used for training a model or performing some analysis on the validation dataset. The context suggests that the code is used to view the values in the \"sample_id\" column in the validation dataset.'},\n",
       " {'id': '00ea6acc-34d2-47c6-8ddb-d0d4b692cf4b',\n",
       "  'embedding': None,\n",
       "  'code': 'print(f\"Total ID\\'s from validation: {len(validation_dataset[\\'sample_id\\'])},\\\\nTotal unique ID\\'s from validation: {len(set(validation_dataset[\\'sample_id\\']))}\")',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is printing the total number of IDs and the total number of unique IDs from a validation dataset. The file name is \"Training.ipynb\" and the context explains that some IDs may not be unique due to windows splitting up inputs.'},\n",
       " {'id': 'b21a62a7-e70b-4ee4-b45e-9f0f8a32a446',\n",
       "  'embedding': None,\n",
       "  'code': 'sample_id2idxs = {}\\n\\nfor i, id_ in enumerate(small_validation_processed[\\'sample_id\\']): # looping through all the ID\\'s and enumerating to get the index\\n    if id_ not in sample_id2idxs: # Checking if this ID existis\\n        sample_id2idxs[id_] = [i] # If not, we create an entry with the format we just saw above.\\n    else:\\n        print(\"here\") # If existis,\\n        sample_id2idxs[id_].append(i) # we just append into the existing list',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet creates an empty dictionary called sample_id2idxs and then loops through all the IDs in the file named small_validation_processed. For each ID, it checks if it already exists in the dictionary. If not, it creates an entry with the ID as the key and the index as the value. If the ID already exists, it appends the index to the existing list of indices for that ID. This process creates a mapping between each ID and the corresponding indices in the file. This code is likely used for organizing and managing data for training a machine learning model.'},\n",
       " {'id': '48360138-12e8-43ca-b3fd-6e56dc16ac58',\n",
       "  'embedding': None,\n",
       "  'code': '# sample_id2idxs # uncoment to see the result',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is a commented function named \"sample_id2idxs\" that is used for building a dictionary of ID-index pairs. The file name is \"Training.ipynb\" and the context suggests that this code is being used to handle input data that has been split into multiple parts. The code creates a dictionary where the keys are the IDs and the values are lists of indexes that correspond to these IDs in a specific file named \"small_validation_processed\". This allows for efficient referencing of specific data points within the split input.'},\n",
       " {'id': '973e8837-e2c1-47ca-a4df-7aaa0d10fdd2',\n",
       "  'embedding': None,\n",
       "  'code': 'start_logits.shape, end_logits.shape # remember that they come from the outputs we got from distilbert-base-cased-distilled-squad',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is used to check the shape of the start and end logits, which are obtained from the outputs of a model called \"distilbert-base-cased-distilled-squad\". The file name is \"Training.ipynb\", which suggests that this code is used for training the model. The context provides additional information that the shape of the logits is expected to be in the form of (number_of_samples, max_length).'},\n",
       " {'id': '0a30e76e-b102-4328-b1ab-37ba63008410',\n",
       "  'embedding': None,\n",
       "  'code': '# uncoment to see\\n#print(start_logits[0])\\n#print()\\n#print(-start_logits[0])',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code above is used for training a model in a Jupyter notebook file called \"Training.ipynb\". It is used to sort the indices of the logits (values used to determine the output of a neural network) in order to find where the values are stored within these indices. The code snippet includes three print statements that are commented out, meaning they are not currently being executed. These statements would print the first start_logits value, a blank line, and the negated value of the first start_logits value, respectively. By negating the values, the indices will be sorted in descending order, which is helpful for training the model.'},\n",
       " {'id': '59a14705-5c0b-482a-8acf-529f297abd3d',\n",
       "  'embedding': None,\n",
       "  'code': 'indices = (-start_logits[0]).argsort() # here, we are taking just the first position for example.\\nindices',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is sorting the values in the start_logits array in descending order and storing the indices of the sorted values in the indices variable. The file name suggests that this code is used in a training process. The context explains that the code is used to organize the array in ascending order.'},\n",
       " {'id': '8f309e41-c054-4eca-8b9a-37a35a12d2e6',\n",
       "  'embedding': None,\n",
       "  'code': 'start_logits[0][indices]',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is accessing the first element in the \"start_logits\" array at the indices specified. This is likely part of a larger training process, potentially for a machine learning model, which is being saved in the file \"Training.ipynb\". The optional context suggests that these indices are used to retrieve a specific result from the original array.'},\n",
       " {'id': 'c443d6ed-aeea-4830-8398-12e0adfb272e',\n",
       "  'embedding': None,\n",
       "  'code': 'n_largest = 20 # Number of start and end logits we want to search\\nmax_answer_length = 30 # Max answer length we want to allow\\npredict_answers = [] # List of predicted answers will be stored \\n\\nfor sample in small_validation_dataset: # For each sample in the NON-processed (it is not tokenized!) small validation dataset (CTRL+click if you want to remember)\\n    sample_id = sample[\"id\"] # Get the id from this sample\\n    context = sample[\"context\"] # and the context\\n\\n    # Initializing best_score and best_answer (they\\'ll be update in the below looping)\\n    best_score = float(\"-inf\") \\n    best_answer = None\\n\\n    for idx in sample_id2idxs[sample_id]: # For each id in the sample_id2idxs (samples here are tokenized!) in the sample_id as index (remebmer it is a dict)\\n        # Grabbing the start and end logits for this index\\n        start_logit = start_logits[idx] \\n        end_logit = end_logits[idx]\\n        # And also get the offset mapping for this index\\n        offsets = small_validation_processed[idx][\"offset_mapping\"] # note that this offset mapping is the processed, containg None for any position\\n                                                                    # that is not in the context\\n        # Sorting the logits as we saw                                                           \\n        start_indices = (-start_logit).argsort() \\n        end_indices = (-end_logit).argsort()\\n\\n        # Next step is to loop through the n_largest start and end logits\\n        for start_idx in start_indices[:n_largest]:\\n            for end_idx in end_indices[:n_largest]:\\n                # Checking the cases where the answer:\\n                if offsets[start_idx] is None or offsets[end_idx] is None: # Answer is not in the context\\n                    continue\\n                if end_idx < start_idx: # Answer does not exist (since is has negative length)\\n                    continue\\n                if (end_idx - start_idx + 1) > max_answer_length: # Answer is longer than allowed\\n                    continue\\n\\n                # If we have an answer,\\n                score = start_logit[start_idx] + end_logit[end_idx] # Compute the score for this answer\\n                if score > best_score: # Checking if score is better than the current best_score\\n                    best_score = score # If yes, compute\\n\\n                    # Getting the position of the first character and of the last character\\n                    first_ch = offsets[start_idx][0] \\n                    last_ch = offsets[end_idx][1]\\n                    # Retrieving the answer as actual text using the them as indices in the context\\n                    best_answer = context[first_ch:last_ch]\\n        # And finally append to the list        \\n        predict_answers.append({\"id\": sample_id, \"prediction_text\": best_answer})',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': \"The code snippet is part of a larger program, likely a natural language processing task, that takes in a dataset and makes predictions based on a given context. It sets the number of start and end logits to be searched, the maximum length allowed for an answer, and creates an empty list to store predicted answers. It then loops through each sample in a small validation dataset, retrieves the sample's ID and context, and initializes variables for the best score and best answer. Within this loop, it retrieves start and end logits for a specific index, as well as an offset mapping. It then sorts the logits and loops through the top n_largest start and end logits, checking for cases where an answer does not exist or is not within the given context. If an answer is found, it computes a score and checks if it is better than the current best score. If so, it updates the best score and retrieves the answer from the given context. Finally, it appends the predicted answer to the list. The file name suggests that this code is part of a training process for a larger machine learning model.\"},\n",
       " {'id': 'd5c9abfb-17c9-4e33-a907-d8eede1a3257',\n",
       "  'embedding': None,\n",
       "  'code': 'true_answers = [\\n    {\\n    \"id\": x[\"id\"],\\n    \"answers\": x[\"answers\"]\\n    }\\n    for x in small_validation_dataset\\n]',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is creating a list of dictionaries containing the \"id\" and \"answers\" of each element in the \"small_validation_dataset\" list. This is being done using a list comprehension, where the variable x represents each element in the list. The file name suggests that this code is part of a training process. The context mentions formatting the true answers for computing metrics, indicating that this code is preparing data for evaluation.'},\n",
       " {'id': '333151f6-84de-4558-b984-caaaadee5da2',\n",
       "  'embedding': None,\n",
       "  'code': '#true_answers # uncoment to see the result',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is a commented out line of code that enables the user to see the results of true answers. The file name is \"Training.ipynb\" and the context suggests that the code is used for formatting true answers in a specific format for computing metrics. It also indicates that the code is part of a larger outline related to metrics and logits.'},\n",
       " {'id': '887f8185-ef73-4868-9258-1bbb02804a7b',\n",
       "  'embedding': None,\n",
       "  'code': 'metric.compute(predictions=predict_answers, references=true_answers)',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'This code snippet is calling the \"compute\" function from a metric library, passing in two parameters: \"predict_answers\" and \"true_answers\". The file name suggests that this code is being used in a training notebook, possibly for a machine learning model. The optional context indicates that the code is taking the \"logits\" and converting them into strings before calculating some metrics. Overall, the code is likely used for evaluating the performance of a model during training.'},\n",
       " {'id': '56778de0-d8bf-449d-b49b-c018034a16c2',\n",
       "  'embedding': None,\n",
       "  'code': 'def compute_metrics(start_logits, end_logits, processed_dataset, orig_dataset):\\n    sample_id2idxs = {}\\n\\n    for i, id_ in enumerate(processed_dataset[\"sample_id\"]):\\n        if id_ not in sample_id2idxs:\\n            sample_id2idxs[id_] = [i]\\n        else:\\n            sample_id2idxs[id_].append(i)\\n\\n    predicted_answers = []\\n    for sample in tqdm(orig_dataset):\\n\\n        sample_id = sample[\"id\"]\\n        context = sample[\\'context\\']\\n\\n        best_score = float(\"-inf\")\\n        best_answer = None\\n\\n        for idx in sample_id2idxs[sample_id]:\\n            start_logit = start_logits[idx]\\n            end_logit = end_logits[idx]\\n\\n            offsets = processed_dataset[idx][\"offset_mapping\"]\\n\\n            start_indices = (-start_logit).argsort()\\n            end_indices = (-end_logit).argsort()\\n\\n            for start_idx in start_indices[:n_largest]:\\n                for end_idx in end_indices[:n_largest]:\\n                    if offsets[start_idx] is None or offsets[end_idx] is None:\\n                        continue\\n\\n                    if end_idx < start_idx:\\n                        continue\\n\\n                    if (end_idx - start_idx + 1) > max_answer_length:\\n                        continue\\n\\n                    score = start_logit[start_idx] + end_logit[end_idx]\\n                    if score > best_score:\\n                        best_score = score\\n\\n                        first_ch = offsets[start_idx][0] \\n                        last_ch = offsets[end_idx][1]\\n                        best_answer = context[first_ch:last_ch]\\n                \\n        predicted_answers.append({\"id\": sample_id, \"prediction_text\": best_answer})\\n    true_answers = [{\"id\": x[\"id\"], \"answers\": x[\"answers\"]} for x in orig_dataset]\\n    y = metric.compute(predictions=predicted_answers, references=true_answers)\\n    return y\\n    ',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code above is a function called \"compute_metrics\" that takes in four parameters: start_logits, end_logits, processed_dataset, and orig_dataset. It first creates a dictionary called \"sample_id2idxs\" and populates it with sample IDs as keys and a list of indices as values. This is done by iterating through the \"processed_dataset\" and checking if the sample ID is already in the dictionary. If not, it adds the sample ID as a key and the index as a value. If it is already in the dictionary, it appends the index to the existing list.\\n\\nNext, the code iterates through the \"orig_dataset\" and retrieves the sample ID and context. It then sets a variable called \"best_score\" to a very low value and \"best_answer\" to None. Then, it iterates through the indices for the given sample ID and retrieves the start and end logits. It also retrieves the offsets from the corresponding index in the \"processed_dataset\". The start and end indices are sorted in descending order and the top n indices are selected.\\n\\nThe code then checks for any invalid offsets and skips them. It also checks for any overlapping indices and disregards them. It also checks for the maximum answer length and skips any indices that exceed it'},\n",
       " {'id': '6c7d8553-4422-42ca-8659-ccdac4f606ac',\n",
       "  'embedding': None,\n",
       "  'code': 'compute_metrics(\\n    start_logits,\\n    end_logits,\\n    small_validation_processed,\\n    small_validation_dataset\\n)',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is calling the function \"compute_metrics\" with the parameters \"start_logits\", \"end_logits\", \"small_validation_processed\", and \"small_validation_dataset\". The function is likely performing some kind of computation or analysis on the data contained in these parameters. The file \"Training.ipynb\" is used to run this code and the function is being applied to the small datasets used previously.'},\n",
       " {'id': '5f56809a-1067-44de-a708-e897e2b6c216',\n",
       "  'embedding': None,\n",
       "  'code': 'mlflow.end_run()\\nmlflow.set_experiment(\"BERT Q&A - distilbert-base-cased\")',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is using the mlflow library to end the current run and set the experiment to \"BERT Q&A - distilbert-base-cased\". This code is most likely used in a Jupyter notebook file called \"Training.ipynb\" to manage and track the training process of a BERT model for question and answer tasks.'},\n",
       " {'id': '856ecbf0-ea92-4f0f-8f79-7a31a471afdc',\n",
       "  'embedding': None,\n",
       "  'code': 'model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint_bbc) # Loading the model we want to fine-tune (distilbert-base-cased)',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code is loading a pre-trained model for question answering (specifically, the model \"distilbert-base-cased\") from the file named \"model_checkpoint_bbc\" and storing it as the variable \"model\". This model will be used for fine-tuning in the file named \"Training.ipynb\".'},\n",
       " {'id': '79742e31-1680-4a48-8202-e6647a1968b9',\n",
       "  'embedding': None,\n",
       "  'code': 'args = TrainingArguments(\\n    \"finetuned-squad\", # this is a default name for this model and task\\n    evaluation_strategy=\"no\", # No, because we\\'ll compute metrics manually\\n    save_strategy=\"epoch\", # saving for each step (you can use epoch as well)\\n    learning_rate=2e-5, # learnin rate value \\n    num_train_epochs=3, # 3 epoch in total (max is 4 since out inputs are very large, more tha that is not recommended)\\n    weight_decay=0.01, # regularization technique\\n    fp16=True # speed up the process\\n)',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet creates an object called \"args\" which contains various arguments for training a model. These arguments include the name of the model and task, the evaluation strategy, the saving strategy, the learning rate, the number of training epochs, the weight decay, and whether or not to use fp16 (a speed optimization technique). The file name is \"Training.ipynb\" and the context suggests that this code is used in the training step of a larger process.'},\n",
       " {'id': 'cf3745b3-9708-4c1c-b427-ade06d91b189',\n",
       "  'embedding': None,\n",
       "  'code': 'trainer = Trainer(\\n    model=model, # our model\\n    args=args, # our args\\n    train_dataset=train_dataset, # our datasets\\n    eval_dataset=validation_dataset,\\n    tokenizer=tokenizer # and our tokenizer\\n)',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code is creating an instance of a trainer object, which will be used to train a model. The trainer is configured with the specified model, arguments, training dataset, evaluation dataset, and tokenizer. This code is likely part of a larger script or notebook file named \"Training.ipynb\".'},\n",
       " {'id': '40c52be4-79cf-4e7c-9ce4-79b18dcd8915',\n",
       "  'embedding': None,\n",
       "  'code': 'torch.cuda.is_available()',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is checking if the GPU (Graphics Processing Unit) is available for use. This is done using the torch.cuda.is_available() function. The file name, \"Training.ipynb\", suggests that this code is being used for training a machine learning model, and the context indicates that the code is checking if the GPU can be used for faster processing during training.'},\n",
       " {'id': '86ccf40f-6e5c-4452-ba3f-e35cbbf0ee29',\n",
       "  'embedding': None,\n",
       "  'code': \"mlflow.end_run()\\nstart_time_training = datetime.now() # this is for computing the time it take for the training\\nwith mlflow.start_run():\\n    trainer.train() \\nprint(f'Total time for training: {datetime.now() - start_time_training}')\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code above is used for recording and tracking the training process of a machine learning model. It uses the mlflow library to create a new run, which includes a specific code snippet for training the model. The code snippet also calculates the time it takes for the training process to complete. The file name suggests that this code is used in a Jupyter Notebook file for training the model. The \"context\" refers to the overall purpose of the code, which is to start and end a training run and track the time it takes to complete.'},\n",
       " {'id': 'e0b3cd6d-be55-4ae9-af8e-e7f58371ae6e',\n",
       "  'embedding': None,\n",
       "  'code': 'trainer_prediction = trainer.predict(validation_dataset) # getting the predictions for the validation set\\ntrainer_prediction',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is using a \"trainer\" object to make predictions on a validation dataset. The result of the prediction is stored in a variable called \"trainer_prediction\". The code is then outputting the value of this variable. This code is likely part of a larger file called \"Training.ipynb\" and is used for evaluating the performance of a model.'},\n",
       " {'id': 'edb1ea29-55c3-49e3-8b92-35724582462e',\n",
       "  'embedding': None,\n",
       "  'code': 'predictions, _, _ = trainer_prediction\\npredictions',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is using unpacking to extract the first element of the object \"trainer_prediction\" and assigning it to the variable \"predictions\". The \"_\" is used as a placeholder for elements that are not needed. The file \"Training.ipynb\" likely contains code related to training a model or making predictions. The optional context suggests that the code is specifically extracting the prediction values from the object.'},\n",
       " {'id': '185ad57d-f96f-4dbf-9435-150bc968a063',\n",
       "  'embedding': None,\n",
       "  'code': 'start_logits, end_logits = predictions',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': \"The code snippet is separating the values in the 'predictions' array into two separate arrays, 'start_logits' and 'end_logits'. The file name suggests that this code is used for training, likely in a machine learning or data science context. The context confirms that the code is handling a tuple with two arrays, providing additional information about the data being manipulated.\"},\n",
       " {'id': '0e9b5bfc-2cfc-4fb8-a5f0-00c5e1092fbe',\n",
       "  'embedding': None,\n",
       "  'code': \"compute_metrics(\\n    start_logits,\\n    end_logits,\\n    validation_dataset,\\n    squad_dataset['validation']\\n)\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': \"The code snippet is calling the function 'compute_metrics' with three arguments: start_logits, end_logits, and two datasets - validation_dataset and squad_dataset['validation']. This function likely calculates some metrics based on the given logits and datasets, possibly for evaluating a model's performance. The file name, 'Training.ipynb', suggests that this code is likely part of a training notebook or script.\"},\n",
       " {'id': '3c79964f-f557-4846-9359-8a0429ad1d37',\n",
       "  'embedding': None,\n",
       "  'code': \"trainer.save_model('distilbert_bertqa')\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is using the \"save_model\" function to save a model named \"distilbert_bertqa\". This is being done in a file named \"Training.ipynb\" and the purpose is to save the model for future use.'},\n",
       " {'id': '0da6a4ff-0486-420f-9d2d-3e611ccbc364',\n",
       "  'embedding': None,\n",
       "  'code': \"qa = pipeline(\\n    'question-answering',\\n    model='distilbert_bertqa',\\n    device=0 #GPU\\n)\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code creates a question-answering pipeline using the DistilBERT BERTQA model from transformers. It also specifies the use of a GPU for training. The file name is \"Training.ipynb\", indicating that this code may be used for training a model.'},\n",
       " {'id': '7e50dfdc-5bae-4a58-a620-e57587cbd3ed',\n",
       "  'embedding': None,\n",
       "  'code': 'context = \"Tomorrow the AtlÃ¢ntico is going to have a delicious team lunch!\"\\nquestion = \"What did the AtlÃ¢ntico is going to have tomorrow?\"\\nqa(context=context, question=question)',\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': 'The code snippet is creating a question and context, and then using the qa() function to search for an answer to the question within the given context. This could be used for testing a pipeline, with the file name \"Training.ipynb\" suggesting it is part of a larger training program.'},\n",
       " {'id': 'b7530b05-2bd4-447a-aa5d-e9e723ca0b6f',\n",
       "  'embedding': None,\n",
       "  'code': \"print(f' {datetime.now() - start_time_all_execution}')\",\n",
       "  'filename': 'Training.ipynb',\n",
       "  'context': \"The code above prints the difference between the current time and the start time of the execution. The file name is 'Training.ipynb' and the code is being used to test the pipeline.\"},\n",
       " {'id': 'dd15b0bb-a5d9-4358-a5e9-7cfcd74ea6ba',\n",
       "  'embedding': None,\n",
       "  'code': '!pip install transformers',\n",
       "  'filename': 'Deployment.ipynb',\n",
       "  'context': 'The code snippet is installing the \"transformers\" package using the pip command. The file name is \"Deployment.ipynb\", indicating that this code is being used for deployment purposes. The context suggests that the code is being used to import necessary libraries or packages for the deployment process.'},\n",
       " {'id': '39666648-cf38-47a4-905c-8c2360585cd8',\n",
       "  'embedding': None,\n",
       "  'code': 'import mlflow\\nfrom mlflow import MlflowClient\\nfrom mlflow.types.schema import Schema, ColSpec\\nfrom mlflow.types import ParamSchema, ParamSpec\\nfrom mlflow.models import ModelSignature\\nfrom transformers import pipeline\\nimport torch\\nimport json\\nimport os',\n",
       "  'filename': 'Deployment.ipynb',\n",
       "  'context': 'The code snippet is importing various packages and modules related to machine learning and deployment, such as mlflow, transformers, torch, and json. The file name suggests that this code is part of a notebook for deploying a machine learning model. The context is likely a section or cell within the notebook.'},\n",
       " {'id': 'a8b96830-2676-453d-b6bc-bf62d721ba93',\n",
       "  'embedding': None,\n",
       "  'code': 'MODEL = \"morgana-rodrigues/bert_qa\"',\n",
       "  'filename': 'Deployment.ipynb',\n",
       "  'context': 'The code snippet defines the MODEL variable as the model \"morgana-rodrigues/bert_qa\", which is likely a pre-trained BERT model for question-answering tasks. The file name suggests that this code is used for deployment, meaning it is used to make the model accessible and usable for other applications or users. The optional context suggests that this code is part of a larger project or script, where the model is being described or introduced.'},\n",
       " {'id': '3e689926-34bd-4b9d-9d64-84c2fa4aba08',\n",
       "  'embedding': None,\n",
       "  'code': \"qa = pipeline(\\n    'question-answering',\\n    model=MODEL,\\n    device=-1 # -1 means running on CPU\\n)\",\n",
       "  'filename': 'Deployment.ipynb',\n",
       "  'context': 'The code snippet is creating a pipeline for question-answering, using a specified model and running on a CPU. The file name is Deployment.ipynb and the context is describing the model.'},\n",
       " {'id': 'c1c83c2b-fd9e-4685-87b6-5feeab610d3b',\n",
       "  'embedding': None,\n",
       "  'code': 'class DistilBERTModel(mlflow.pyfunc.PythonModel):\\n    def _preprocess(self, inputs):\\n        context = inputs[\\'context\\'][0]\\n        question = inputs[\\'question\\'][0]\\n        print(\"pre processing\", context,question)\\n        return context, question\\n        \\n    def load_context(self, context):\\n        self.model = pipeline(\\n            \\'question-answering\\',\\n             model=context.artifacts[\"model\"],\\n             device=-1\\n        )\\n        \\n    def predict(self, context, model_input, params):\\n        in_ctx, question = self._preprocess(model_input)\\n        output = self.model(context=in_ctx, question=question)\\n        return output\\n\\n    @classmethod\\n    def log_model(cls, model_name, trainer = None, pipeline = None, demo_folder=\"demo\"): #eg (model, \\'\\', \\'my_model\\')\\n        input_schema = Schema(\\n            [\\n                ColSpec(\"string\", \"context\"),\\n                ColSpec(\"string\", \"question\"),\\n            ]\\n        )\\n        output_schema = Schema(\\n            [\\n                ColSpec(\"string\", \"answer\")\\n            ]\\n        )\\n        \\n        params_schema = ParamSchema(\\n            [\\n                ParamSpec(\"show_score\", \"boolean\", False)\\n            ]\\n        )\\n      \\n        signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=params_schema)\\n        if trainer is not None:\\n            trainer.save_model(model_name)\\n        elif pipeline is not None:\\n            pipeline.save_pretrained(model_name)\\n             \\n        requirements = [\\n            \"transformers==4.37.0\",\\n            \"numpy==1.24.3\",\\n            \"torch==2.0.0\",\\n            \"tqdm==4.65.0\",\\n        ]\\n        mlflow.pyfunc.log_model(\\n            model_name,\\n            python_model=cls(),\\n            artifacts={\"model\": model_name, \"demo\": demo_folder},\\n            signature=signature,\\n            pip_requirements=requirements\\n        )',\n",
       "  'filename': 'Deployment.ipynb',\n",
       "  'context': 'The code above is a class called DistilBERTModel that is used for deploying a model. It contains functions for preprocessing, loading the context (model), and making predictions. The function log_model is used to save the model and its dependencies, and define the input and output schemas. The file name is Deployment.ipynb.'},\n",
       " {'id': '66bcc4a9-b046-4cea-acb5-07c26c7bc784',\n",
       "  'embedding': None,\n",
       "  'code': \"mlflow.set_experiment(experiment_name='BERT for Q&A')\",\n",
       "  'filename': 'Deployment.ipynb',\n",
       "  'context': \"The code snippet is using the mlflow library to set an experiment named 'BERT for Q&A'. The file name is Deployment.ipynb, which suggests that this code is part of a deployment notebook. The context suggests that this code is related to a model registry, which is a centralized location for storing and managing machine learning models. Overall, the code is setting up an experiment for BERT (a popular natural language processing model) for question and answer tasks, likely as part of a deployment process.\"},\n",
       " {'id': 'b4625382-03ce-4492-a9a7-38d9c73a245a',\n",
       "  'embedding': None,\n",
       "  'code': 'with mlflow.start_run(run_name=\\'BERT_QA\\') as run:\\n    print(f\"Run\\'s Artifact URI: {run.info.artifact_uri}\")\\n    DistilBERTModel.log_model(model_name=\\'BERT_QA\\', pipeline=qa)\\n    mlflow.register_model(model_uri = f\"runs:/{run.info.run_id}/BERT_QA\", name=\\'BERT_QA\\')',\n",
       "  'filename': 'Deployment.ipynb',\n",
       "  'context': \"The code snippet is using the mlflow library to start a new run with the name 'BERT_QA'. It then prints the artifact URI for the run and logs the model 'BERT_QA' with its associated pipeline. Finally, it registers the model with the name 'BERT_QA' using the model URI from the current run. The deployment.ipynb file is most likely used for deploying the model to a production environment. This code is part of the model registry process, which allows for tracking and managing different versions of the model.\"},\n",
       " {'id': '9ed1e645-f0a2-42b9-a4a6-f4ced23ed002',\n",
       "  'embedding': None,\n",
       "  'code': 'client = mlflow.MlflowClient()\\nmodel_metadata = client.get_latest_versions(\"BERT_QA\", stages=[\"None\"])\\nlatest_model_version = model_metadata[0].version\\nprint(latest_model_version, mlflow.models.get_model_info(f\"models:/BERT_QA/{latest_model_version}\").signature)',\n",
       "  'filename': 'Deployment.ipynb',\n",
       "  'context': 'The code snippet is using the mlflow library to retrieve the latest version of a model named \"BERT_QA\" and its metadata. It then assigns the version number to a variable and uses that variable to retrieve the model\\'s signature. The file name is \"Deployment.ipynb\" and the context is for testing the latest registered model.'},\n",
       " {'id': '6fa7fc11-c57c-4d65-9e0f-a472992f1b09',\n",
       "  'embedding': None,\n",
       "  'code': 'model = mlflow.pyfunc.load_model(model_uri=f\"models:/BERT_QA/{latest_model_version}\")\\ncontext = \"Marta is mother of John and Amanda\"\\nquestion = \"what is the name of Marta\\'s daugther?\"\\nmodel.predict({\"context\": [context], \"question\":[question]})',\n",
       "  'filename': 'Deployment.ipynb',\n",
       "  'context': 'The code snippet is loading a model from a specific directory using the mlflow.pyfunc.load_model function. The directory is specified as \"models:/BERT_QA/\" with the latest version of the model being used. The context and question variables are then defined, with the context being set as \"Marta is mother of John and Amanda\" and the question being \"what is the name of Marta\\'s daughter?\". Finally, the model is used to predict the answer to the question by passing in the context and question as inputs. This code is likely being used for testing the latest version of the model in the Deployment.ipynb file.'},\n",
       " {'id': 'acd59e8f-9d05-47bd-9ca3-ac69544cafbd',\n",
       "  'embedding': None,\n",
       "  'code': '',\n",
       "  'filename': 'Deployment.ipynb',\n",
       "  'context': 'The code snippet in the file \"Deployment.ipynb\" is used for testing the latest model that has been registered. This could be part of a deployment process for a machine learning model, where the model is being tested to ensure it is working properly and producing accurate results. The code likely includes steps for loading the model, providing test data, and evaluating the results. The optional context may provide more specific information about the testing process or the model being tested.'},\n",
       " {'id': 'a03cbf04-869f-4281-ab9b-c1cfbb93ed67',\n",
       "  'embedding': None,\n",
       "  'code': 'import requests\\nimport mlflow',\n",
       "  'filename': 'Testing Mlflow Server.ipynb',\n",
       "  'context': 'The code snippet imports the requests and mlflow libraries. The file name is \"Testing Mlflow Server.ipynb\". The code is used to test and interact with a MLflow server, which is a platform for managing machine learning experiments and models. The context may contain additional information or code related to this task.'},\n",
       " {'id': '0dedf3b0-73f3-4f94-82d6-dff4e3fa80d3',\n",
       "  'embedding': None,\n",
       "  'code': 'context = \"Marta is mother of John and Amanda\"\\nquestion = \"what is the name of Marta\\'s daugther?\"\\n\\nurl = \\'http://localhost:5004/invocations\\'\\nbody = {\\n    \"inputs\": {\\n        \"context\": [context],\\n        \"question\":[question]\\n    }\\n}\\nheaders = {\\'Content-type\\': \\'application/json\\', }\\nresponse = requests.post(url, headers=headers, json=body)\\nresponse.reason, response.status_code, response.json()',\n",
       "  'filename': 'Testing Mlflow Server.ipynb',\n",
       "  'context': 'The code snippet is making a request to a local server (url) using the Python requests library. It is passing in two inputs, the context and the question, in a specific format (body). The server is expected to return a response with the reason for the request, the status code, and the response data in JSON format. The file name suggests that this code is part of a larger process of testing an MLflow server.'},\n",
       " {'id': '64edb7e5-2a43-4d81-8cbc-4de9f0308c6a',\n",
       "  'embedding': None,\n",
       "  'code': 'import pandas as pd',\n",
       "  'filename': 'Spam_Detection.ipynb',\n",
       "  'context': 'The code snippet imports the pandas library and assigns it to the variable \"pd.\" The file name is \"Spam_Detection.ipynb\" and the code is being used in the context of importing necessary libraries.'},\n",
       " {'id': 'baabd9d6-b3b7-4114-8244-3e5d59546956',\n",
       "  'embedding': None,\n",
       "  'code': 'pd.read_csv(\"SMS_Spam_Collection.csv\")',\n",
       "  'filename': 'Spam_Detection.ipynb',\n",
       "  'context': 'The code snippet is using the pandas library to read a CSV file named \"SMS_Spam_Collection.csv\" and store it as a dataframe. This dataframe is likely used for spam detection analysis and is saved in a file named \"Spam_Detection.ipynb\". The context suggests that this code is part of a larger project or notebook related to analyzing SMS spam data.'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6436a808-52a4-406c-8f2b-d3a4283fffc9",
   "metadata": {},
   "source": [
    "## Step 3: Generate Embeddings and Structure Data\n",
    "\n",
    "In this step, we use an embeddings model to generate embedding vectors for the context extracted from each code snippet. The code performs the following operations:\n",
    "\n",
    "**HuggingFace Embeddings**: We use the HuggingFace embeddings model \"all-MiniLM-L6-v2\" to generate vectors that semantically represent the context of the code snippets.\n",
    "\n",
    "**Function** *update_embeddings*: This function iterates through the previously extracted data structure. For each item:\n",
    "\n",
    "- Generates an embedding vector from the context field using the embed_query method of the embeddings model.\n",
    "- Updates the item in the data structure, inserting the new embedding vector into the embedding field.\n",
    "Conversion to DataFrame: After updating the data structure with the embeddings, we use the to_dataframe_row function to convert the list of code snippets and their respective metadata into a format suitable for a Pandas DataFrame.\n",
    "\n",
    "Each item in the data structure is converted into a dictionary containing:\n",
    "\n",
    "- **ID**: A unique identifier for the code snippet.\n",
    "- **Embeddings**: The embedding vector generated for the context.\n",
    "- **Code**: The extracted code.\n",
    "- **Metadata**: Additional metadata, such as the filename and updated context.\n",
    "  \n",
    "The list of dictionaries is then converted into a DataFrame.\n",
    "\n",
    "Creating the DataFrame: The to_dataframe_row function organizes this data, and Pandas is used to create a DataFrame, facilitating the manipulation and future use of the data with the results stored in a DataFrame for easy visualization and further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9be8588b-6138-468a-9a00-32d0191f9a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8fbdc09d604ddd9854192cfb12cc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085e40c04d4d438b8afaeaff86b48bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821bbdea39a24d1db6e26da8d4313723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ce526247f54aecab9d26bdfaf19ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f5a75d728f40d390d80eea582cff60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2c0ad0153646f78957d84161938ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c975c-bd47-476e-9791-73feafc3b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_embeddings(data_structure):\n",
    "    updated_structure = []\n",
    "    for item in data_structure:\n",
    "        context = item['context']\n",
    "\n",
    "        # Generate the embedding for the context\n",
    "        embedding_vector = embeddings.embed_query(context)\n",
    "\n",
    "        # Update the item with the new embedding\n",
    "        item['embedding'] = embedding_vector\n",
    "        updated_structure.append(item)\n",
    "    \n",
    "    return updated_structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe046de9-f2e4-48ac-afa1-bf4d401edf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_structure = update_embeddings(updated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3ca2e-2508-4efc-a9ae-d41feaf977ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def to_dataframe_row(embedded_snippets: list):\n",
    "    \"\"\"\n",
    "    Helper function to convert a list of embedded snippets into a dataframe row\n",
    "    in dictionary format.\n",
    "\n",
    "    Args:\n",
    "        embedded_snippets: List of dictionaries containing Snippets to be converted\n",
    "\n",
    "    Returns:\n",
    "        List of Dictionaries suitable for conversion to a DataFrame\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for snippet in embedded_snippets:\n",
    "        output = {\n",
    "            \"ids\": snippet['id'],\n",
    "            \"embeddings\": snippet['embedding'],\n",
    "            \"code\": snippet['code'],\n",
    "            \"metadatas\": {\n",
    "                \"filenames\": snippet['filename'],\n",
    "                \"context\": snippet['context'],\n",
    "            },\n",
    "        }\n",
    "        outputs.append(output)\n",
    "    return outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1903e6-e135-4a19-a3e1-4c0add9f981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = to_dataframe_row(updated_structure)\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405a0859-89a0-4ca5-a2c9-c2f2e752c82f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e3d85-02f1-4316-93ef-63df29dcf60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the 'context' field within dictionaries in the 'metadatas' column\n",
    "contexts = df['metadatas'].apply(lambda x: x.get('context', None))\n",
    "\n",
    "# Display the contexts\n",
    "print(contexts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b43d8f8-09ed-49c7-8504-2574a78bfb5a",
   "metadata": {},
   "source": [
    "## Step 4: Store and Query Documents in ChromaDB ðŸ”—ðŸ¦\n",
    "\n",
    "In this step, we use ChromaDB, a vector database system, to store code snippets and their respective metadata. We also implement a function to retrieve documents based on queries. The code performs the following operations:\n",
    "\n",
    "####  Connection and Collection Creation\n",
    "- **ChromaDB Client**: A ChromaDB client is initialized to interact with the database.\n",
    "- **Collection Creation or Retrieval**: The collection named \"my_collection\" is created (or retrieved, if it already exists) within the ChromaDB database. Collections are used to store documents and their corresponding embeddings.\n",
    "#### Inserting Documents\n",
    "- **Data Extraction**: The following fields are extracted from the DataFrame and converted into lists:\n",
    "   - **ids**: A list of unique identifiers for each document (code snippet).\n",
    "   - **documents**: A list of code snippets.\n",
    "   - **metadatas**: A list of metadata associated with each document, such as the filename and context.\n",
    "   - **embeddings_list**: A list of embedding vectors previously generated for the context of each code snippet.\n",
    "- **Inserting into ChromaDB**: The upsert method is used to insert or update the documents, ids, metadata, and embeddings in the created collection.\n",
    "#### Querying Documents\n",
    "- **Query**: After adding the documents to the collection, a query is performed. The code searches for documents related to the query text \"!pip install\", returning the 5 most relevant results.\n",
    "#### *retriever* **Function*\n",
    "- **Document Retrieval**: The retriever function is implemented to query the collection. It takes a query string, the collection, and the number of results to return (top_n) as parameters.\n",
    "  - **Query in ChromaDB**: The function executes a query in the collection using the provided string.\n",
    "  - **Creating Document Objects**: For each result returned, the function creates a Document instance containing the page content (code snippet) and its metadata.\n",
    "  - **Returning Documents**: The function returns a list of Document objects that contain the page content and metadata for easy retrieval and future analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ae488-1639-48f9-b988-30651042ee8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.get_or_create_collection(name=\"my_collection\")\n",
    "\n",
    "ids = df[\"ids\"].tolist()\n",
    "documents = df[\"code\"].tolist()\n",
    "metadatas = df[\"metadatas\"].tolist()\n",
    "embeddings_list = df[\"embeddings\"].tolist()\n",
    "\n",
    "data_to_insert = {\n",
    "    \"ids\": ids,\n",
    "    \"documents\": documents,\n",
    "    \"metadatas\": metadatas,\n",
    "    \"embeddings\": embeddings_list\n",
    "}\n",
    "\n",
    "for i in range(len(ids)):\n",
    "    print(f\"ID: {ids[i]}\")\n",
    "    print(f\"Document: {documents[i]}\")\n",
    "    print(f\"Metadata: {metadatas[i]}\")\n",
    "    print(f\"Embedding: {embeddings_list[i]}\\n\")\n",
    "\n",
    "collection.upsert(\n",
    "    documents=documents,\n",
    "    ids=ids,\n",
    "    metadatas=metadatas,\n",
    "    embeddings=embeddings_list  \n",
    ")\n",
    "\n",
    "print(\"Documents added successfully!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea2e862-d333-40a7-bea8-befb20570f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_count = collection.count()\n",
    "print(f\"Total documents in the collection: {document_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c4fee-50ae-4a13-bea4-7ae479723f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"!pip install\"],\n",
    "    n_results=5,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22ef3dc4-9154-420e-823f-60d0548b20ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['e174ba3d-d46f-467c-bdd5-5169bcab1eb3',\n",
       "   '2894eeb6-a424-4821-9af1-a4e49bee5a0d',\n",
       "   '453ca5ea-fea7-4261-9c2e-df7e334ac2f4',\n",
       "   '83135cda-a56a-4936-af8a-76fbbdba8cad',\n",
       "   '73f1d4c7-94b3-4dcc-811a-df379b4db2ea']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['!pip install webvtt-py\\n!pip install pandas',\n",
       "   '%pip install --upgrade --quiet  GitPython',\n",
       "   '\"\"\"\\nChain with Local model\\n\"\"\"\\n\\n#from typing import List\\n#from langchain.prompts import ChatPromptTemplate\\n#from langchain.schema.runnable import RunnablePassthrough\\n#from langchain.schema import StrOutputParser\\n#import uuid\\n\\n#def format_docs(docs: List[Document]) -> str:\\n#    return \"\\\\n\\\\n\".join([doc.page_content for doc in docs])\\n\\n#template = \"\"\"You are a Python wizard tasked with generating code for a Jupyter Notebook (.ipynb) based on the given context.\\n#Your answer should consist of just the Python code, without any additional text or explanation.\\n\\n#Context:\\n#{context}\\n\\n#Question: {question}\\n#\"\"\"\\n\\n#prompt = ChatPromptTemplate.from_template(template)\\n\\n#model = llm_local()\\n\\n#chain = {\\n#    \"context\": lambda inputs: format_docs(retriever(inputs[\\'query\\'], collection)),  \\n#    \"question\": RunnablePassthrough()  \\n#} | prompt | model | StrOutputParser()\\n',\n",
       "   'import promptquality as pq\\n\\nos.environ[\\'GALILEO_API_KEY\\'] = \"9zjBwRIhyWo4zzkdsJhvg2y-NTT92qjEQmt2DIFmCFg\" #your api Key\\ngalileo_url = \"https://console.hp.galileocloud.io/\"\\npq.login(galileo_url)',\n",
       "   '### Alternate code to load local models. \\n###This specific example requires the project to have an asset call Llama7b, associated with the cloud S3 URI s3://dsp-demo-bucket/LLMs (public bucket)\\n\\n#from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\\n#from langchain_community.llms import LlamaCpp\\n\\n#callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\\n\\n#llm = LlamaCpp(\\n            #model_path=\"/home/jovyan/datafabric/Llama7b/ggml-model-f16-Q5_K_M.gguf\",\\n            #n_gpu_layers=64,\\n            #n_batch=512,\\n            #n_ctx=4096,\\n            #max_tokens=1024,\\n            #f16_kv=True,  \\n            #callback_manager=callback_manager,\\n            #verbose=False,\\n            #stop=[],\\n            #streaming=False,\\n            #temperature=0.4,\\n        #)']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[{'context': 'The code installs two libraries, webvtt-py and pandas, using the pip command. These libraries will be used in the development of an example in a file named \"summarization-with-langchain.ipynb\". This code is used to add support for transcripts in the webvtt format using the webvtt-py library.',\n",
       "    'filenames': 'summarization-with-langchain.ipynb'},\n",
       "   {'context': 'The code is installing a library called GitPython in a quiet mode, which will allow for the use of Git version control in the project. This is being done as a part of setting up the environment for a project that will use LangChain, Huggingface models, OpenAI, and ChromaDB for storing embeddings. The code is being executed in a Jupyter notebook file named \"code-generation-with-langchain.ipynb\".',\n",
       "    'filenames': 'code-generation-with-langchain.ipynb'},\n",
       "   {'context': 'The code sets up a chain that uses a local model to generate code for a Jupyter Notebook based on a given context. It imports necessary libraries, defines a function for formatting documents, and creates a template for a chat prompt. The prompt is passed through a model and the output is parsed into a string format. The code is for text generation using Langchain and the file name is \"text-generation-with-langchain.ipynb\".',\n",
       "    'filenames': 'text-generation-with-langchain.ipynb'},\n",
       "   {'context': 'The code above sets up a connection to the Galileo console using an API key and a specific URL. It also imports a library for prompt quality and uses it to login to the console. The file \"chatbot-with-langchain.ipynb\" likely contains code for a chatbot that utilizes a language chain.',\n",
       "    'filenames': 'chatbot-with-langchain.ipynb'},\n",
       "   {'context': 'The code snippet loads a local model for summarizing text using the LlamaCpp library. It sets up parameters and options for the model and imports relevant libraries and packages. The file name suggests that this code is used in a Jupyter Notebook for summarizing text using the Langchain library. The optional context provided explains the purpose of the code and how it fits into a larger project or workflow, specifically for summarizing text.',\n",
       "    'filenames': 'summarization-with-langchain.ipynb'}]],\n",
       " 'distances': [[1.3229604959487915,\n",
       "   1.4430855512619019,\n",
       "   1.485620141029358,\n",
       "   1.4960834980010986,\n",
       "   1.5184643268585205]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44ea4d5b-5d97-4906-8f96-9c73569bf349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def retriever(query: str, collection, top_n: int = 10) -> List[Document]:\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_n\n",
    "    )\n",
    "    \n",
    "    documents = [\n",
    "        Document(\n",
    "            page_content=str(results['documents'][i]),\n",
    "            metadata=results['metadatas'][i] if isinstance(results['metadatas'][i], dict) else results['metadatas'][i][0]  # Corrigir o metadado se for uma lista\n",
    "        )\n",
    "        for i in range(len(results['documents']))\n",
    "    ]\n",
    "    \n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0d2ac5-1ad2-4125-ad23-03708db54075",
   "metadata": {},
   "source": [
    "## Step 5: Chain ðŸ¦œâ›“ï¸\n",
    "\n",
    "In this step, we use a flow to automatically generate Python code based on a provided context and question. The code performs the following:\n",
    "\n",
    "#### Function *format_docs(docs: List[Document]) -> str:*\n",
    "- **Purpose**: This function formats a list of documents docs into a single string by concatenating the content of each document (doc.page_content) with two line breaks (\\n\\n) between them. This ensures that the context used in code generation is organized and readable.\n",
    "\n",
    "#### Language Model and Processing Chain:\n",
    "- **ChatOpenAI**: A language model from OpenAI is used to generate responses based on the provided prompt.\n",
    "- The **chain**processes data using the following components:\n",
    "  - **Context**: The context is formatted using the *format_docs* function, which calls the retriever function to fetch relevant context from the document base.\n",
    "  - **Question**: The question is passed directly through the chain to process the prompt.\n",
    "  - **Model**: The model generates the code based on the template and the provided data.\n",
    "  - **Output Parser**: The output is processed with StrOutputParser to ensure the return is a clean string.\n",
    "\n",
    "#### Function *clean_and_print_code(result: str)*:\n",
    "- Purpose: This function takes the generated code string from the model and removes any formatting markers (e.g., ```python). After cleaning, the code is printed in a clean format, ready for execution.\n",
    "\n",
    "#### Interaction with Galileo:\n",
    "- The *promptquality* library is used to evaluate the quality of the generated prompts.\n",
    "- **Galileo Callback**: A custom callback is configured using the Galileo API Key, where the following evaluation scopes are set:\n",
    "   - **Context Adherence**: Evaluates whether the generated code aligns with the provided context.\n",
    "   - **Correctness**: Checks the factual accuracy of the generated code.\n",
    "   - **Prompt Perplexity**: Measures the complexity of the prompt, useful for evaluating its clarity.\n",
    " \n",
    "#### Chain Execution:\n",
    "- A set of inputs containing the query and the question is provided to run the chain. The system generates code based on questions like \"How can I use audio in RAG?\" and \"create code audio with RAG\" using the vector base.\n",
    "\n",
    "#### Results Publishing:\n",
    "- The Galileo callback finalizes and publishes the results, recording the evaluation of each run of the code generation chain.\n",
    "\n",
    "#### Function *create_new_code_cell_from_output(output)*:\n",
    " - Purpose: This function dynamically creates a new code cell in the Jupyter Notebook from the generated output. It handles different output formats such as strings or dictionaries (if the output contains JSON) and inserts the resulting code into the next code cell in the notebook.\n",
    "\n",
    "\n",
    "#### Processing the results: \n",
    "- After the chain execution, the function iterates over each generated result, attempts to parse it as JSON, and creates a new code cell in the notebook from the output. If the result is not JSON, it treats the output as a code string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2fe1793-7624-44f9-8878-fc861b83dfd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "template = \"\"\"You are a Python wizard tasked with generating code for a Jupyter Notebook (.ipynb) based on the given context.\n",
    "Your answer should consist of just the Python code, without any additional text or explanation.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI() \n",
    "\n",
    "chain = {\n",
    "    \"context\": lambda inputs: format_docs(retriever(inputs['query'], collection)), \n",
    "    \"question\": RunnablePassthrough()\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b31ca-f9ae-4c7d-b732-76568d00fdf6",
   "metadata": {},
   "source": [
    "### Local Model\n",
    "Cell to run a local model using LlamaCPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e531fbad-3fee-4005-8d1d-9e573901e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_docs(docs: List[Document]) -> str:\n",
    "    # return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# template = \"\"\"You are a Python wizard tasked with generating code for a Jupyter Notebook (.ipynb) based on the given context.\n",
    "# Your answer should consist of just the Python code, without any additional text or explanation.\n",
    "\n",
    "# Context:\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_template(template)\n",
    "# model = llm_local() \n",
    "\n",
    "# chain = {\n",
    "    # \"context\": lambda inputs: format_docs(retriever(inputs['query'], collection)), \n",
    "    # \"question\": RunnablePassthrough()\n",
    "# } | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9790269-881b-4dab-9d8d-0e18a4fa2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_print_code(result: str):\n",
    "    clean_code = result.replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
    "    \n",
    "    print(clean_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7dbbab73-a108-4652-8ec9-80cef16727fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‹ You have logged into ðŸ”­ Galileo (https://console.hp.galileocloud.io/) as diogo.vieira@hp.com.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Config(console_url=Url('https://console.hp.galileocloud.io/'), username=None, password=None, api_key=SecretStr('**********'), token=SecretStr('**********'), current_user='diogo.vieira@hp.com', current_project_id=None, current_project_name=None, current_run_id=None, current_run_name=None, current_run_url=None, current_run_task_type=None, current_template_id=None, current_template_name=None, current_template_version_id=None, current_template_version=None, current_template=None, current_dataset_id=None, current_job_id=None, current_prompt_optimization_job_id=None, api_url=Url('https://api.hp.galileocloud.io/'))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import promptquality as pq\n",
    "\n",
    "os.environ['GALILEO_API_KEY'] = \"htMRukWlQyvOEDMnAUYQUTQnEZL6_3ubALGkhn6ph70\" #your api Key\n",
    "galileo_url = \"https://console.hp.galileocloud.io/\"\n",
    "pq.login(galileo_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11037204-855f-4149-980e-782dc9b34766",
   "metadata": {},
   "source": [
    "### Information Parameter ðŸ’¡\n",
    "\n",
    "**Query**: A query is generally used to retrieve information, such as documents or code snippets, from a database or retrieval system, like a vector database or an embeddings database. In this case, the query is likely being used to search for code snippets related to the specific request, such as the creation of an LLM model and an embedding model.\n",
    "\n",
    "**Question**: The question represents the specific task you are asking the language model to perform. This involves generating code based on the context retrieved by the query. The question is sent to the LLM to generate the appropriate response or code based on the provided information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf3441bd-a570-4010-8af8-2f4ed3f30d7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdbd1b07b6b4d9c860d09f855caf97f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chain run...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial job complete, executing scorers asynchronously. Current status:\n",
      "cost: Done âœ…\n",
      "toxicity: Done âœ…\n",
      "pii: Done âœ…\n",
      "protect_status: Done âœ…\n",
      "prompt_perplexity: Failed âŒ, error was: Executing this metric requires credentials for OpenAI or Azure OpenAI service to be set.\n",
      "latency: Done âœ…\n",
      "groundedness: Failed âŒ, error was: Executing this metric requires credentials for OpenAI, Azure OpenAI or Vertex to be set.\n",
      "factuality: Failed âŒ, error was: Executing this metric requires credentials for OpenAI, Azure OpenAI or Vertex to be set.\n",
      "ðŸ”­ View your prompt run on the Galileo console at: https://console.hp.galileocloud.io/prompt/chains/ac3e990f-623d-4616-ade7-fb2b9d3cefd3/e7ded322-462f-45f4-b751-468180879c48?taskType=12\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from IPython import get_ipython\n",
    "from IPython.display import display, Code\n",
    "\n",
    "\n",
    "prompt_handler = pq.GalileoPromptCallback(\n",
    "    scorers=[\n",
    "        pq.Scorers.context_adherence_plus,  # groundedness\n",
    "        pq.Scorers.correctness,             # factuality\n",
    "        pq.Scorers.prompt_perplexity        # perplexity \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Example of inputs to run the chain\n",
    "inputs = [\n",
    "    {\"query\": \"instantiate a model with llama cpp local\", \"question\": \"create code local llm model with llamacpp\"},\n",
    "\n",
    "]\n",
    "#How to create a vector bank?\n",
    "#create code a chromadb vector database\n",
    "\n",
    "results = chain.batch(inputs, config=dict(callbacks=[prompt_handler]))\n",
    "\n",
    "# Publish run results\n",
    "prompt_handler.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9185a95d-6a9a-428a-8074-d394246d73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.core.getipython import get_ipython\n",
    "\n",
    "def create_new_code_cell_from_output(output):\n",
    "    \"\"\"\n",
    "    Creates a new code cell in Jupyter Notebook from an output,\n",
    "    dealing with different output formats.\n",
    "\n",
    "    Args:\n",
    "        output: The output to be inserted into the new cell. It can be a string, a dictionary\n",
    "                or another type of object.\n",
    "    \"\"\"\n",
    "\n",
    "    shell = get_ipython()\n",
    "\n",
    "    if isinstance(output, dict):\n",
    "        code = output['cells'][0]['source']\n",
    "        code = ''.join(code)\n",
    "    else:\n",
    "        code = str(output)\n",
    "\n",
    "    clean_code = code.strip()\n",
    "\n",
    "    shell.set_next_input(clean_code, replace=False)\n",
    "\n",
    "for result in results:\n",
    "    try:\n",
    "        output = json.loads(result)\n",
    "        create_new_code_cell_from_output(output)\n",
    "    except json.JSONDecodeError:\n",
    "        # If it's not JSON, just treat it as a string of code\n",
    "        create_new_code_cell_from_output(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38f1c0f-52e2-40ac-a858-93b76da4032a",
   "metadata": {},
   "source": [
    "### LLM generated code here!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd7f498-7b5d-4790-95bc-f58150823d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm_local = LlamaCpp(\n",
    "            model_path=\"/home/jovyan/datafabric/Llama7b/ggml-model-f16-Q5_K_M.gguf\",\n",
    "            n_gpu_layers=64,\n",
    "            n_batch=512,\n",
    "            n_ctx=4096,\n",
    "            max_tokens=1024,\n",
    "            f16_kv=True,  \n",
    "            callback_manager=callback_manager,\n",
    "            verbose=False,\n",
    "            stop=[],\n",
    "            streaming=False,\n",
    "            temperature=0.4,\n",
    "        )\n",
    "\n",
    "llm_chain = prompt | llm_local"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
